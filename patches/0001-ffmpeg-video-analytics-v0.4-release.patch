From 1adfa7232fb6804070af3b3327e4d4e320c8d516 Mon Sep 17 00:00:00 2001
From: Lin Xie <lin.xie@intel.com>
Date: Mon, 6 Jan 2020 09:33:30 +0800
Subject: [PATCH] ffmpeg video analytics v0.4 release

Change-Id: I905b22cf407ce915fd8392c8e03147c820783f6b
---
 configure                                          |  24 +
 fftools/ffmpeg.c                                   | 341 ++++++++-
 fftools/ffmpeg.h                                   |  15 +
 fftools/ffmpeg_filter.c                            |  22 +-
 fftools/ffmpeg_opt.c                               |  55 +-
 libavfilter/Makefile                               |  22 +
 libavfilter/allfilters.c                           |   5 +
 libavfilter/avfilter.c                             |  22 +-
 libavfilter/avfilter.h                             |  16 +
 libavfilter/avfiltergraph.c                        |  37 +
 libavfilter/graphparser.c                          |  86 ++-
 libavfilter/inference_backend/ff_base_inference.c  | 140 ++++
 libavfilter/inference_backend/ff_base_inference.h  | 241 ++++++
 libavfilter/inference_backend/ff_inference_impl.c  | 581 ++++++++++++++
 libavfilter/inference_backend/ff_inference_impl.h  |  41 +
 libavfilter/inference_backend/ff_list.c            |  93 +++
 libavfilter/inference_backend/ff_list.h            |  55 ++
 libavfilter/inference_backend/ff_proc_factory.c    | 804 ++++++++++++++++++++
 libavfilter/inference_backend/ff_proc_factory.h    |  27 +
 libavfilter/inference_backend/image.c              |  92 +++
 libavfilter/inference_backend/image.h              |  90 +++
 libavfilter/inference_backend/image_inference.c    | 199 +++++
 libavfilter/inference_backend/image_inference.h    | 163 ++++
 .../image_inference_async_preproc.c                | 245 ++++++
 .../image_inference_async_preproc.h                |  46 ++
 libavfilter/inference_backend/logger.c             |  57 ++
 libavfilter/inference_backend/logger.h             |  88 +++
 libavfilter/inference_backend/metaconverter.c      | 320 ++++++++
 libavfilter/inference_backend/metaconverter.h      |  52 ++
 libavfilter/inference_backend/model_proc.c         | 275 +++++++
 libavfilter/inference_backend/model_proc.h         |  37 +
 .../inference_backend/openvino_image_inference.c   | 835 +++++++++++++++++++++
 .../inference_backend/openvino_image_inference.h   |  77 ++
 libavfilter/inference_backend/pre_proc.c           | 239 ++++++
 libavfilter/inference_backend/pre_proc.h           |  81 ++
 libavfilter/inference_backend/pre_proc_mocker.c    |  61 ++
 libavfilter/inference_backend/pre_proc_swscale.c   | 201 +++++
 libavfilter/inference_backend/pre_proc_vaapi.c     | 279 +++++++
 libavfilter/inference_backend/queue.c              | 171 +++++
 libavfilter/inference_backend/safe_queue.c         | 167 +++++
 libavfilter/inference_backend/safe_queue.h         |  45 ++
 libavfilter/vf_inference_classify.c                | 370 +++++++++
 libavfilter/vf_inference_detect.c                  | 357 +++++++++
 libavfilter/vf_inference_identify.c                | 357 +++++++++
 libavfilter/vf_inference_metaconvert.c             | 272 +++++++
 libavfilter/vf_ocv_overlay.c                       | 319 ++++++++
 libavformat/Makefile                               |   2 +
 libavformat/allformats.c                           |   1 +
 libavformat/kafkaproto.c                           | 186 +++++
 libavformat/metapublishenc.c                       | 167 +++++
 libavformat/protocols.c                            |   1 +
 libavutil/buffer.c                                 |  11 +
 libavutil/buffer.h                                 |   6 +
 libavutil/frame.c                                  |   8 +-
 libavutil/frame.h                                  |   4 +
 libavutil/hwcontext_vaapi.c                        |   1 +
 libavutil/log.c                                    |  33 +
 libavutil/log.h                                    |   9 +
 libavutil/pixdesc.c                                |  12 +
 libavutil/pixfmt.h                                 |   1 +
 60 files changed, 8539 insertions(+), 25 deletions(-)
 mode change 100644 => 100755 fftools/ffmpeg.c
 mode change 100644 => 100755 fftools/ffmpeg.h
 mode change 100644 => 100755 fftools/ffmpeg_filter.c
 mode change 100644 => 100755 fftools/ffmpeg_opt.c
 mode change 100644 => 100755 libavfilter/Makefile
 mode change 100644 => 100755 libavfilter/allfilters.c
 mode change 100644 => 100755 libavfilter/avfilter.c
 mode change 100644 => 100755 libavfilter/avfilter.h
 mode change 100644 => 100755 libavfilter/avfiltergraph.c
 mode change 100644 => 100755 libavfilter/graphparser.c
 create mode 100644 libavfilter/inference_backend/ff_base_inference.c
 create mode 100644 libavfilter/inference_backend/ff_base_inference.h
 create mode 100644 libavfilter/inference_backend/ff_inference_impl.c
 create mode 100644 libavfilter/inference_backend/ff_inference_impl.h
 create mode 100644 libavfilter/inference_backend/ff_list.c
 create mode 100644 libavfilter/inference_backend/ff_list.h
 create mode 100755 libavfilter/inference_backend/ff_proc_factory.c
 create mode 100644 libavfilter/inference_backend/ff_proc_factory.h
 create mode 100644 libavfilter/inference_backend/image.c
 create mode 100644 libavfilter/inference_backend/image.h
 create mode 100644 libavfilter/inference_backend/image_inference.c
 create mode 100644 libavfilter/inference_backend/image_inference.h
 create mode 100644 libavfilter/inference_backend/image_inference_async_preproc.c
 create mode 100644 libavfilter/inference_backend/image_inference_async_preproc.h
 create mode 100644 libavfilter/inference_backend/logger.c
 create mode 100644 libavfilter/inference_backend/logger.h
 create mode 100644 libavfilter/inference_backend/metaconverter.c
 create mode 100644 libavfilter/inference_backend/metaconverter.h
 create mode 100644 libavfilter/inference_backend/model_proc.c
 create mode 100644 libavfilter/inference_backend/model_proc.h
 create mode 100644 libavfilter/inference_backend/openvino_image_inference.c
 create mode 100644 libavfilter/inference_backend/openvino_image_inference.h
 create mode 100644 libavfilter/inference_backend/pre_proc.c
 create mode 100644 libavfilter/inference_backend/pre_proc.h
 create mode 100644 libavfilter/inference_backend/pre_proc_mocker.c
 create mode 100644 libavfilter/inference_backend/pre_proc_swscale.c
 create mode 100644 libavfilter/inference_backend/pre_proc_vaapi.c
 create mode 100644 libavfilter/inference_backend/queue.c
 create mode 100644 libavfilter/inference_backend/safe_queue.c
 create mode 100644 libavfilter/inference_backend/safe_queue.h
 create mode 100644 libavfilter/vf_inference_classify.c
 create mode 100644 libavfilter/vf_inference_detect.c
 create mode 100644 libavfilter/vf_inference_identify.c
 create mode 100644 libavfilter/vf_inference_metaconvert.c
 create mode 100644 libavfilter/vf_ocv_overlay.c
 mode change 100644 => 100755 libavformat/Makefile
 mode change 100644 => 100755 libavformat/allformats.c
 create mode 100644 libavformat/kafkaproto.c
 create mode 100644 libavformat/metapublishenc.c
 mode change 100644 => 100755 libavformat/protocols.c
 mode change 100644 => 100755 libavutil/buffer.c
 mode change 100644 => 100755 libavutil/buffer.h
 mode change 100644 => 100755 libavutil/frame.c
 mode change 100644 => 100755 libavutil/frame.h
 mode change 100644 => 100755 libavutil/hwcontext_vaapi.c
 mode change 100644 => 100755 libavutil/log.c
 mode change 100644 => 100755 libavutil/log.h
 mode change 100644 => 100755 libavutil/pixdesc.c
 mode change 100644 => 100755 libavutil/pixfmt.h

diff --git a/configure b/configure
index 34c2adb..9cf255b 100755
--- a/configure
+++ b/configure
@@ -240,7 +240,9 @@ External library support:
   --enable-libgsm          enable GSM de/encoding via libgsm [no]
   --enable-libiec61883     enable iec61883 via libiec61883 [no]
   --enable-libilbc         enable iLBC de/encoding via libilbc [no]
+  --enable-libinference_engine_c_api enable dldt inference engine c api [no]
   --enable-libjack         enable JACK audio sound server [no]
+  --enable-libjson_c       enable libjson-c [no]
   --enable-libklvanc       enable Kernel Labs VANC processing [no]
   --enable-libkvazaar      enable HEVC encoding via libkvazaar [no]
   --enable-liblensfun      enable lensfun lens correction [no]
@@ -263,6 +265,7 @@ External library support:
   --enable-libsoxr         enable Include libsoxr resampling [no]
   --enable-libspeex        enable Speex de/encoding via libspeex [no]
   --enable-libsrt          enable Haivision SRT protocol via libsrt [no]
+  --enable-librdkafka      enable Kafka protocol via librdkafka [no]
   --enable-libssh          enable SFTP protocol via libssh [no]
   --enable-libtensorflow   enable TensorFlow as a DNN module backend
                            for DNN based filters like sr [no]
@@ -1772,7 +1775,9 @@ EXTERNAL_LIBRARY_LIST="
     libgsm
     libiec61883
     libilbc
+    libinference_engine_c_api
     libjack
+    libjson_c
     libklvanc
     libkvazaar
     libmodplug
@@ -1812,6 +1817,7 @@ EXTERNAL_LIBRARY_LIST="
     opengl
     pocketsphinx
     vapoursynth
+    librdkafka
 "
 
 HWACCEL_AUTODETECT_LIBRARY_LIST="
@@ -2357,6 +2363,7 @@ CONFIG_EXTRA="
     huffyuvdsp
     huffyuvencdsp
     idctdsp
+    image_inference
     iirfilter
     mdct15
     intrax8
@@ -2603,6 +2610,8 @@ cbs_vp9_select="cbs"
 dct_select="rdft"
 dirac_parse_select="golomb"
 dnn_suggest="libtensorflow"
+image_inference_suggest="libinference_engine_c_api"
+image_inference_deps="libjson_c"
 error_resilience_select="me_cmp"
 faandct_deps="faan"
 faandct_select="fdctdsp"
@@ -3408,6 +3417,7 @@ libsrt_protocol_deps="libsrt"
 libsrt_protocol_select="network"
 libssh_protocol_deps="libssh"
 libtls_conflict="openssl gnutls mbedtls"
+rdkafka_protocol_deps="librdkafka"
 
 # filters
 afftdn_filter_deps="avcodec"
@@ -3471,6 +3481,13 @@ fspp_filter_deps="gpl"
 geq_filter_deps="gpl"
 histeq_filter_deps="gpl"
 hqdn3d_filter_deps="gpl"
+inference_identify_filter_deps="libjson_c"
+inference_identify_filter_select="dnn"
+inference_metaconvert_filter_deps="libjson_c"
+inference_classify_filter_deps="libinference_engine_c_api libjson_c"
+inference_classify_filter_select="image_inference"
+inference_detect_filter_deps="libinference_engine_c_api libjson_c"
+inference_detect_filter_select="image_inference"
 interlace_filter_deps="gpl"
 kerndeint_filter_deps="gpl"
 ladspa_filter_deps="ladspa libdl"
@@ -3487,6 +3504,7 @@ nlmeans_opencl_filter_deps="opencl"
 nnedi_filter_deps="gpl"
 ocr_filter_deps="libtesseract"
 ocv_filter_deps="libopencv"
+ocv_overlay_filter_deps="libopencv"
 openclsrc_filter_deps="opencl"
 overlay_opencl_filter_deps="opencl"
 overlay_qsv_filter_deps="libmfx"
@@ -6367,6 +6385,12 @@ enabled rkmpp             && { require_pkg_config rkmpp rockchip_mpp  rockchip/r
                              }
 enabled vapoursynth       && require_pkg_config vapoursynth "vapoursynth-script >= 42" VSScript.h vsscript_init
 
+enabled librdkafka  && require_pkg_config librdkafka rdkafka "librdkafka/rdkafka.h" rd_kafka_version
+
+enabled libjson_c && check_pkg_config libjson_c json-c json-c/json.h json_c_version
+
+enabled libinference_engine_c_api &&
+    require_pkg_config libinference_engine_c_api dldt_c_api "ie_c_api.h" ie_c_api_version
 
 if enabled gcrypt; then
     GCRYPT_CONFIG="${cross_prefix}libgcrypt-config"
diff --git a/fftools/ffmpeg.c b/fftools/ffmpeg.c
old mode 100644
new mode 100755
index 01f0410..d8d6dd5
--- a/fftools/ffmpeg.c
+++ b/fftools/ffmpeg.c
@@ -151,6 +151,7 @@ int        nb_input_files   = 0;
 
 OutputStream **output_streams = NULL;
 int         nb_output_streams = 0;
+int         total_frames_num = 0;
 OutputFile   **output_files   = NULL;
 int         nb_output_files   = 0;
 
@@ -1524,6 +1525,112 @@ static int reap_filters(int flush)
     return 0;
 }
 
+static int pipeline_reap_filters(int flush, InputFilter * ifilter)
+{
+    AVFrame *filtered_frame = NULL;
+    int i;
+
+    for (i = 0; i < nb_output_streams; i++) {
+        if (ifilter == output_streams[i]->filter->graph->inputs[0]) break;
+    }
+    OutputStream *ost = output_streams[i];
+    OutputFile    *of = output_files[ost->file_index];
+    AVFilterContext *filter;
+    AVCodecContext *enc = ost->enc_ctx;
+    int ret = 0;
+
+    if (!ost->filter || !ost->filter->graph->graph)
+        return 0;
+    filter = ost->filter->filter;
+
+    if (!ost->initialized) {
+        char error[1024] = "";
+        ret = init_output_stream(ost, error, sizeof(error));
+        if (ret < 0) {
+            av_log(NULL, AV_LOG_ERROR, "Error initializing output stream %d:%d -- %s\n",
+                   ost->file_index, ost->index, error);
+            exit_program(1);
+        }
+    }
+
+    if (!ost->filtered_frame && !(ost->filtered_frame = av_frame_alloc())) {
+        return AVERROR(ENOMEM);
+    }
+    filtered_frame = ost->filtered_frame;
+
+    while (1) {
+        double float_pts = AV_NOPTS_VALUE; // this is identical to filtered_frame.pts but with higher precision
+        ret = av_buffersink_get_frame_flags(filter, filtered_frame,
+                                           AV_BUFFERSINK_FLAG_NO_REQUEST);
+        if (ret < 0) {
+            if (ret != AVERROR(EAGAIN) && ret != AVERROR_EOF) {
+                av_log(NULL, AV_LOG_WARNING,
+                       "Error in av_buffersink_get_frame_flags(): %s\n", av_err2str(ret));
+            } else if (flush && ret == AVERROR_EOF) {
+                if (av_buffersink_get_type(filter) == AVMEDIA_TYPE_VIDEO)
+                    do_video_out(of, ost, NULL, AV_NOPTS_VALUE);
+            }
+            break;
+        }
+        if (ost->finished) {
+            av_frame_unref(filtered_frame);
+            continue;
+        }
+        if (filtered_frame->pts != AV_NOPTS_VALUE) {
+            int64_t start_time = (of->start_time == AV_NOPTS_VALUE) ? 0 : of->start_time;
+            AVRational filter_tb = av_buffersink_get_time_base(filter);
+            AVRational tb = enc->time_base;
+            int extra_bits = av_clip(29 - av_log2(tb.den), 0, 16);
+
+            tb.den <<= extra_bits;
+            float_pts =
+                av_rescale_q(filtered_frame->pts, filter_tb, tb) -
+                av_rescale_q(start_time, AV_TIME_BASE_Q, tb);
+            float_pts /= 1 << extra_bits;
+            // avoid exact midoints to reduce the chance of rounding differences, this can be removed in case the fps code is changed to work with integers
+            float_pts += FFSIGN(float_pts) * 1.0 / (1<<17);
+
+            filtered_frame->pts =
+                av_rescale_q(filtered_frame->pts, filter_tb, enc->time_base) -
+                av_rescale_q(start_time, AV_TIME_BASE_Q, enc->time_base);
+        }
+        //if (ost->source_index >= 0)
+        //    *filtered_frame= *input_streams[ost->source_index]->decoded_frame; //for me_threshold
+
+        switch (av_buffersink_get_type(filter)) {
+        case AVMEDIA_TYPE_VIDEO:
+            if (!ost->frame_aspect_ratio.num)
+                enc->sample_aspect_ratio = filtered_frame->sample_aspect_ratio;
+
+            if (debug_ts) {
+                av_log(NULL, AV_LOG_INFO, "filter -> pts:%s pts_time:%s exact:%f time_base:%d/%d\n",
+                        av_ts2str(filtered_frame->pts), av_ts2timestr(filtered_frame->pts, &enc->time_base),
+                        float_pts,
+                        enc->time_base.num, enc->time_base.den);
+            }
+
+            do_video_out(of, ost, filtered_frame, float_pts);
+            break;
+        case AVMEDIA_TYPE_AUDIO:
+            if (!(enc->codec->capabilities & AV_CODEC_CAP_PARAM_CHANGE) &&
+                enc->channels != filtered_frame->channels) {
+                av_log(NULL, AV_LOG_ERROR,
+                       "Audio filter graph output is not normalized and encoder does not support parameter changes\n");
+                break;
+            }
+            do_audio_out(of, ost, filtered_frame);
+            break;
+        default:
+            // TODO support subtitle filters
+            av_assert0(0);
+        }
+
+        av_frame_unref(filtered_frame);
+    }
+
+    return 0;
+}
+
 static void print_final_stats(int64_t total_size)
 {
     uint64_t video_size = 0, audio_size = 0, extra_size = 0, other_size = 0;
@@ -1653,6 +1760,7 @@ static void print_report(int is_last_report, int64_t timer_start, int64_t cur_ti
     double speed;
     int64_t pts = INT64_MIN + 1;
     static int64_t last_time = -1;
+    static int64_t init_time = 0;
     static int qp_histogram[52];
     int hours, mins, secs, us;
     const char *hours_sign;
@@ -1674,6 +1782,20 @@ static void print_report(int is_last_report, int64_t timer_start, int64_t cur_ti
 
     t = (cur_time-timer_start) / 1000000.0;
 
+    if (init_time == 0 && do_profiling_all) {
+        for (i = 0; i < nb_filtergraphs; i++) {
+            FilterGraph *fg = filtergraphs[i];
+            if (!fg || !fg->graph)
+                continue;
+            for (int j = 0; j < fg->graph->nb_filters; j++) {
+                AVFilterContext *ft = fg->graph->filters[j];
+                if (!ft)
+                    continue;
+                init_time += ft->init_working_time;
+                av_log(ft, AV_LOG_INFO, "init time:%"PRId64"\n", ft->init_working_time);
+            }
+        }
+    }
 
     oc = output_files[0]->ctx;
 
@@ -1696,7 +1818,7 @@ static void print_report(int is_last_report, int64_t timer_start, int64_t cur_ti
             av_bprintf(&buf_script, "stream_%d_%d_q=%.1f\n",
                        ost->file_index, ost->index, q);
         }
-        if (!vid && enc->codec_type == AVMEDIA_TYPE_VIDEO) {
+        if (!vid && enc->codec_type == AVMEDIA_TYPE_VIDEO && !do_profiling_all) {
             float fps;
 
             frame_number = ost->frame_number;
@@ -1756,6 +1878,21 @@ static void print_report(int is_last_report, int64_t timer_start, int64_t cur_ti
         if (is_last_report)
             nb_frames_drop += ost->last_dropped;
     }
+    if (do_profiling_all) {
+        total_frames_num = 0;
+        for (i = 0; i < nb_output_streams; i++) {
+            ost = output_streams[i];
+            if (ost->enc_ctx->codec_type == AVMEDIA_TYPE_VIDEO)
+                total_frames_num += ost->frames_encoded;
+        }
+        float total_fps;
+        total_fps = t > 1 ? total_frames_num / t : 0;
+        av_bprintf(&buf, "| profiling | total frame=%d ", total_frames_num);
+        av_bprintf(&buf, "fps=%.2f |", total_fps);
+        total_fps = t > 1 ? total_frames_num / (t - init_time / 1000000.0 ): 0;
+        if (total_fps > 0)
+            av_bprintf(&buf, " fps without filter init=%.2f |", total_fps);
+    }
 
     secs = FFABS(pts) / AV_TIME_BASE;
     us = FFABS(pts) % AV_TIME_BASE;
@@ -2159,6 +2296,12 @@ static int ifilter_send_frame(InputFilter *ifilter, AVFrame *frame)
             return ret;
     }
 
+    if (need_reinit && av_finit_firstly_get()) {
+        av_finit_firstly_set(0);
+        if (fg->filter_inited_graph)
+            avfilter_graph_free(&fg->filter_inited_graph);
+    }
+
     /* (re)init the graph if possible, otherwise buffer the frame and return */
     if (need_reinit || !fg->graph) {
         for (i = 0; i < fg->nb_inputs; i++) {
@@ -2180,7 +2323,15 @@ static int ifilter_send_frame(InputFilter *ifilter, AVFrame *frame)
             }
         }
 
+#if HAVE_THREADS
+        if (!abr_pipeline) {
+            ret = reap_filters(1);
+        } else {
+            ret = pipeline_reap_filters(1, ifilter);
+        }
+#else
         ret = reap_filters(1);
+#endif
         if (ret < 0 && ret != AVERROR_EOF) {
             av_log(NULL, AV_LOG_ERROR, "Error while filtering: %s\n", av_err2str(ret));
             return ret;
@@ -2209,6 +2360,16 @@ static int ifilter_send_eof(InputFilter *ifilter, int64_t pts)
 
     ifilter->eof = 1;
 
+#if HAVE_THREADS
+    if (abr_pipeline) {
+        ifilter->waited_frm = NULL;
+        pthread_mutex_lock(&ifilter->process_mutex);
+        ifilter->t_end = 1;
+        pthread_cond_signal(&ifilter->process_cond);
+        pthread_mutex_unlock(&ifilter->process_mutex);
+        pthread_join(ifilter->f_thread, NULL);
+    }
+#endif
     if (ifilter->filter) {
         ret = av_buffersrc_close(ifilter->filter, pts, AV_BUFFERSRC_FLAG_PUSH);
         if (ret < 0)
@@ -2253,6 +2414,42 @@ static int decode(AVCodecContext *avctx, AVFrame *frame, int *got_frame, AVPacke
     return 0;
 }
 
+#if HAVE_THREADS
+static void *filter_pipeline(void *arg)
+{
+    InputFilter *fl = arg;
+    AVFrame *frm;
+    int ret;
+    while(1) {
+        pthread_mutex_lock(&fl->process_mutex);
+        while (fl->waited_frm == NULL && !fl->t_end)
+            pthread_cond_wait(&fl->process_cond, &fl->process_mutex);
+        pthread_mutex_unlock(&fl->process_mutex);
+
+        if (fl->t_end) break;
+
+        frm = fl->waited_frm;
+        ret = ifilter_send_frame(fl, frm);
+        if (ret < 0) {
+            av_log(NULL, AV_LOG_ERROR,
+                   "Failed to inject frame into filter network: %s\n", av_err2str(ret));
+        } else {
+            ret = pipeline_reap_filters(0, fl);
+        }
+        fl->t_error = ret;
+
+        pthread_mutex_lock(&fl->finish_mutex);
+        fl->waited_frm = NULL;
+        pthread_cond_signal(&fl->finish_cond);
+        pthread_mutex_unlock(&fl->finish_mutex);
+
+        if (ret < 0) {
+            break;
+        }
+    }
+    return;
+}
+#endif
 static int send_frame_to_filters(InputStream *ist, AVFrame *decoded_frame)
 {
     int i, ret;
@@ -2260,22 +2457,72 @@ static int send_frame_to_filters(InputStream *ist, AVFrame *decoded_frame)
 
     av_assert1(ist->nb_filters > 0); /* ensure ret is initialized */
     for (i = 0; i < ist->nb_filters; i++) {
-        if (i < ist->nb_filters - 1) {
-            f = ist->filter_frame;
-            ret = av_frame_ref(f, decoded_frame);
-            if (ret < 0)
+#if HAVE_THREADS
+        if (!abr_pipeline) {
+#endif
+            if (i < ist->nb_filters - 1) {
+                f = ist->filter_frame;
+                ret = av_frame_ref(f, decoded_frame);
+                if (ret < 0)
+                    break;
+            } else
+                f = decoded_frame;
+
+                ret = ifilter_send_frame(ist->filters[i], f);
+                if (ret == AVERROR_EOF)
+                    ret = 0; /* ignore */
+                if (ret < 0) {
+                    av_log(NULL, AV_LOG_ERROR,
+                           "Failed to inject frame into filter network: %s\n", av_err2str(ret));
+                    break;
+                }
+#if HAVE_THREADS
+        } else {
+            if (i < ist->nb_filters - 1) {
+                f = &ist->filters[i]->input_frm;
+                ret = av_frame_ref(f, decoded_frame);
+                if (ret < 0)
+                    break;
+            } else
+                f = decoded_frame;
+
+            if(ist->filters[i]->f_thread == 0) {
+                if ((ret = pthread_create(&ist->filters[i]->f_thread, NULL, filter_pipeline, ist->filters[i]))) {
+                    av_log(NULL, AV_LOG_ERROR, "pthread_create failed: %s. Try to increase `ulimit -v` or decrease `ulimit -s`.\n", strerror(ret));
+                    return AVERROR(ret);
+                }
+                pthread_mutex_init(&ist->filters[i]->process_mutex, NULL);
+                pthread_mutex_init(&ist->filters[i]->finish_mutex, NULL);
+                pthread_cond_init(&ist->filters[i]->process_cond, NULL);
+                pthread_cond_init(&ist->filters[i]->finish_cond, NULL);
+                ist->filters[i]->t_end = 0;
+                ist->filters[i]->t_error = 0;
+            }
+
+            pthread_mutex_lock(&ist->filters[i]->process_mutex);
+            ist->filters[i]->waited_frm = f;
+            pthread_cond_signal(&ist->filters[i]->process_cond);
+            pthread_mutex_unlock(&ist->filters[i]->process_mutex);
+        }
+#endif
+    }
+#if HAVE_THREADS
+    if (abr_pipeline) {
+        for (i = 0; i < ist->nb_filters; i++) {
+            pthread_mutex_lock(&ist->filters[i]->finish_mutex);
+            while(ist->filters[i]->waited_frm != NULL)
+                pthread_cond_wait(&ist->filters[i]->finish_cond, &ist->filters[i]->finish_mutex);
+            pthread_mutex_unlock(&ist->filters[i]->finish_mutex);
+        }
+        for (i = 0; i < ist->nb_filters; i++) {
+            if (ist->filters[i]->t_error < 0) {
+                ret = ist->filters[i]->t_error;
                 break;
-        } else
-            f = decoded_frame;
-        ret = ifilter_send_frame(ist->filters[i], f);
-        if (ret == AVERROR_EOF)
-            ret = 0; /* ignore */
-        if (ret < 0) {
-            av_log(NULL, AV_LOG_ERROR,
-                   "Failed to inject frame into filter network: %s\n", av_err2str(ret));
-            break;
+            }
         }
     }
+#endif
+
     return ret;
 }
 
@@ -4262,6 +4509,40 @@ static int process_input(int file_index)
     int ret, thread_ret, i, j;
     int64_t duration;
     int64_t pkt_dts;
+    AVCodecContext *avctx;
+
+    for (i = 0; i < ifile->nb_streams; i++) {
+        ist = input_streams[ifile->ist_index + i];
+
+        if (load_balance && ist->filters && ist->filters[0]->filter) {
+            ret = avfilter_chain_occupation(ist->filters[0]->filter);
+            if (ret >= load_balance) {
+                for (j = 0; j < nb_filtergraphs; j++) {
+                    FilterGraph *fg = filtergraphs[j];
+                    avfilter_graph_set_parsed(fg->graph);
+                }
+                usleep(1000);
+                return 0;
+            }
+        }
+
+        avctx = ist->dec_ctx;
+        if (avctx && avctx->hw_frames_ctx) {
+            AVHWFramesContext *frames_ctx = (AVHWFramesContext*)avctx->hw_frames_ctx->data;
+
+            int empty = av_buffer_pool_is_empty(frames_ctx->pool);
+
+            if (empty) {
+                av_log(NULL, AV_LOG_VERBOSE, "Buffer pool is empty.\n");
+                for (j = 0; j < nb_filtergraphs; j++) {
+                    FilterGraph *fg = filtergraphs[j];
+                    avfilter_graph_set_parsed(fg->graph);
+                }
+                usleep(1000);
+                return 0;
+            }
+        }
+    }
 
     is  = ifile->ctx;
     ret = get_input_packet(ifile, &pkt);
@@ -4645,6 +4926,9 @@ static int transcode_step(void)
     if (ret < 0)
         return ret == AVERROR_EOF ? 0 : ret;
 
+#if HAVE_THREADS
+    if (abr_pipeline) return 0;
+#endif
     return reap_filters(0);
 }
 
@@ -4757,6 +5041,35 @@ static int transcode(void)
         }
     }
 
+    if (do_profiling_all) { //for filters
+        for (i = 0; i < nb_filtergraphs; i++) {
+            FilterGraph *fg = filtergraphs[i];
+            if (!fg || !fg->graph)
+                continue;
+            for (int j = 0; j < fg->graph->nb_filters; j++) {
+                AVFilterContext *ft = fg->graph->filters[j];
+                int64_t frame_cnt = 0;
+                int k;
+
+                if (!ft)
+                    continue;
+                for (k = 0; k < ft->nb_outputs; k++) {
+                    if (ft->outputs[k])
+                        frame_cnt += ft->outputs[k]->frame_count_out;
+                }
+                if (frame_cnt == 0)
+                    continue;
+                if (ft->sum_working_time > 1) {
+                    double fps = (double)(frame_cnt * 1000000) / ft->sum_working_time;
+                    if (fps < 10000) { //some filter delivered too big fps is not we focused
+                        printf("| filter profiling | name=%s, init=%.2f ms, frame=%ld, fps=%.2f\n",
+                                ft->filter->name, (double)ft->init_working_time / 1000, frame_cnt, fps);
+                    }
+                }
+            }
+        }
+    }
+
     av_buffer_unref(&hw_device_ctx);
     hw_device_free_all();
 
diff --git a/fftools/ffmpeg.h b/fftools/ffmpeg.h
old mode 100644
new mode 100755
index 7b6f802..65bded8
--- a/fftools/ffmpeg.h
+++ b/fftools/ffmpeg.h
@@ -253,6 +253,17 @@ typedef struct InputFilter {
 
     AVBufferRef *hw_frames_ctx;
 
+    // for abr pipeline
+    AVFrame *waited_frm;
+    AVFrame input_frm;
+    pthread_t f_thread;
+    pthread_cond_t process_cond;
+    pthread_cond_t finish_cond;
+    pthread_mutex_t process_mutex;
+    pthread_mutex_t finish_mutex;
+    int t_end;
+    int t_error;
+
     int eof;
 } InputFilter;
 
@@ -284,6 +295,7 @@ typedef struct FilterGraph {
     const char    *graph_desc;
 
     AVFilterGraph *graph;
+    AVFilterGraph *filter_inited_graph;
     int reconfiguration;
 
     InputFilter   **inputs;
@@ -590,6 +602,8 @@ extern int video_sync_method;
 extern float frame_drop_threshold;
 extern int do_benchmark;
 extern int do_benchmark_all;
+extern int do_profiling_all;
+extern int load_balance;
 extern int do_deinterlace;
 extern int do_hex_dump;
 extern int do_pkt_dump;
@@ -606,6 +620,7 @@ extern int frame_bits_per_raw_sample;
 extern AVIOContext *progress_avio;
 extern float max_error_rate;
 extern char *videotoolbox_pixfmt;
+extern int abr_pipeline;
 
 extern int filter_nbthreads;
 extern int filter_complex_nbthreads;
diff --git a/fftools/ffmpeg_filter.c b/fftools/ffmpeg_filter.c
old mode 100644
new mode 100755
index 72838de..16e2bb9
--- a/fftools/ffmpeg_filter.c
+++ b/fftools/ffmpeg_filter.c
@@ -197,6 +197,7 @@ DEF_CHOOSE_FORMAT(channel_layouts, uint64_t, channel_layout, channel_layouts, 0,
 int init_simple_filtergraph(InputStream *ist, OutputStream *ost)
 {
     FilterGraph *fg = av_mallocz(sizeof(*fg));
+    int i;
 
     if (!fg)
         exit_program(1);
@@ -225,6 +226,11 @@ int init_simple_filtergraph(InputStream *ist, OutputStream *ost)
     GROW_ARRAY(ist->filters, ist->nb_filters);
     ist->filters[ist->nb_filters - 1] = fg->inputs[0];
 
+    if (abr_pipeline) {
+        for (i = 0; i < ist->nb_filters; i++) {
+            ist->filters[i]->f_thread = 0;
+        }
+    }
     GROW_ARRAY(filtergraphs, nb_filtergraphs);
     filtergraphs[nb_filtergraphs - 1] = fg;
 
@@ -350,7 +356,10 @@ int init_complex_filtergraph(FilterGraph *fg)
         return AVERROR(ENOMEM);
     graph->nb_threads = 1;
 
-    ret = avfilter_graph_parse2(graph, fg->graph_desc, &inputs, &outputs);
+    if (av_finit_firstly_get() && fg->filter_inited_graph)
+        ret = avfilter_graph_parse2(fg->filter_inited_graph, fg->graph_desc, &inputs, &outputs);
+    else
+        ret = avfilter_graph_parse2(graph, fg->graph_desc, &inputs, &outputs);
     if (ret < 0)
         goto fail;
 
@@ -460,6 +469,7 @@ static int configure_output_video_filter(FilterGraph *fg, OutputFilter *ofilter,
     int pad_idx = out->pad_idx;
     int ret;
     char name[255];
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(ofilter->format);
 
     snprintf(name, sizeof(name), "out_%d_%d", ost->file_index, ost->index);
     ret = avfilter_graph_create_filter(&ofilter->filter,
@@ -469,7 +479,8 @@ static int configure_output_video_filter(FilterGraph *fg, OutputFilter *ofilter,
     if (ret < 0)
         return ret;
 
-    if (ofilter->width || ofilter->height) {
+    if ((ofilter->width || ofilter->height) &&
+        (!desc || !(desc->flags & AV_PIX_FMT_FLAG_HWACCEL))) {
         char args[255];
         AVFilterContext *filter;
         AVDictionaryEntry *e = NULL;
@@ -1010,8 +1021,11 @@ int configure_filtergraph(FilterGraph *fg)
                                       fg->graph_desc;
 
     cleanup_filtergraph(fg);
-    if (!(fg->graph = avfilter_graph_alloc()))
-        return AVERROR(ENOMEM);
+    if (!av_finit_firstly_get() || !fg->filter_inited_graph) {
+        if (!(fg->graph = avfilter_graph_alloc()))
+            return AVERROR(ENOMEM);
+    } else
+        fg->graph = fg->filter_inited_graph;
 
     if (simple) {
         OutputStream *ost = fg->outputs[0]->ost;
diff --git a/fftools/ffmpeg_opt.c b/fftools/ffmpeg_opt.c
old mode 100644
new mode 100755
index f5ca18a..ce3697e
--- a/fftools/ffmpeg_opt.c
+++ b/fftools/ffmpeg_opt.c
@@ -94,6 +94,9 @@ float frame_drop_threshold = 0;
 int do_deinterlace    = 0;
 int do_benchmark      = 0;
 int do_benchmark_all  = 0;
+int do_profiling_all  = 0;
+int load_balance      = 0;
+int finit_firstly     = 0;
 int do_hex_dump       = 0;
 int do_pkt_dump       = 0;
 int copy_ts           = 0;
@@ -110,6 +113,7 @@ float max_error_rate  = 2.0/3;
 int filter_nbthreads = 0;
 int filter_complex_nbthreads = 0;
 int vstats_version = 2;
+int abr_pipeline      = 0;
 
 
 static int intra_only         = 0;
@@ -2105,6 +2109,24 @@ static int init_complex_filters(void)
     return 0;
 }
 
+static int init_filters_firstly(void)
+{
+    int i, ret = 0;
+
+    for (i = 0; i < nb_filtergraphs; i++) {
+        if (!filtergraphs[i]->filter_inited_graph) {
+           filtergraphs[i]->filter_inited_graph = avfilter_graph_alloc();
+           if (!filtergraphs[i]->filter_inited_graph)
+               return AVERROR(ENOMEM);
+        }
+        ret = avfilter_graph_filter_init_firstly(filtergraphs[i]->filter_inited_graph,
+                filtergraphs[i]->graph_desc);
+        if (ret < 0)
+            return ret;
+    }
+    return 0;
+}
+
 static int open_output_file(OptionsContext *o, const char *filename)
 {
     AVFormatContext *oc;
@@ -3289,7 +3311,7 @@ int ffmpeg_parse_options(int argc, char **argv)
 {
     OptionParseContext octx;
     uint8_t error[128];
-    int ret;
+    int i, ret;
 
     memset(&octx, 0, sizeof(octx));
 
@@ -3311,6 +3333,18 @@ int ffmpeg_parse_options(int argc, char **argv)
     /* configure terminal and setup signal handlers */
     term_init();
 
+    if (finit_firstly) {
+        for (i = 0; i < nb_filtergraphs; i++)
+            if (!filtergraph_is_simple(filtergraphs[i])) {
+                av_finit_firstly_set(finit_firstly);
+                if (init_filters_firstly() < 0) {
+                    av_log(NULL, AV_LOG_FATAL, "Error initializing filters firstly.\n");
+                    goto fail;
+                }
+                break;
+            }
+    }
+
     /* open input files */
     ret = open_files(&octx.groups[GROUP_INFILE], "input", open_input_file);
     if (ret < 0) {
@@ -3334,6 +3368,14 @@ int ffmpeg_parse_options(int argc, char **argv)
 
     check_filter_outputs();
 
+    if (do_profiling_all) {
+        av_profiling_set(do_profiling_all);
+    }
+
+    if (load_balance) {
+        av_load_balance_set(load_balance);
+    }
+
 fail:
     uninit_parse_context(&octx);
     if (ret < 0) {
@@ -3436,6 +3478,12 @@ const OptionDef options[] = {
         "add timings for benchmarking" },
     { "benchmark_all",  OPT_BOOL | OPT_EXPERT,                       { &do_benchmark_all },
       "add timings for each task" },
+    { "profiling_all",  OPT_BOOL | OPT_EXPERT,                       { &do_profiling_all },
+      "print performance info based on all running pipelines" },
+    { "load_balance",   HAS_ARG | OPT_INT | OPT_EXPERT,              { &load_balance },
+      "enable the load balance, set the number of accumulated frames allowed" },
+    { "finit_firstly",  OPT_BOOL | OPT_EXPERT,                       { &finit_firstly },
+      "to init the filters in the filtergraph firstly not depend on stream input/output/link. filter_complex is required." },
     { "progress",       HAS_ARG | OPT_EXPERT,                        { .func_arg = opt_progress },
       "write program-readable progress information", "url" },
     { "stdin",          OPT_BOOL | OPT_EXPERT,                       { &stdin_interaction },
@@ -3542,7 +3590,10 @@ const OptionDef options[] = {
         "set the maximum number of queued packets from the demuxer" },
     { "find_stream_info", OPT_BOOL | OPT_PERFILE | OPT_INPUT | OPT_EXPERT, { &find_stream_info },
         "read and decode the streams to fill missing information with heuristics" },
-
+#if HAVE_THREADS
+    { "abr_pipeline",    OPT_BOOL,                                    { &abr_pipeline },
+        "adaptive bitrate pipeline (1 decode to N filter graphs, and 1 to N transcode" },
+#endif
     /* video options */
     { "vframes",      OPT_VIDEO | HAS_ARG  | OPT_PERFILE | OPT_OUTPUT,           { .func_arg = opt_video_frames },
         "set the number of video frames to output", "number" },
diff --git a/libavfilter/Makefile b/libavfilter/Makefile
old mode 100644
new mode 100755
index 455c809..0ce29b0
--- a/libavfilter/Makefile
+++ b/libavfilter/Makefile
@@ -29,6 +29,22 @@ OBJS-$(CONFIG_QSVVPP)                        += qsvvpp.o
 DNN-OBJS-$(CONFIG_LIBTENSORFLOW)             += dnn_backend_tf.o
 OBJS-$(CONFIG_DNN)                           += dnn_interface.o dnn_backend_native.o $(DNN-OBJS-yes)
 OBJS-$(CONFIG_SCENE_SAD)                     += scene_sad.o
+OBJS-$(CONFIG_IMAGE_INFERENCE)               += inference_backend/ff_base_inference.o             \
+                                                inference_backend/ff_inference_impl.o             \
+                                                inference_backend/ff_list.o                       \
+                                                inference_backend/ff_proc_factory.o               \
+                                                inference_backend/image.o                         \
+                                                inference_backend/image_inference.o               \
+                                                inference_backend/image_inference_async_preproc.o \
+                                                inference_backend/logger.o                        \
+                                                inference_backend/model_proc.o                    \
+                                                inference_backend/openvino_image_inference.o      \
+                                                inference_backend/pre_proc.o                      \
+                                                inference_backend/pre_proc_mocker.o               \
+                                                inference_backend/pre_proc_swscale.o              \
+                                                inference_backend/pre_proc_vaapi.o                \
+                                                inference_backend/safe_queue.o                    \
+                                                inference_backend/metaconverter.o                 \
 
 # audio filters
 OBJS-$(CONFIG_ABENCH_FILTER)                 += f_bench.o
@@ -269,6 +285,10 @@ OBJS-$(CONFIG_HWUPLOAD_FILTER)               += vf_hwupload.o
 OBJS-$(CONFIG_HYSTERESIS_FILTER)             += vf_hysteresis.o framesync.o
 OBJS-$(CONFIG_IDET_FILTER)                   += vf_idet.o
 OBJS-$(CONFIG_IL_FILTER)                     += vf_il.o
+OBJS-$(CONFIG_INFERENCE_IDENTIFY_FILTER)     += vf_inference_identify.o
+OBJS-$(CONFIG_INFERENCE_METACONVERT_FILTER)  += vf_inference_metaconvert.o
+OBJS-$(CONFIG_INFERENCE_CLASSIFY_FILTER)     += vf_inference_classify.o
+OBJS-$(CONFIG_INFERENCE_DETECT_FILTER)       += vf_inference_detect.o
 OBJS-$(CONFIG_INFLATE_FILTER)                += vf_neighbor.o
 OBJS-$(CONFIG_INTERLACE_FILTER)              += vf_tinterlace.o
 OBJS-$(CONFIG_INTERLEAVE_FILTER)             += f_interleave.o
@@ -307,6 +327,7 @@ OBJS-$(CONFIG_NORMALIZE_FILTER)              += vf_normalize.o
 OBJS-$(CONFIG_NULL_FILTER)                   += vf_null.o
 OBJS-$(CONFIG_OCR_FILTER)                    += vf_ocr.o
 OBJS-$(CONFIG_OCV_FILTER)                    += vf_libopencv.o
+OBJS-$(CONFIG_OCV_OVERLAY_FILTER)            += vf_ocv_overlay.o
 OBJS-$(CONFIG_OSCILLOSCOPE_FILTER)           += vf_datascope.o
 OBJS-$(CONFIG_OVERLAY_FILTER)                += vf_overlay.o framesync.o
 OBJS-$(CONFIG_OVERLAY_OPENCL_FILTER)         += vf_overlay_opencl.o opencl.o \
@@ -497,6 +518,7 @@ TOOLS-$(CONFIG_LIBZMQ) += zmqsend
 
 clean::
 	$(RM) $(CLEANSUFFIXES:%=libavfilter/libmpcodecs/%)
+	$(RM) $(CLEANSUFFIXES:%=libavfilter/inference_backend/%)
 
 OPENCL = $(subst $(SRC_PATH)/,,$(wildcard $(SRC_PATH)/libavfilter/opencl/*.cl))
 .SECONDARY: $(OPENCL:.cl=.c)
diff --git a/libavfilter/allfilters.c b/libavfilter/allfilters.c
old mode 100644
new mode 100755
index 04a3df7..31abaf3
--- a/libavfilter/allfilters.c
+++ b/libavfilter/allfilters.c
@@ -254,6 +254,10 @@ extern AVFilter ff_vf_hwupload_cuda;
 extern AVFilter ff_vf_hysteresis;
 extern AVFilter ff_vf_idet;
 extern AVFilter ff_vf_il;
+extern AVFilter ff_vf_inference_identify;
+extern AVFilter ff_vf_inference_metaconvert;
+extern AVFilter ff_vf_inference_classify;
+extern AVFilter ff_vf_inference_detect;
 extern AVFilter ff_vf_inflate;
 extern AVFilter ff_vf_interlace;
 extern AVFilter ff_vf_interleave;
@@ -292,6 +296,7 @@ extern AVFilter ff_vf_normalize;
 extern AVFilter ff_vf_null;
 extern AVFilter ff_vf_ocr;
 extern AVFilter ff_vf_ocv;
+extern AVFilter ff_vf_ocv_overlay;
 extern AVFilter ff_vf_oscilloscope;
 extern AVFilter ff_vf_overlay;
 extern AVFilter ff_vf_overlay_opencl;
diff --git a/libavfilter/avfilter.c b/libavfilter/avfilter.c
old mode 100644
new mode 100755
index 93e866b..8d8a42c
--- a/libavfilter/avfilter.c
+++ b/libavfilter/avfilter.c
@@ -44,6 +44,7 @@
 #include "internal.h"
 
 #include "libavutil/ffversion.h"
+#include "libavutil/time.h"
 const char av_filter_ffversion[] = "FFmpeg version " FFMPEG_VERSION;
 
 void ff_tlog_ref(void *ctx, AVFrame *ref, int end)
@@ -278,6 +279,7 @@ int avfilter_config_links(AVFilterContext *filter)
 {
     int (*config_link)(AVFilterLink *);
     unsigned i;
+    int64_t tm_init = 0;
     int ret;
 
     for (i = 0; i < filter->nb_inputs; i ++) {
@@ -365,13 +367,18 @@ int avfilter_config_links(AVFilterContext *filter)
                     return AVERROR(ENOMEM);
             }
 
-            if ((config_link = link->dstpad->config_props))
+            if ((config_link = link->dstpad->config_props)) {
+                if (av_profiling_get())
+                    tm_init = av_gettime();
                 if ((ret = config_link(link)) < 0) {
                     av_log(link->dst, AV_LOG_ERROR,
                            "Failed to configure input pad on %s\n",
                            link->dst->name);
                     return ret;
                 }
+                if (av_profiling_get())
+                    filter->init_working_time += av_gettime() - tm_init;
+            }
 
             link->init_state = AVLINK_INIT;
         }
@@ -925,6 +932,7 @@ int avfilter_init_str(AVFilterContext *filter, const char *args)
 {
     AVDictionary *options = NULL;
     AVDictionaryEntry *e;
+    int64_t tm_init;
     int ret = 0;
 
     if (args && *args) {
@@ -1015,7 +1023,12 @@ int avfilter_init_str(AVFilterContext *filter, const char *args)
         }
     }
 
+    if (av_profiling_get())
+        tm_init = av_gettime();
     ret = avfilter_init_dict(filter, &options);
+    if (av_profiling_get())
+        filter->init_working_time += av_gettime() - tm_init;
+
     if (ret < 0)
         goto fail;
 
@@ -1422,12 +1435,19 @@ int ff_filter_activate(AVFilterContext *filter)
 {
     int ret;
 
+    if (av_profiling_get())
+        filter->last_tm = av_gettime();
+
     /* Generic timeline support is not yet implemented but should be easy */
     av_assert1(!(filter->filter->flags & AVFILTER_FLAG_SUPPORT_TIMELINE_GENERIC &&
                  filter->filter->activate));
     filter->ready = 0;
     ret = filter->filter->activate ? filter->filter->activate(filter) :
           ff_filter_activate_default(filter);
+
+    if (av_profiling_get())
+        filter->sum_working_time += av_gettime() - filter->last_tm;
+
     if (ret == FFERROR_NOT_READY)
         ret = 0;
     return ret;
diff --git a/libavfilter/avfilter.h b/libavfilter/avfilter.h
old mode 100644
new mode 100755
index 9d70e71..7545883
--- a/libavfilter/avfilter.h
+++ b/libavfilter/avfilter.h
@@ -422,6 +422,8 @@ struct AVFilterContext {
      * configured.
      */
     int extra_hw_frames;
+
+    int64_t last_tm, init_working_time, sum_working_time;
 };
 
 /**
@@ -1096,6 +1098,8 @@ int avfilter_graph_parse2(AVFilterGraph *graph, const char *filters,
                           AVFilterInOut **inputs,
                           AVFilterInOut **outputs);
 
+int avfilter_graph_filter_init_firstly(AVFilterGraph *graph, const char *filters);
+
 /**
  * Send a command to one or more filter instances.
  *
@@ -1162,6 +1166,18 @@ char *avfilter_graph_dump(AVFilterGraph *graph, const char *options);
 int avfilter_graph_request_oldest(AVFilterGraph *graph);
 
 /**
+ * To get the filter chain queue frames occupation number.
+ *
+ * @return the number of buffered frames in the chain
+ */
+int avfilter_chain_occupation(AVFilterContext *avctx);
+
+/**
+ * Set parsed filter to be ready on a filter graph.
+ */
+int avfilter_graph_set_parsed(AVFilterGraph *graph);
+
+/**
  * @}
  */
 
diff --git a/libavfilter/avfiltergraph.c b/libavfilter/avfiltergraph.c
old mode 100644
new mode 100755
index a149f8f..072f8f0
--- a/libavfilter/avfiltergraph.c
+++ b/libavfilter/avfiltergraph.c
@@ -41,6 +41,7 @@
 #include "formats.h"
 #include "internal.h"
 #include "thread.h"
+#include "filters.h"
 
 #define OFFSET(x) offsetof(AVFilterGraph, x)
 #define F AV_OPT_FLAG_FILTERING_PARAM
@@ -298,6 +299,19 @@ AVFilterContext *avfilter_graph_get_filter(AVFilterGraph *graph, const char *nam
     return NULL;
 }
 
+int avfilter_graph_set_parsed(AVFilterGraph *graph)
+{
+    AVFilterContext *filter;
+    unsigned i;
+
+    for (i = 0; i < graph->nb_filters; i++)
+        if (!strncmp(graph->filters[i]->name, "Parsed", 6)) {
+            filter = graph->filters[i];
+            ff_filter_set_ready(filter, 300);
+        }
+    return 0;
+}
+
 static void sanitize_channel_layouts(void *log, AVFilterChannelLayouts *l)
 {
     if (!l)
@@ -1439,6 +1453,29 @@ int avfilter_graph_request_oldest(AVFilterGraph *graph)
     return 0;
 }
 
+int avfilter_chain_occupation(AVFilterContext *avctx)
+{
+    AVFilterLink *link = NULL;
+    int frm_num = 0;
+
+    if (!avctx)
+        return 0;
+
+    while (avctx) {
+        if (avctx->nb_inputs) {
+            link = avctx->inputs[0];
+            frm_num += ff_framequeue_queued_frames(&link->fifo);
+        }
+        if (avctx->nb_outputs) {
+            link = avctx->outputs[0];
+            avctx = link->dst;
+        } else
+            break;
+    }
+
+    return frm_num;
+}
+
 int ff_filter_graph_run_once(AVFilterGraph *graph)
 {
     AVFilterContext *filter;
diff --git a/libavfilter/graphparser.c b/libavfilter/graphparser.c
old mode 100644
new mode 100755
index d92b536..88469a4
--- a/libavfilter/graphparser.c
+++ b/libavfilter/graphparser.c
@@ -39,6 +39,8 @@ static int link_filter(AVFilterContext *src, int srcpad,
                        void *log_ctx)
 {
     int ret;
+    if (src->outputs[srcpad] && dst->inputs[dstpad] && av_finit_firstly_get())
+        return 0;
     if ((ret = avfilter_link(src, srcpad, dst, dstpad))) {
         av_log(log_ctx, AV_LOG_ERROR,
                "Cannot create the link %s:%d -> %s:%d\n",
@@ -195,6 +197,37 @@ static int parse_filter(AVFilterContext **filt_ctx, const char **buf, AVFilterGr
     return ret;
 }
 
+static int parse_created_filter(AVFilterContext **filt_ctx, const char **buf, AVFilterGraph *graph, int index)
+{
+    int i = 0;
+    char index_str[30];
+    char *opts = NULL;
+    char *name = av_get_token(buf, "=,;[");
+
+    if (!name)
+        return 0;
+
+    if (**buf == '=') {
+        (*buf)++;
+        opts = av_get_token(buf, "[],;");
+    }
+
+    snprintf(index_str, sizeof(index_str), "Parsed_%s_%d", name, index);
+    for (i = 0; i < graph->nb_filters; i++) {
+        if ((strcmp(graph->filters[i]->name, index_str) == 0)) {
+            *filt_ctx = graph->filters[i];
+            av_free(name);
+            av_free(opts);
+            return 0;
+        }
+    }
+
+    av_log(graph, AV_LOG_ERROR, "The created filter is not founded.\n");
+    av_free(name);
+    av_free(opts);
+    return AVERROR(EINVAL);
+}
+
 AVFilterInOut *avfilter_inout_alloc(void)
 {
     return av_mallocz(sizeof(AVFilterInOut));
@@ -404,6 +437,49 @@ static int parse_sws_flags(const char **buf, AVFilterGraph *graph)
     return 0;
 }
 
+int avfilter_graph_filter_init_firstly(AVFilterGraph *graph, const char *filters)
+{
+    //init filters only without input output link firstly
+    int index = 0, ret = 0;
+    char chr = 0;
+    char *p_filters, *desc;
+
+    desc = av_mallocz(strlen(filters) + 1);
+    if (!desc)
+        goto fail;
+    strncpy(desc, filters, strlen(filters));
+    desc[strlen(filters)] = '\0';
+    p_filters = desc;
+    p_filters += strspn(p_filters, WHITESPACES);
+
+    if ((ret = parse_sws_flags(&p_filters, graph)) < 0)
+        goto fail;
+
+    do {
+      AVFilterContext *filter;
+      p_filters += strspn(p_filters, WHITESPACES);
+
+      if ((ret = parse_filter(&filter, &p_filters, graph, index, graph)) < 0)
+          goto fail;
+
+      p_filters += strspn(p_filters, WHITESPACES);
+      chr = *p_filters++;
+      index++;
+    } while (chr == ',' || chr == ';');
+
+    if (desc)
+        av_free(desc);
+    return 0;
+
+fail:
+    while (graph->nb_filters)
+        avfilter_free(graph->filters[0]);
+    if (desc)
+        av_free(desc);
+    av_freep(&graph->filters);
+    return ret;
+}
+
 int avfilter_graph_parse2(AVFilterGraph *graph, const char *filters,
                           AVFilterInOut **inputs,
                           AVFilterInOut **outputs)
@@ -424,9 +500,13 @@ int avfilter_graph_parse2(AVFilterGraph *graph, const char *filters,
 
         if ((ret = parse_inputs(&filters, &curr_inputs, &open_outputs, graph)) < 0)
             goto end;
-        if ((ret = parse_filter(&filter, &filters, graph, index, graph)) < 0)
-            goto end;
-
+        if (!av_finit_firstly_get() || graph->nb_filters <= index) { //FIXME: must be complex
+            if ((ret = parse_filter(&filter, &filters, graph, index, graph)) < 0)
+                goto end;
+        } else {
+            if ((ret = parse_created_filter(&filter, &filters, graph, index)) < 0)
+                goto end;
+        }
 
         if ((ret = link_filter_inouts(filter, &curr_inputs, &open_inputs, graph)) < 0)
             goto end;
diff --git a/libavfilter/inference_backend/ff_base_inference.c b/libavfilter/inference_backend/ff_base_inference.c
new file mode 100644
index 0000000..add2bf8
--- /dev/null
+++ b/libavfilter/inference_backend/ff_base_inference.c
@@ -0,0 +1,140 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "ff_base_inference.h"
+#include "ff_inference_impl.h"
+#include "ff_proc_factory.h"
+#include "logger.h"
+#include <libavutil/avassert.h>
+#include <libavutil/mem.h>
+
+static const int log_levels[] = {AV_LOG_QUIET,   AV_LOG_ERROR, AV_LOG_WARNING, AV_LOG_INFO,
+                                 AV_LOG_VERBOSE, AV_LOG_DEBUG, AV_LOG_TRACE,   AV_LOG_PANIC};
+
+static void ff_log_function(int level, const char *file, const char *function, int line, const char *message) {
+    av_log(NULL, log_levels[level], "%s:%i : %s \t %s \n", file, line, function, message);
+}
+
+static void ff_trace_function(int level, const char *fmt, va_list vl) {
+    av_vlog(NULL, log_levels[level], fmt, vl);
+}
+
+FFBaseInference *av_base_inference_create(const char *inference_id) {
+    FFBaseInference *base_inference = (FFBaseInference *)av_mallocz(sizeof(*base_inference));
+    if (base_inference == NULL)
+        return NULL;
+
+    set_log_function(ff_log_function);
+    set_trace_function(ff_trace_function);
+
+    base_inference->inference_id = inference_id ? av_strdup(inference_id) : NULL;
+
+    base_inference->param.is_full_frame = TRUE; // default to true
+    base_inference->param.every_nth_frame = 1;  // default to 1
+    base_inference->param.nireq = 1;
+    base_inference->param.batch_size = 1;
+    base_inference->param.threshold = 0.5;
+    base_inference->param.vpp_device = VPP_DEVICE_SW;
+    base_inference->param.opaque = NULL;
+
+    base_inference->num_skipped_frames = UINT_MAX - 1; // always run inference on first frame
+
+    return base_inference;
+}
+
+int av_base_inference_init(FFBaseInference *base, FFInferenceParam *param) {
+    if (!base || !param)
+        return AVERROR(EINVAL);
+
+    if (base->initialized)
+        return 0;
+
+    base->param = *param;
+    base->inference = (void *)FFInferenceImplCreate(base);
+    base->crop_full_frame =
+        (!param->crop_rect.x && !param->crop_rect.y && !param->crop_rect.width && !param->crop_rect.height) ? FALSE
+                                                                                                            : TRUE;
+    base->initialized = TRUE;
+
+    return 0;
+}
+
+int av_base_inference_set_params(FFBaseInference *base, FFInferenceParam *param) {
+    if (!base || !param)
+        return AVERROR(EINVAL);
+
+    base->param = *param;
+    FFInferenceImplSetParams(base);
+    base->post_proc = (void *)getPostProcFunctionByName(base->inference_id, base->param.model);
+
+    return 0;
+}
+
+void av_base_inference_release(FFBaseInference *base) {
+    if (!base)
+        return;
+
+    if (base->inference) {
+        FFInferenceImplRelease((FFInferenceImpl *)base->inference);
+        base->inference = NULL;
+    }
+
+    if (base->inference_id) {
+        av_free((void *)base->inference_id);
+        base->inference_id = NULL;
+    }
+
+    av_free(base);
+}
+
+int av_base_inference_send_frame(void *ctx, FFBaseInference *base, AVFrame *frame_in) {
+    if (!base || !frame_in)
+        return AVERROR(EINVAL);
+
+    return FFInferenceImplAddFrame(ctx, (FFInferenceImpl *)base->inference, frame_in);
+}
+
+int av_base_inference_get_frame(void *ctx, FFBaseInference *base, AVFrame **frame_out) {
+    if (!base || !frame_out)
+        return AVERROR(EINVAL);
+
+    return FFInferenceImplGetFrame(ctx, (FFInferenceImpl *)base->inference, frame_out);
+}
+
+int av_base_inference_frame_queue_empty(void *ctx, FFBaseInference *base) {
+    if (!base)
+        return AVERROR(EINVAL);
+
+    return FFInferenceImplGetQueueSize(ctx, (FFInferenceImpl *)base->inference) == 0 ? TRUE : FALSE;
+}
+
+int av_base_inference_resource_status(void *ctx, FFBaseInference *base) {
+    if (!base)
+        return AVERROR(EINVAL);
+
+    return FFInferenceImplResourceStatus(ctx, (FFInferenceImpl *)base->inference);
+}
+
+void av_base_inference_send_event(void *ctx, FFBaseInference *base, FF_INFERENCE_EVENT event) {
+    if (!base)
+        return;
+
+    FFInferenceImplSinkEvent(ctx, (FFInferenceImpl *)base->inference, event);
+}
diff --git a/libavfilter/inference_backend/ff_base_inference.h b/libavfilter/inference_backend/ff_base_inference.h
new file mode 100644
index 0000000..c1cbf7e
--- /dev/null
+++ b/libavfilter/inference_backend/ff_base_inference.h
@@ -0,0 +1,241 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "image_inference.h"
+#include <libavutil/frame.h>
+#include <stdint.h>
+
+#if CONFIG_VAAPI
+#include <libavutil/hwcontext_vaapi.h>
+#endif
+
+typedef enum {
+    INFERENCE_EVENT_NONE,
+    INFERENCE_EVENT_EOS,
+} FF_INFERENCE_EVENT;
+
+typedef enum { VPP_DEVICE_SW, VPP_DEVICE_HW } VPPDevice;
+
+#ifndef TRUE
+/** The TRUE value of a UBool @stable ICU 2.0 */
+#define TRUE 1
+#endif
+
+#ifndef FALSE
+/** The FALSE value of a UBool @stable ICU 2.0 */
+#define FALSE 0
+#endif
+
+#ifndef UNUSED
+#define UNUSED(x) (void)(x)
+#endif
+
+#define MOCKER_PRE_PROC_MAGIC 0x47474747
+
+typedef struct __FFBaseInference FFBaseInference;
+typedef struct __FFInferenceParam FFInferenceParam;
+typedef struct __ProcessedFrame ProcessedFrame;
+typedef struct __InputPreproc InputPreproc;
+typedef struct __OutputPostproc OutputPostproc;
+typedef struct __InputPreproc ModelInputPreproc;
+typedef struct __ModelOutputPostproc ModelOutputPostproc;
+
+#define FF_INFERENCE_OPTIONS                                                                                           \
+    char *model;                                                                                                       \
+    char *object_class;                                                                                                \
+    char *model_proc;                                                                                                  \
+    char *device;                                                                                                      \
+    int batch_size;                                                                                                    \
+    int every_nth_frame;                                                                                               \
+    int nireq;                                                                                                         \
+    char *cpu_streams;                                                                                                 \
+    char *infer_config;                                                                                                \
+    float threshold;                                                                                                   \
+    int square_bbox;                                                                                                   \
+    int realtime_qos;
+
+struct __FFInferenceParam {
+    // exposed options
+    FF_INFERENCE_OPTIONS
+
+    VPPDevice vpp_device; // VPPDevice default:SW
+    void *opaque;         // VADisplay for vaapi
+
+    Rectangle crop_rect;
+    int is_full_frame;
+};
+
+struct __FFBaseInference {
+    // unique infer string id
+    const char *inference_id;
+
+    FFInferenceParam param;
+
+    // other fields
+    int initialized;
+    int crop_full_frame;    // crop needed for full frame
+    void *inference;        // type: FFInferenceImpl*
+    void *pre_proc;         // type: PreProcFunction
+    void *post_proc;        // type: PostProcFunction
+    void *get_roi_pre_proc; // type: GetROIPreProcFunction
+
+    unsigned int num_skipped_frames;
+};
+
+/* ROI for analytics */
+typedef struct __FFVideoRegionOfInterestMeta {
+    char type_name[16]; ///<! type name, e.g. face, vechicle etc.
+    unsigned int index; ///<! mark as the serial no. in side data
+
+    unsigned int x;
+    unsigned int y;
+    unsigned int w;
+    unsigned int h;
+} FFVideoRegionOfInterestMeta;
+
+/* model preproc */
+struct __InputPreproc {
+    int color_format;   ///<! input data format
+    char *layer_name;   ///<! layer name of input
+    char *object_class; ///<! interested object class
+};
+
+/* model postproc */
+struct __OutputPostproc {
+    char *layer_name;
+    char *converter;
+    char *attribute_name;
+    char *method;
+    double threshold;
+    double tensor_to_text_scale;
+    int tensor_to_text_precision;
+    AVBufferRef *labels;
+};
+
+typedef enum {
+    ANY = 0,
+    NCHW = 1,
+    NHWC = 2,
+} IELayout;
+
+typedef enum {
+    FP32 = 10,
+    U8 = 40,
+} IEPrecision;
+
+#define FF_TENSOR_MAX_RANK 8
+typedef struct _IETensorMeta {
+    const char *precision;           /**< tensor precision */
+    size_t ranks;                    /**< tensor rank */
+    size_t dims[FF_TENSOR_MAX_RANK]; /**< array describing tensor's dimensions */
+    const char *layout;              /**< tensor layout */
+    char *layer_name;                /**< tensor output layer name */
+    const char *model_name;          /**< model name */
+    AVBufferRef *buffer;             /**< tensor buffer */
+} IETensorMeta;
+
+/* dynamic labels array */
+typedef struct _LabelsArray {
+    char **label;
+    int num;
+} LabelsArray;
+
+typedef struct _InferDetection {
+    float x_min;
+    float y_min;
+    float x_max;
+    float y_max;
+    float confidence;
+    int label_id;
+    AVBufferRef *label_buf;
+    FFVideoRegionOfInterestMeta roi_meta;
+    IETensorMeta tensor;
+} InferDetection;
+
+/* dynamic bounding boxes array */
+typedef struct _BBoxesArray {
+    InferDetection **bbox;
+    int num;
+} BBoxesArray;
+
+typedef struct _InferDetectionMeta {
+    BBoxesArray *bboxes;
+} InferDetectionMeta;
+
+typedef struct __InferenceROI {
+    AVFrame *frame;
+    FFVideoRegionOfInterestMeta roi;
+} InferenceROI;
+
+typedef struct __InferenceROIArray {
+    InferenceROI **infer_ROIs;
+    int num_infer_ROIs;
+} InferenceROIArray;
+
+typedef struct InferClassification {
+    int detect_id; ///< detected bbox index
+    char *name;    ///< class name, e.g. emotion, age
+    char *attributes;
+    float confidence;
+    int label_id;           ///< label index in labels
+    AVBufferRef *label_buf; ///< label buffer
+    IETensorMeta tensor;
+} InferClassification;
+
+/* dynamic classifications array */
+typedef struct ClassifyArray {
+    InferClassification **classifications;
+    int num;
+} ClassifyArray;
+
+typedef struct InferClassificationMeta {
+    ClassifyArray *c_array;
+} InferClassificationMeta;
+
+#define MAX_MODEL_OUTPUT 4
+struct __ModelOutputPostproc {
+    OutputPostproc procs[MAX_MODEL_OUTPUT];
+};
+
+typedef void (*PostProcFunction)(const OutputBlobArray *output_blobs, InferenceROIArray *infer_roi_array,
+                                 ModelOutputPostproc *model_postproc, const char *model_name,
+                                 const FFBaseInference *ff_base_inference);
+
+FFBaseInference *av_base_inference_create(const char *inference_id);
+
+int av_base_inference_init(FFBaseInference *base, FFInferenceParam *param);
+
+int av_base_inference_set_params(FFBaseInference *base, FFInferenceParam *param);
+
+// TODO: add interface to set options separately
+
+void av_base_inference_release(FFBaseInference *base);
+
+int av_base_inference_send_frame(void *ctx, FFBaseInference *base, AVFrame *frame);
+
+int av_base_inference_get_frame(void *ctx, FFBaseInference *base, AVFrame **frame_out);
+
+int av_base_inference_frame_queue_empty(void *ctx, FFBaseInference *base);
+
+int av_base_inference_resource_status(void *ctx, FFBaseInference *base);
+
+void av_base_inference_send_event(void *ctx, FFBaseInference *base, FF_INFERENCE_EVENT event);
diff --git a/libavfilter/inference_backend/ff_inference_impl.c b/libavfilter/inference_backend/ff_inference_impl.c
new file mode 100644
index 0000000..5a0e918
--- /dev/null
+++ b/libavfilter/inference_backend/ff_inference_impl.c
@@ -0,0 +1,581 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "ff_inference_impl.h"
+#include "ff_base_inference.h"
+#include "ff_list.h"
+#include "image_inference.h"
+#include "logger.h"
+#include "model_proc.h"
+#include <libavutil/avassert.h>
+#include <libavutil/log.h>
+#include <pthread.h>
+
+typedef enum {
+    INFERENCE_EXECUTED = 1,
+    INFERENCE_SKIPPED_PER_PROPERTY = 2, // frame skipped due to every-nth-frame set to value greater than 1
+    INFERENCE_SKIPPED_REALTIME_QOS = 3, // frame skipped due to realtime-qos policy
+    INFERENCE_SKIPPED_ROI = 4           // roi skipped because is_roi_classification_needed() returned false
+} InferenceStatus;
+
+typedef struct __Model {
+    const char *name;
+    char *object_class;
+    ImageInferenceContext *infer_ctx;
+    FFInferenceImpl *infer_impl;
+    // std::map<std::string, void *> proc;
+    void *input_preproc;
+
+    void *proc_config;
+    ModelInputPreproc model_preproc;
+    ModelOutputPostproc model_postproc;
+} Model;
+
+typedef struct __ROIMetaArray {
+    FFVideoRegionOfInterestMeta **roi_metas;
+    int num_metas;
+} ROIMetaArray;
+
+/* \brief output frames stored in queue */
+typedef struct __OutputFrame {
+    AVFrame *frame;
+    AVFrame *writable_frame;
+    int inference_count;
+} OutputFrame;
+
+/* \brief structure taken as IFramPtr carried by \func SubmitImage */
+typedef struct __InferenceResult {
+    InferenceROI inference_frame;
+    Model *model;
+} InferenceResult;
+
+struct __FFInferenceImpl {
+    int frame_num;
+    pthread_mutex_t _mutex; // Maybe not necessary for ffmpeg
+    const FFBaseInference *base_inference;
+
+    Model *model;
+
+    // output frames list
+    pthread_mutex_t output_frames_mutex;
+    ff_list_t *output_frames;
+    ff_list_t *processed_frames; // TODO: consider remove it if all output frames can be consumed instantly
+};
+
+static void SplitString(char *str, const char *delim, char **array, int *num, int max) {
+    char *p;
+    int i = 0;
+
+    if (!str || !delim || !array || !num)
+        return;
+
+    while (p = strtok(str, delim)) {
+        int j = 0;
+        char *s;
+        size_t end;
+
+        /* remove head blanks */
+        while (p[j] == '\n' || p[j] == ' ')
+            j++;
+
+        if (!p[j])
+            continue;
+
+        /* remove tail blanks */
+        s = p + j;
+        end = strlen(s) - 1;
+        while (s[end] == '\n' || s[end] == ' ')
+            s[end--] = '\0';
+
+        array[i++] = s;
+        av_assert0(i < max);
+
+        /* string is cached */
+        str = NULL;
+    }
+
+    *num = i;
+}
+
+static inline int avFormatToFourCC(int format) {
+    switch (format) {
+    case AV_PIX_FMT_NV12:
+        VAII_DEBUG("AV_PIX_FMT_NV12");
+        return FOURCC_NV12;
+    case AV_PIX_FMT_BGR0:
+        VAII_DEBUG("AV_PIX_FMT_BGR0");
+        return FOURCC_BGRX;
+    case AV_PIX_FMT_BGRA:
+        VAII_DEBUG("AV_PIX_FMT_BGRA");
+        return FOURCC_BGRA;
+    case AV_PIX_FMT_BGR24:
+        VAII_DEBUG("AV_PIX_FMT_BGR24");
+        return FOURCC_BGR;
+    case AV_PIX_FMT_RGBP:
+        VAII_DEBUG("AV_PIX_FMT_RGBP");
+        return FOURCC_RGBP;
+    case AV_PIX_FMT_YUV420P:
+        VAII_DEBUG("AV_PIX_FMT_YUV420P");
+        return FOURCC_I420;
+    case AV_PIX_FMT_VAAPI:
+        VAII_DEBUG("AV_PIX_FMT_VAAPI");
+        return FOURCC_RGBP;
+    }
+
+    VAII_LOGE("Unsupported AV Format: %d.", format);
+    return 0;
+}
+
+static void ff_buffer_map(AVFrame *frame, Image *image, MemoryType memoryType) {
+    const int n_planes = 4;
+
+    image->type = memoryType;
+    image->format = avFormatToFourCC(frame->format);
+    image->width = frame->width;
+    image->height = frame->height;
+    for (int i = 0; i < n_planes; i++) {
+        image->stride[i] = frame->linesize[i];
+    }
+
+#ifdef CONFIG_VAAPI
+    if (memoryType == MEM_TYPE_VAAPI) {
+        image->surface_id = (uint32_t)frame->data[3];
+        image->colorspace = frame->colorspace;
+    }
+#endif
+    if (memoryType == MEM_TYPE_SYSTEM) {
+        for (int i = 0; i < n_planes; i++) {
+            image->planes[i] = frame->data[i];
+        }
+    }
+}
+
+static int CheckObjectClass(const char *requested, const InferDetection *detection) {
+    LabelsArray *label_array = NULL;
+    if (!requested)
+        return 1;
+
+    if (!detection->label_buf)
+        return 1;
+
+    label_array = (LabelsArray *)detection->label_buf->data;
+    av_assert0(detection->label_id < label_array->num);
+
+    return !strcmp(requested, label_array->label[detection->label_id]) ? 1 : 0;
+}
+
+static inline void PushOutput(FFInferenceImpl *impl) {
+    ff_list_t *out = impl->output_frames;
+    ff_list_t *processed = impl->processed_frames;
+    while (!out->empty(out)) {
+        OutputFrame *front = (OutputFrame *)out->front(out);
+        AVFrame *frame = front->frame;
+        if (front->inference_count > 0) {
+            break; // inference not completed yet
+        }
+
+        processed->push_back(processed, frame);
+        out->pop_front(out);
+        av_free(front);
+    }
+}
+
+static void InferenceCompletionCallback(OutputBlobArray *blobs, UserDataBuffers *user_data) {
+    Model *model = NULL;
+    FFInferenceImpl *impl = NULL;
+    InferenceResult *result = NULL;
+    const FFBaseInference *base = NULL;
+    InferenceROIArray inference_frames_array = {};
+
+    if (0 == user_data->num_buffers)
+        return;
+
+    result = (InferenceResult *)user_data->frames[0];
+    model = result->model;
+    impl = model->infer_impl;
+    base = impl->base_inference;
+
+    for (int i = 0; i < user_data->num_buffers; i++) {
+        result = (InferenceResult *)user_data->frames[i];
+        av_dynarray_add(&inference_frames_array.infer_ROIs, &inference_frames_array.num_infer_ROIs,
+                        &result->inference_frame);
+    }
+
+    if (base->post_proc) {
+        ((PostProcFunction)base->post_proc)(blobs, &inference_frames_array, &model->model_postproc, model->name, base);
+    }
+
+    pthread_mutex_lock(&impl->output_frames_mutex);
+
+    for (int i = 0; i < inference_frames_array.num_infer_ROIs; i++) {
+        OutputFrame *output;
+        ff_list_t *out = impl->output_frames;
+        InferenceROI *frame = inference_frames_array.infer_ROIs[i];
+        iterator it = out->iterator_get(out);
+        while (it) {
+            output = (OutputFrame *)out->iterate_value(it);
+            if (frame->frame == output->frame || frame->frame == output->writable_frame) {
+                output->inference_count--;
+                break;
+            }
+            it = out->iterate_next(out, it);
+        }
+    }
+
+    PushOutput(impl);
+
+    for (int i = 0; i < user_data->num_buffers; i++)
+        av_free(user_data->frames[i]);
+
+    pthread_mutex_unlock(&impl->output_frames_mutex);
+
+    av_free(inference_frames_array.infer_ROIs);
+}
+
+static int ConfigPreProc(FFBaseInference *base, FFInferenceImpl *impl) {
+    ImageInferenceContext *context = NULL;
+    Model *model = impl->model;
+    if (!model)
+        return 0;
+
+    context = model->infer_ctx;
+
+    // Create async pre_proc image inference backend
+    if (base->param.opaque) {
+        PreProcContext *preproc_ctx = NULL;
+        ImageInferenceContext *async_preproc_ctx = NULL;
+
+        const ImageInference *inference = image_inference_get_by_name("async_preproc");
+        async_preproc_ctx = image_inference_alloc(inference, NULL, "async-preproc-infer");
+        if (base->param.vpp_device == VPP_DEVICE_HW)
+            preproc_ctx = pre_proc_alloc(pre_proc_get_by_name("vaapi"));
+        else
+            preproc_ctx = pre_proc_alloc(pre_proc_get_by_name("mocker"));
+
+        av_assert0(async_preproc_ctx && preproc_ctx);
+
+        async_preproc_ctx->inference->CreateAsyncPreproc(async_preproc_ctx, context, preproc_ctx, 6,
+                                                         base->param.opaque);
+
+        // substitute for opevino image inference
+        context = async_preproc_ctx;
+    }
+
+    model->infer_ctx = context;
+    model->name = context->inference->GetModelName(context);
+
+    return 0;
+}
+
+static Model *CreateModel(FFBaseInference *base, const char *model_file, const char *model_proc_path,
+                          const char *object_class) {
+    int ret = 0;
+    Model *model = NULL;
+    const ImageInference *inference = image_inference_get_by_name("openvino");
+    const OutputBlobMethod *method = output_blob_method_get_by_name("openvino");
+    ImageInferenceContext *context = NULL;
+
+    VAII_LOGI("Loading model: device=%s, path=%s\n", base->param.device, model_file);
+    VAII_LOGI("Setting batch_size=%d, nireq=%d\n", base->param.batch_size, base->param.nireq);
+
+    context = image_inference_alloc(inference, method, "ffmpeg-image-infer");
+    model = (Model *)av_mallocz(sizeof(*model));
+    av_assert0(context && model);
+
+    if (model_proc_path) {
+        void *proc = model_proc_read_config_file(model_proc_path);
+        if (!proc) {
+            VAII_LOGE("Could not read proc config file:"
+                   "%s\n",
+                   model_proc_path);
+            av_assert0(proc);
+        }
+
+        if (model_proc_parse_input_preproc(proc, &model->model_preproc) < 0) {
+            VAII_ERROR("Parse input preproc error.\n");
+        }
+
+        if (model_proc_parse_output_postproc(proc, &model->model_postproc) < 0) {
+            VAII_ERROR("Parse output postproc error.\n");
+        }
+
+        model->proc_config = proc;
+    }
+
+    ret = context->inference->Create(context, MEM_TYPE_ANY, base->param.device, model_file, base->param.batch_size,
+                                     base->param.nireq, base->param.infer_config, NULL, InferenceCompletionCallback);
+    av_assert0(ret == 0);
+
+    model->infer_ctx = context;
+    model->name = context->inference->GetModelName(context);
+    model->object_class = object_class ? av_strdup(object_class) : NULL;
+    model->input_preproc = NULL;
+
+    return model;
+}
+
+static void ReleaseModel(Model *model) {
+    ImageInferenceContext *ii_ctx;
+    if (!model)
+        return;
+
+    ii_ctx = model->infer_ctx;
+    ii_ctx->inference->Close(ii_ctx);
+    image_inference_free(ii_ctx);
+
+    model_proc_release_model_proc(model->proc_config, &model->model_preproc, &model->model_postproc);
+
+    if (model->object_class)
+        av_free(model->object_class);
+    av_free(model);
+}
+
+static void SubmitImage(Model *model, FFVideoRegionOfInterestMeta *meta, Image *image, AVFrame *frame) {
+    ImageInferenceContext *s = model->infer_ctx;
+    PreProcessor preProcessFunction = NULL;
+
+    InferenceResult *result = (InferenceResult *)malloc(sizeof(*result));
+    av_assert0(result);
+    result->inference_frame.frame = frame;
+    result->inference_frame.roi = *meta;
+    result->model = model;
+
+    image->rect = (Rectangle){.x = (int)meta->x, .y = (int)meta->y, .width = (int)meta->w, .height = (int)meta->h};
+#if 0
+    if (ff_base_inference->pre_proc && model.input_preproc) {
+        preProcessFunction = [&](InferenceBackend::Image &image) {
+            ((PreProcFunction)ff_base_inference->pre_proc)(model.input_preproc, image);
+        };
+    }
+    if (ff_base_inference->get_roi_pre_proc && model.input_preproc) {
+        preProcessFunction = ((GetROIPreProcFunction)ff_base_inference->get_roi_pre_proc)(model.input_preproc, meta);
+    }
+#endif
+    s->inference->SubmitImage(s, image, (IFramePtr)result, preProcessFunction);
+}
+
+static int SubmitImages(FFInferenceImpl *impl, const ROIMetaArray *metas, AVFrame *frame) {
+    int ret = 0;
+    Image image = {};
+
+    // TODO: map frame w/ different memory type to image
+    // BufferMapContext mapContext;
+
+    // map to the image according to the mem type
+    ff_buffer_map(frame, &image, frame->hw_frames_ctx ? MEM_TYPE_VAAPI : MEM_TYPE_SYSTEM);
+
+    for (int i = 0; i < metas->num_metas; i++) {
+        SubmitImage(impl->model, metas->roi_metas[i], &image, frame);
+    }
+
+    // ff_buffer_unmap(buffer, image, mapContext);
+
+    return ret;
+}
+
+FFInferenceImpl *FFInferenceImplCreate(FFBaseInference *ff_base_inference) {
+    Model *dnn_model = NULL;
+    FFInferenceImpl *impl = (FFInferenceImpl *)av_mallocz(sizeof(*impl));
+
+    av_assert0(impl && ff_base_inference && ff_base_inference->param.model);
+
+    dnn_model = CreateModel(ff_base_inference, ff_base_inference->param.model, ff_base_inference->param.model_proc,
+                            ff_base_inference->param.object_class);
+    dnn_model->infer_impl = impl;
+
+    impl->model = dnn_model;
+    impl->base_inference = ff_base_inference;
+    impl->output_frames = ff_list_alloc();
+    impl->processed_frames = ff_list_alloc();
+
+    av_assert0(impl->output_frames && impl->processed_frames);
+
+    pthread_mutex_init(&impl->_mutex, NULL);
+    pthread_mutex_init(&impl->output_frames_mutex, NULL);
+
+    return impl;
+}
+
+int FFInferenceImplSetParams(FFBaseInference *ff_base_inference) {
+    av_assert0(ff_base_inference);
+    FFInferenceImpl *impl = (FFInferenceImpl *)ff_base_inference->inference;
+    av_assert0(impl);
+
+    // here currently mainly about preproc
+    ConfigPreProc(ff_base_inference, impl);
+
+    return 0;
+}
+
+void FFInferenceImplRelease(FFInferenceImpl *impl) {
+    if (!impl)
+        return;
+
+    ReleaseModel(impl->model);
+
+    ff_list_free(impl->output_frames);
+    ff_list_free(impl->processed_frames);
+
+    pthread_mutex_destroy(&impl->_mutex);
+    pthread_mutex_destroy(&impl->output_frames_mutex);
+
+    av_free(impl);
+}
+
+int FFInferenceImplAddFrame(void *ctx, FFInferenceImpl *impl, AVFrame *frame) {
+    FFBaseInference *base_inference = (FFBaseInference *)impl->base_inference;
+    ROIMetaArray metas = {};
+    FFVideoRegionOfInterestMeta full_frame_meta = {};
+    int inference_count = 0;
+
+    InferenceStatus status = INFERENCE_EXECUTED;
+    if (++base_inference->num_skipped_frames < base_inference->param.every_nth_frame) {
+        status = INFERENCE_SKIPPED_PER_PROPERTY;
+    }
+
+    if (base_inference->param.realtime_qos) {
+        ImageInferenceContext *ii_ctx = impl->model->infer_ctx;
+        if (ii_ctx->inference->IsQueueFull(ii_ctx)) {
+            status = INFERENCE_SKIPPED_REALTIME_QOS;
+        }
+    }
+
+    if (status == INFERENCE_EXECUTED) {
+        base_inference->num_skipped_frames = 0;
+    }
+
+    // Collect all ROI metas into ROIMetaArray
+    if (base_inference->param.is_full_frame) {
+        if (base_inference->crop_full_frame) {
+            full_frame_meta.x = base_inference->param.crop_rect.x;
+            full_frame_meta.y = base_inference->param.crop_rect.y;
+            full_frame_meta.w = base_inference->param.crop_rect.width;
+            full_frame_meta.h = base_inference->param.crop_rect.height;
+            full_frame_meta.index = 0;
+        } else {
+            full_frame_meta.x = 0;
+            full_frame_meta.y = 0;
+            full_frame_meta.w = frame->width;
+            full_frame_meta.h = frame->height;
+            full_frame_meta.index = 0;
+        }
+        av_dynarray_add(&metas.roi_metas, &metas.num_metas, &full_frame_meta);
+    } else {
+        BBoxesArray *bboxes = NULL;
+        InferDetectionMeta *detect_meta = NULL;
+        AVFrameSideData *side_data = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_DETECTION);
+        if (side_data) {
+            detect_meta = (InferDetectionMeta *)(side_data->data);
+            av_assert0(detect_meta);
+            bboxes = detect_meta->bboxes;
+            if (bboxes) {
+                ModelInputPreproc *model_preproc = &impl->model->model_preproc;
+                for (int i = 0; i < bboxes->num; i++) {
+                    FFVideoRegionOfInterestMeta *roi_meta = NULL;
+                    if (!CheckObjectClass(model_preproc->object_class, bboxes->bbox[i]))
+                        continue;
+                    roi_meta = (FFVideoRegionOfInterestMeta *)av_malloc(sizeof(*roi_meta));
+                    if (roi_meta == NULL)
+                        goto exit;
+                    memcpy(roi_meta, &bboxes->bbox[i]->roi_meta, sizeof(*roi_meta));
+                    roi_meta->index = i;
+                    av_dynarray_add(&metas.roi_metas, &metas.num_metas, roi_meta);
+                }
+            }
+        }
+    }
+
+    // count number ROIs to run inference on
+    inference_count = (status == INFERENCE_EXECUTED) ? metas.num_metas : 0;
+    impl->frame_num++;
+
+    // push into output_frames queue
+    {
+        OutputFrame *output_frame;
+        ff_list_t *output = impl->output_frames;
+        ff_list_t *processed = impl->processed_frames;
+        pthread_mutex_lock(&impl->output_frames_mutex);
+
+        if (!inference_count && output->empty(output)) {
+            processed->push_back(processed, frame);
+            pthread_mutex_unlock(&impl->output_frames_mutex);
+            goto exit;
+        }
+
+        output_frame = (OutputFrame *)av_malloc(sizeof(*output_frame));
+        if (output_frame == NULL) {
+            pthread_mutex_unlock(&impl->output_frames_mutex);
+            goto exit;
+        }
+        output_frame->frame = frame;
+        output_frame->writable_frame = NULL; // TODO: alloc new frame if not writable
+        output_frame->inference_count = inference_count;
+        impl->output_frames->push_back(impl->output_frames, output_frame);
+
+        if (!inference_count) {
+            // If we don't need to run inference and there are no frames queued for inference then finish transform
+            pthread_mutex_unlock(&impl->output_frames_mutex);
+            goto exit;
+        }
+
+        pthread_mutex_unlock(&impl->output_frames_mutex);
+    }
+
+    SubmitImages(impl, &metas, frame);
+
+exit:
+    if (!base_inference->param.is_full_frame) {
+        for (int n = 0; n < metas.num_metas; n++)
+            av_free(metas.roi_metas[n]);
+    }
+    av_free(metas.roi_metas);
+    return 0;
+}
+
+int FFInferenceImplGetFrame(void *ctx, FFInferenceImpl *impl, AVFrame **frame) {
+    ff_list_t *l = impl->processed_frames;
+
+    if (l->empty(l) || !frame)
+        return AVERROR(EAGAIN);
+
+    pthread_mutex_lock(&impl->output_frames_mutex);
+    *frame = (AVFrame *)l->front(l);
+    l->pop_front(l);
+    pthread_mutex_unlock(&impl->output_frames_mutex);
+
+    return 0;
+}
+
+size_t FFInferenceImplGetQueueSize(void *ctx, FFInferenceImpl *impl) {
+    ff_list_t *out = impl->output_frames;
+    ff_list_t *pro = impl->processed_frames;
+    VAII_LOGI("output:%zu processed:%zu\n", out->size(out), pro->size(pro));
+    return out->size(out) + pro->size(pro);
+}
+
+size_t FFInferenceImplResourceStatus(void *ctx, FFInferenceImpl *impl) {
+    return impl->model->infer_ctx->inference->ResourceStatus(impl->model->infer_ctx);
+}
+
+void FFInferenceImplSinkEvent(void *ctx, FFInferenceImpl *impl, FF_INFERENCE_EVENT event) {
+    if (event == INFERENCE_EVENT_EOS) {
+        impl->model->infer_ctx->inference->Flush(impl->model->infer_ctx);
+    }
+}
diff --git a/libavfilter/inference_backend/ff_inference_impl.h b/libavfilter/inference_backend/ff_inference_impl.h
new file mode 100644
index 0000000..e1c1fcf
--- /dev/null
+++ b/libavfilter/inference_backend/ff_inference_impl.h
@@ -0,0 +1,41 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "ff_base_inference.h"
+
+typedef struct __FFInferenceImpl FFInferenceImpl;
+
+FFInferenceImpl *FFInferenceImplCreate(FFBaseInference *ff_base_inference);
+
+int FFInferenceImplSetParams(FFBaseInference *ff_base_inference);
+
+void FFInferenceImplRelease(FFInferenceImpl *impl);
+
+int FFInferenceImplAddFrame(void *ctx, FFInferenceImpl *impl, AVFrame *frame);
+
+int FFInferenceImplGetFrame(void *ctx, FFInferenceImpl *impl, AVFrame **frame);
+
+size_t FFInferenceImplGetQueueSize(void *ctx, FFInferenceImpl *impl);
+
+size_t FFInferenceImplResourceStatus(void *ctx, FFInferenceImpl *impl);
+
+void FFInferenceImplSinkEvent(void *ctx, FFInferenceImpl *impl, FF_INFERENCE_EVENT event);
diff --git a/libavfilter/inference_backend/ff_list.c b/libavfilter/inference_backend/ff_list.c
new file mode 100644
index 0000000..33b00d7
--- /dev/null
+++ b/libavfilter/inference_backend/ff_list.c
@@ -0,0 +1,93 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "ff_list.h"
+#include "queue.c"
+
+static void *_ff_list_pop_back(void *thiz) {
+    return queue_pop_back((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+static void *_ff_list_pop_front(void *thiz) {
+    return queue_pop_front((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+static void _ff_list_push_back(void *thiz, void *item) {
+    queue_push_back((queue_t *)((ff_list_t *)thiz)->opaque, item);
+}
+
+static void _ff_list_push_front(void *thiz, void *item) {
+    queue_push_front((queue_t *)((ff_list_t *)thiz)->opaque, item);
+}
+
+static void *_ff_list_front(void *thiz) {
+    return queue_peek_front((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+static int _ff_list_empty(void *thiz) {
+    return queue_count((queue_t *)((ff_list_t *)thiz)->opaque) == 0;
+}
+
+static iterator _ff_list_iterator_get(void *thiz) {
+    return queue_iterate((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+static iterator _ff_list_iterate_next(void *thiz, iterator it) {
+    return queue_iterate_next((queue_t *)((ff_list_t *)thiz)->opaque, (queue_entry_t *)it);
+}
+
+static void *_ff_list_iterate_value(iterator it) {
+    return queue_iterate_value((queue_entry_t *)it);
+}
+
+static size_t _ff_list_size(void *thiz) {
+    return queue_count((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+ff_list_t *ff_list_alloc(void) {
+    ff_list_t *thiz = (ff_list_t *)malloc(sizeof(*thiz));
+    if (!thiz)
+        return NULL;
+
+    queue_t *q = queue_create();
+    assert(q);
+
+    thiz->opaque = q;
+    thiz->size = _ff_list_size;
+    thiz->empty = _ff_list_empty;
+    thiz->front = _ff_list_front;
+    thiz->pop_back = _ff_list_pop_back;
+    thiz->pop_front = _ff_list_pop_front;
+    thiz->push_back = _ff_list_push_back;
+    thiz->push_front = _ff_list_push_front;
+    thiz->iterator_get = _ff_list_iterator_get;
+    thiz->iterate_next = _ff_list_iterate_next;
+    thiz->iterate_value = _ff_list_iterate_value;
+
+    return thiz;
+}
+
+void ff_list_free(ff_list_t *thiz) {
+    if (!thiz)
+        return;
+
+    queue_destroy((queue_t *)thiz->opaque);
+    free(thiz);
+}
\ No newline at end of file
diff --git a/libavfilter/inference_backend/ff_list.h b/libavfilter/inference_backend/ff_list.h
new file mode 100644
index 0000000..22e77a6
--- /dev/null
+++ b/libavfilter/inference_backend/ff_list.h
@@ -0,0 +1,55 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef __FF_LIST_H
+#define __FF_LIST_H
+
+typedef void *iterator;
+
+typedef struct __ff_list {
+    const void *opaque; // private data
+
+    void *(*pop_back)(void *thiz);
+
+    void *(*pop_front)(void *thiz);
+
+    void (*push_back)(void *thiz, void *item);
+
+    void (*push_front)(void *thiz, void *item);
+
+    void *(*front)(void *thiz);
+
+    void *(*next)(void *thiz, void *current);
+
+    int (*empty)(void *thiz);
+
+    unsigned long (*size)(void *thiz);
+
+    iterator (*iterator_get)(void *thiz);
+
+    iterator (*iterate_next)(void *thiz, iterator it);
+
+    void *(*iterate_value)(iterator it);
+} ff_list_t;
+
+ff_list_t *ff_list_alloc(void);
+void ff_list_free(ff_list_t *thiz);
+
+#endif // __FF_LIST_H
diff --git a/libavfilter/inference_backend/ff_proc_factory.c b/libavfilter/inference_backend/ff_proc_factory.c
new file mode 100755
index 0000000..ded8a5b
--- /dev/null
+++ b/libavfilter/inference_backend/ff_proc_factory.c
@@ -0,0 +1,804 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "ff_proc_factory.h"
+#include "logger.h"
+#include <libavutil/avassert.h>
+#include <math.h>
+
+#define ENUM_STRING_PAIR(x)                                                                                            \
+    { x, #x }
+
+struct _precision {
+    IEPrecision value;
+    const char *str;
+};
+static struct _precision precision_table[] = {
+    ENUM_STRING_PAIR(FP32),
+    ENUM_STRING_PAIR(U8),
+};
+
+struct _layout {
+    IELayout value;
+    const char *str;
+};
+static struct _layout layout_table[] = {
+    ENUM_STRING_PAIR(ANY),
+    ENUM_STRING_PAIR(NCHW),
+    ENUM_STRING_PAIR(NHWC),
+};
+
+static inline const char *get_precision_string(int value) {
+    int size = sizeof(precision_table) / sizeof(precision_table[0]);
+    for (int i = 0; i < size; i++)
+        if (precision_table[i].value == (IEPrecision)value)
+            return precision_table[i].str;
+    return NULL;
+}
+
+static inline const char *get_layout_string(int value) {
+    int size = sizeof(layout_table) / sizeof(layout_table[0]);
+    for (int i = 0; i < size; i++)
+        if (layout_table[i].value == (IELayout)value)
+            return layout_table[i].str;
+    return NULL;
+}
+
+static void infer_detect_metadata_buffer_free(void *opaque, uint8_t *data) {
+    BBoxesArray *bboxes = ((InferDetectionMeta *)data)->bboxes;
+
+    if (bboxes) {
+        int i;
+        for (i = 0; i < bboxes->num; i++) {
+            InferDetection *p = bboxes->bbox[i];
+            av_buffer_unref(&p->label_buf);
+            av_buffer_unref(&p->tensor.buffer);
+            if (p->tensor.layer_name)
+                av_freep(&p->tensor.layer_name);
+            av_freep(&p);
+        }
+        av_free(bboxes->bbox);
+        av_freep(&bboxes);
+    }
+
+    av_free(data);
+}
+
+static void infer_classify_metadata_buffer_free(void *opaque, uint8_t *data) {
+    int i;
+    InferClassificationMeta *meta = (InferClassificationMeta *)data;
+    ClassifyArray *classes = meta->c_array;
+
+    if (classes) {
+        for (i = 0; i < classes->num; i++) {
+            InferClassification *c = classes->classifications[i];
+            av_freep(&c->attributes);
+            av_buffer_unref(&c->label_buf);
+            av_buffer_unref(&c->tensor.buffer);
+            if (c->tensor.layer_name)
+                av_freep(&c->tensor.layer_name);
+            av_freep(&c);
+        }
+        av_free(classes->classifications);
+        av_freep(&classes);
+    }
+
+    av_free(data);
+}
+
+static inline void enhanced_face_bounding_box(FFVideoRegionOfInterestMeta *roi) {
+    const float bb_enlarge_coefficient = 1.2;
+    const float bb_dx_coefficient = 1.0;
+    const float bb_dy_coefficient = 1.0;
+
+    // Make square and enlarge face bounding box for more robust operation of face analytics networks
+    int bb_width = roi->w;
+    int bb_height = roi->h;
+
+    int bb_center_x = roi->x + bb_width / 2;
+    int bb_center_y = roi->y + bb_height / 2;
+
+    int max_of_sizes = fmax(bb_width, bb_height);
+
+    int bb_new_width = (int)(bb_enlarge_coefficient * max_of_sizes);
+    int bb_new_height = (int)(bb_enlarge_coefficient * max_of_sizes);
+
+    roi->x = bb_center_x - (int)(floor(bb_dx_coefficient * bb_new_width / 2));
+    roi->y = bb_center_y - (int)(floor(bb_dy_coefficient * bb_new_height / 2));
+
+    roi->w = bb_new_width;
+    roi->h = bb_new_height;
+}
+
+static void inline fill_tensor_metadata(IETensorMeta *tensor, const char *layer_name, const char *model_name,
+                                        IEPrecision precision, IELayout layout, size_t ranks, size_t *dims, void *data,
+                                        size_t data_size) {
+    tensor->precision = get_precision_string(precision);
+    tensor->layout = get_layout_string(layout);
+    tensor->ranks = ranks;
+    for (int i = 0; i < ranks; i++)
+        tensor->dims[i] = dims[i];
+    tensor->layer_name = strdup(layer_name);
+    tensor->model_name = model_name;
+    if (data) {
+        tensor->buffer = av_buffer_alloc(data_size);
+        av_assert0(tensor->buffer);
+        memcpy(tensor->buffer->data, data, data_size);
+    }
+}
+
+static int get_unbatched_size_in_bytes(OutputBlobContext *blob_ctx, size_t batch_size) {
+    const OutputBlobMethod *blob = blob_ctx->output_blob_method;
+    size_t size;
+    Dimensions dim = blob->GetDims(blob_ctx);
+
+    if (dim.dims[0] != batch_size) {
+        VAII_ERROR("Blob last dimension should be equal to batch size");
+        av_assert0(0);
+    }
+    size = dim.dims[1];
+    for (int i = 2; i < dim.num_dims; i++) {
+        size *= dim.dims[i];
+    }
+    switch (blob->GetPrecision(blob_ctx)) {
+    case II_FP32:
+        size *= sizeof(float);
+        break;
+    case II_U8:
+    default:
+        break;
+    }
+    return size;
+}
+
+typedef struct DetectionObject {
+    int xmin, ymin, xmax, ymax, class_id;
+    float confidence;
+} DetectionObject;
+
+typedef struct DetectionObjectArray {
+    DetectionObject **objects;
+    int num_detection_objects;
+} DetectionObjectArray;
+
+static void DetectionObjectInit(DetectionObject *this, double x, double y, double h, double w, int class_id,
+                                float confidence, float h_scale, float w_scale) {
+    this->xmin = (int)((x - w / 2) * w_scale);
+    this->ymin = (int)((y - h / 2) * h_scale);
+    this->xmax = (int)(this->xmin + w * w_scale);
+    this->ymax = (int)(this->ymin + h * h_scale);
+    this->class_id = class_id;
+    this->confidence = confidence;
+}
+
+static int DetectionObjectCompare(const void *p1, const void *p2) {
+    return (*(DetectionObject **)p1)->confidence < (*(DetectionObject **)p2)->confidence ? 1 : -1;
+}
+
+static double IntersectionOverUnion(const DetectionObject *box_1, const DetectionObject *box_2) {
+    double width_of_overlap_area = fmin(box_1->xmax, box_2->xmax) - fmax(box_1->xmin, box_2->xmin);
+    double height_of_overlap_area = fmin(box_1->ymax, box_2->ymax) - fmax(box_1->ymin, box_2->ymin);
+    double area_of_overlap, area_of_union;
+    double box_1_area = (box_1->ymax - box_1->ymin) * (box_1->xmax - box_1->xmin);
+    double box_2_area = (box_2->ymax - box_2->ymin) * (box_2->xmax - box_2->xmin);
+
+    if (width_of_overlap_area < 0 || height_of_overlap_area < 0)
+        area_of_overlap = 0;
+    else
+        area_of_overlap = width_of_overlap_area * height_of_overlap_area;
+    area_of_union = box_1_area + box_2_area - area_of_overlap;
+    return area_of_overlap / area_of_union;
+}
+
+static int EntryIndex(int side, int lcoords, int lclasses, int location, int entry) {
+    int n = location / (side * side);
+    int loc = location % (side * side);
+    return n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc;
+}
+
+#define YOLOV3_INPUT_SIZE 320 // TODO: add interface to get this info
+
+static void ParseYOLOV3Output(OutputBlobContext *blob_ctx, int image_width, int image_height,
+                              DetectionObjectArray *objects, const FFBaseInference *base) {
+    const OutputBlobMethod *blob = blob_ctx->output_blob_method;
+    Dimensions dim = blob->GetDims(blob_ctx);
+    const int out_blob_h = (int)dim.dims[2];
+    const int out_blob_w = (int)dim.dims[3];
+    const int coords = 4, num = 3, classes = 80;
+    const float anchors[] = {10.0, 13.0, 16.0,  30.0,  33.0, 23.0,  30.0,  61.0,  62.0,
+                             45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0};
+    int side = out_blob_h;
+    int anchor_offset = 0;
+    int side_square = side * side;
+    const float *output_blob = (const float *)blob->GetData(blob_ctx);
+
+    av_assert0(out_blob_h == out_blob_w);
+    switch (side) {
+    case 13:
+    case 10:
+    case 19:
+        anchor_offset = 2 * 6;
+        break;
+    case 26:
+    case 20:
+    case 38:
+        anchor_offset = 2 * 3;
+        break;
+    case 52:
+    case 40:
+    case 76:
+        anchor_offset = 2 * 0;
+        break;
+    default:
+        VAII_ERROR("Invaild output size\n");
+        return;
+    }
+
+    for (int i = 0; i < side_square; ++i) {
+        int row = i / side;
+        int col = i % side;
+        for (int n = 0; n < num; ++n) {
+            int obj_index = EntryIndex(side, coords, classes, n * side * side + i, coords);
+            int box_index = EntryIndex(side, coords, classes, n * side * side + i, 0);
+            double x, y, width, height;
+
+            float scale = output_blob[obj_index];
+            float threshold = base->param.threshold;
+            if (scale < threshold)
+                continue;
+
+            x = (col + output_blob[box_index + 0 * side_square]) / side * YOLOV3_INPUT_SIZE;
+            y = (row + output_blob[box_index + 1 * side_square]) / side * YOLOV3_INPUT_SIZE;
+            width = exp(output_blob[box_index + 2 * side_square]) * anchors[anchor_offset + 2 * n];
+            height = exp(output_blob[box_index + 3 * side_square]) * anchors[anchor_offset + 2 * n + 1];
+
+            for (int j = 0; j < classes; ++j) {
+                int class_index = EntryIndex(side, coords, classes, n * side_square + i, coords + 1 + j);
+                float prob = scale * output_blob[class_index];
+                DetectionObject *obj = NULL;
+                if (prob < threshold)
+                    continue;
+                obj = av_mallocz(sizeof(*obj));
+                av_assert0(obj);
+                if (!base->crop_full_frame)
+                    DetectionObjectInit(obj, x, y, height, width, j, prob,
+                                        (float)(image_height) / (float)(YOLOV3_INPUT_SIZE),
+                                        (float)(image_width) / (float)(YOLOV3_INPUT_SIZE));
+                else
+                    DetectionObjectInit(obj, x, y, height, width, j, prob,
+                                        (float)(base->param.crop_rect.height) / (float)(YOLOV3_INPUT_SIZE),
+                                        (float)(base->param.crop_rect.width) / (float)(YOLOV3_INPUT_SIZE));
+                av_dynarray_add(&objects->objects, &objects->num_detection_objects, obj);
+            }
+        }
+    }
+}
+
+static void ExtractYOLOV3BoundingBoxes(const OutputBlobArray *blob_array, InferenceROIArray *infer_roi_array,
+                                       ModelOutputPostproc *model_postproc, const char *model_name,
+                                       const FFBaseInference *ff_base_inference) {
+    DetectionObjectArray obj_array = {};
+    BBoxesArray *boxes;
+    AVBufferRef *ref;
+    AVBufferRef *labels = NULL;
+    AVFrame *av_frame;
+    AVFrameSideData *side_data;
+    InferDetectionMeta *detect_meta;
+    const char *layer_name;
+    Dimensions dimensions;
+    IILayout layout;
+    IEPrecision precision;
+
+    av_assert0(blob_array->num_blobs == 3);           // This accepts networks with three layers
+    av_assert0(infer_roi_array->num_infer_ROIs == 1); // YoloV3 cannot support batch mode
+
+    for (int n = 0; n < blob_array->num_blobs; n++) {
+        OutputBlobContext *blob_ctx = blob_array->output_blobs[n];
+        const OutputBlobMethod *blob = blob_ctx->output_blob_method;
+        layer_name = blob->GetOutputLayerName(blob_ctx);
+        dimensions = blob->GetDims(blob_ctx);
+        layout = blob->GetLayout(blob_ctx);
+        precision = blob->GetPrecision(blob_ctx);
+
+        if (model_postproc) {
+            int idx = findModelPostProcByName(model_postproc, layer_name);
+            if (idx != MAX_MODEL_OUTPUT)
+                labels = model_postproc->procs[idx].labels;
+        }
+
+        av_assert0(blob_ctx);
+        ParseYOLOV3Output(blob_ctx, infer_roi_array->infer_ROIs[0]->roi.w, infer_roi_array->infer_ROIs[0]->roi.h,
+                          &obj_array, ff_base_inference);
+    }
+
+    qsort(obj_array.objects, obj_array.num_detection_objects, sizeof(DetectionObject *), DetectionObjectCompare);
+    for (int i = 0; i < obj_array.num_detection_objects; ++i) {
+        DetectionObjectArray *d = &obj_array;
+        if (d->objects[i]->confidence == 0)
+            continue;
+        for (int j = i + 1; j < d->num_detection_objects; ++j)
+            if (IntersectionOverUnion(d->objects[i], d->objects[j]) >= 0.4)
+                d->objects[j]->confidence = 0;
+    }
+
+    boxes = av_mallocz(sizeof(*boxes));
+    av_assert0(boxes);
+
+    for (int i = 0; i < obj_array.num_detection_objects; ++i) {
+        InferDetection *new_bbox = NULL;
+        DetectionObject *object = obj_array.objects[i];
+        if (object->confidence < ff_base_inference->param.threshold)
+            continue;
+
+        new_bbox = (InferDetection *)av_mallocz(sizeof(*new_bbox));
+        av_assert0(new_bbox);
+
+        if (!ff_base_inference->crop_full_frame) {
+            new_bbox->x_min = object->xmin;
+            new_bbox->y_min = object->ymin;
+            new_bbox->x_max = object->xmax;
+            new_bbox->y_max = object->ymax;
+        } else {
+            int x_offset = ff_base_inference->param.crop_rect.x;
+            int y_offset = ff_base_inference->param.crop_rect.y;
+            new_bbox->x_min = object->xmin + x_offset;
+            new_bbox->y_min = object->ymin + y_offset;
+            new_bbox->x_max = object->xmax + x_offset;
+            new_bbox->y_max = object->ymax + y_offset;
+        }
+        new_bbox->confidence = object->confidence;
+        new_bbox->label_id = object->class_id;
+
+        new_bbox->x_min = new_bbox->x_min < 0 ? 0 : new_bbox->x_min;
+        new_bbox->y_min = new_bbox->y_min < 0 ? 0 : new_bbox->y_min;
+
+        if (labels)
+            new_bbox->label_buf = av_buffer_ref(labels);
+
+        new_bbox->roi_meta.x = new_bbox->x_min;
+        new_bbox->roi_meta.y = new_bbox->y_min;
+        new_bbox->roi_meta.w = new_bbox->x_max - new_bbox->x_min;
+        new_bbox->roi_meta.h = new_bbox->y_max - new_bbox->y_min;
+
+        av_dynarray_add(&boxes->bbox, &boxes->num, new_bbox);
+
+        fill_tensor_metadata(&new_bbox->tensor, layer_name, model_name, precision, layout, dimensions.num_dims,
+                             dimensions.dims, NULL, 0);
+        VAII_LOGD("bbox %d %d %d %d\n", (int)new_bbox->x_min, (int)new_bbox->y_min, (int)new_bbox->x_max,
+                  (int)new_bbox->y_max);
+    }
+
+    detect_meta = av_mallocz(sizeof(*detect_meta));
+    av_assert0(detect_meta);
+    detect_meta->bboxes = boxes;
+
+    ref = av_buffer_create((uint8_t *)detect_meta, sizeof(*detect_meta), &infer_detect_metadata_buffer_free, NULL, 0);
+    if (!ref) {
+        infer_detect_metadata_buffer_free(NULL, (uint8_t *)detect_meta);
+        VAII_ERROR("Create buffer ref failed.\n");
+        av_assert0(0);
+    }
+
+    av_frame = infer_roi_array->infer_ROIs[0]->frame;
+    // add meta data to side data
+    side_data = av_frame_new_side_data_from_buf(av_frame, AV_FRAME_DATA_INFERENCE_DETECTION, ref);
+    av_assert0(side_data);
+
+    // free all detection objects
+    for (int n = 0; n < obj_array.num_detection_objects; n++)
+        av_free(obj_array.objects[n]);
+    av_free(obj_array.objects);
+}
+
+static void ExtractBoundingBoxes(const OutputBlobArray *blob_array, InferenceROIArray *infer_roi_array,
+                                 ModelOutputPostproc *model_postproc, const char *model_name,
+                                 const FFBaseInference *ff_base_inference) {
+    for (int n = 0; n < blob_array->num_blobs; n++) {
+        AVBufferRef *labels = NULL;
+        BBoxesArray **boxes = NULL;
+        OutputBlobContext *ctx = blob_array->output_blobs[n];
+        const OutputBlobMethod *blob = ctx->output_blob_method;
+
+        const char *layer_name = blob->GetOutputLayerName(ctx);
+        const float *detections = (const float *)blob->GetData(ctx);
+
+        Dimensions dim = blob->GetDims(ctx);
+        IILayout layout = blob->GetLayout(ctx);
+        IEPrecision precision = blob->GetPrecision(ctx);
+
+        int object_size = 0;
+        int max_proposal_count = 0;
+        float threshold = ff_base_inference->param.threshold;
+
+        switch (layout) {
+        case II_LAYOUT_NCHW:
+            object_size = dim.dims[3];
+            max_proposal_count = dim.dims[2];
+            break;
+        default:
+            VAII_ERROR("Unsupported output layout, boxes won't be extracted\n");
+            continue;
+        }
+
+        if (object_size != 7) { // SSD DetectionOutput format
+            VAII_ERROR("Unsupported output dimensions, boxes won't be extracted\n");
+            continue;
+        }
+
+        if (model_postproc) {
+            int idx = findModelPostProcByName(model_postproc, layer_name);
+            if (idx != MAX_MODEL_OUTPUT)
+                labels = model_postproc->procs[idx].labels;
+        }
+
+        boxes = (BBoxesArray **)av_mallocz_array(infer_roi_array->num_infer_ROIs, sizeof(boxes[0]));
+        av_assert0(boxes);
+
+        for (int i = 0; i < max_proposal_count; i++) {
+            int image_id = (int)detections[i * object_size + 0];
+            int label_id = (int)detections[i * object_size + 1];
+            float confidence = detections[i * object_size + 2];
+            float x_min = detections[i * object_size + 3];
+            float y_min = detections[i * object_size + 4];
+            float x_max = detections[i * object_size + 5];
+            float y_max = detections[i * object_size + 6];
+            if (image_id < 0 || (size_t)image_id >= infer_roi_array->num_infer_ROIs)
+                break;
+
+            if (confidence < threshold)
+                continue;
+
+            if (boxes[image_id] == NULL) {
+                boxes[image_id] = (BBoxesArray *)av_mallocz(sizeof(*boxes[image_id]));
+                av_assert0(boxes[image_id]);
+            }
+
+            /* using integer to represent */
+            {
+                FFVideoRegionOfInterestMeta *roi = &infer_roi_array->infer_ROIs[image_id]->roi;
+                InferDetection *new_bbox = (InferDetection *)av_mallocz(sizeof(*new_bbox));
+
+                int width = roi->w;
+                int height = roi->h;
+                int ix_min = (int)(x_min * width + 0.5);
+                int iy_min = (int)(y_min * height + 0.5);
+                int ix_max = (int)(x_max * width + 0.5);
+                int iy_max = (int)(y_max * height + 0.5);
+
+                if (ix_min < 0)
+                    ix_min = 0;
+                if (iy_min < 0)
+                    iy_min = 0;
+                if (ix_max > width)
+                    ix_max = width;
+                if (iy_max > height)
+                    iy_max = height;
+
+                av_assert0(new_bbox);
+                new_bbox->x_min = x_min;
+                new_bbox->y_min = y_min;
+                new_bbox->x_max = x_max;
+                new_bbox->y_max = y_max;
+                new_bbox->confidence = confidence;
+                new_bbox->label_id = label_id;
+                if (labels)
+                    new_bbox->label_buf = av_buffer_ref(labels);
+                new_bbox->roi_meta.x = ix_min;
+                new_bbox->roi_meta.y = iy_min;
+                new_bbox->roi_meta.w = ix_max - ix_min;
+                new_bbox->roi_meta.h = iy_max - iy_min;
+
+                if (ff_base_inference->param.square_bbox)
+                    enhanced_face_bounding_box(&new_bbox->roi_meta);
+
+                fill_tensor_metadata(&new_bbox->tensor, layer_name, model_name, precision, layout, dim.num_dims,
+                                     dim.dims, NULL, 0);
+
+                av_dynarray_add(&boxes[image_id]->bbox, &boxes[image_id]->num, new_bbox);
+            }
+        }
+
+        for (int n = 0; n < infer_roi_array->num_infer_ROIs; n++) {
+            AVBufferRef *ref;
+            AVFrame *av_frame;
+            AVFrameSideData *sd;
+
+            InferDetectionMeta *detect_meta = (InferDetectionMeta *)av_malloc(sizeof(*detect_meta));
+            av_assert0(detect_meta);
+
+            detect_meta->bboxes = boxes[n];
+
+            ref = av_buffer_create((uint8_t *)detect_meta, sizeof(*detect_meta), &infer_detect_metadata_buffer_free,
+                                   NULL, 0);
+            if (ref == NULL) {
+                infer_detect_metadata_buffer_free(NULL, (uint8_t *)detect_meta);
+                av_assert0(ref);
+            }
+
+            av_frame = infer_roi_array->infer_ROIs[n]->frame;
+            // add meta data to side data
+            sd = av_frame_new_side_data_from_buf(av_frame, AV_FRAME_DATA_INFERENCE_DETECTION, ref);
+            if (sd == NULL) {
+                av_buffer_unref(&ref);
+                av_assert0(sd);
+            }
+            VAII_LOGD("av_frame:%p sd:%d\n", av_frame, av_frame->nb_side_data);
+        }
+
+        av_free(boxes);
+    }
+}
+
+static int CreateNewClassifySideData(AVFrame *frame, InferClassificationMeta *classify_meta) {
+    AVBufferRef *ref;
+    AVFrameSideData *new_sd;
+    ref = av_buffer_create((uint8_t *)classify_meta, sizeof(*classify_meta), &infer_classify_metadata_buffer_free, NULL,
+                           0);
+    if (!ref)
+        return AVERROR(ENOMEM);
+
+    // add meta data to side data
+    new_sd = av_frame_new_side_data_from_buf(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION, ref);
+    if (!new_sd) {
+        av_buffer_unref(&ref);
+        VAII_ERROR("Could not add new side data\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return 0;
+}
+
+static av_cold void dump_softmax(char *name, int label_id, float conf, AVBufferRef *label_buf) {
+    LabelsArray *array = (LabelsArray *)label_buf->data;
+
+    VAII_LOGD("CLASSIFY META - Label id:%d %s:%s Conf:%f\n", label_id, name, array->label[label_id], conf);
+}
+
+static av_cold void dump_tensor_value(char *name, int value) {
+    VAII_LOGD("CLASSIFY META - %s:%d\n", name, value);
+}
+
+static void find_max_element_index(const float *array, int len, int *index, float *value) {
+    int i;
+    *index = 0;
+    *value = array[0];
+    for (i = 1; i < len; i++) {
+        if (array[i] > *value) {
+            *index = i;
+            *value = array[i];
+        }
+    }
+}
+
+static int attributes_to_text(FFVideoRegionOfInterestMeta *meta, OutputPostproc *post_proc, void *data, Dimensions *dim,
+                              InferClassification *classification, InferClassificationMeta *classify_meta) {
+    const float *blob_data = (const float *)data;
+    uint32_t method_max, method_compound, method_index;
+
+    method_max = !strcmp(post_proc->method, "max");
+    method_compound = !strcmp(post_proc->method, "compound");
+    method_index = !strcmp(post_proc->method, "index");
+
+    if (!blob_data)
+        return -1;
+
+    if (method_max) {
+        int index;
+        float confidence;
+        size_t n = dim->dims[1];
+
+        find_max_element_index(data, n, &index, &confidence);
+
+        classification->detect_id = meta->index;
+        classification->name = post_proc->attribute_name;
+        classification->label_id = index;
+        classification->confidence = confidence;
+        classification->label_buf = av_buffer_ref(post_proc->labels);
+
+        if (classification->label_buf) {
+            dump_softmax(classification->name, classification->label_id, classification->confidence,
+                         classification->label_buf);
+        }
+    } else if (method_compound) {
+        int i;
+        double threshold = 0.5;
+        float confidence = 0;
+        LabelsArray *array;
+        classification->attributes = av_mallocz(4096);
+        if (classification->attributes == NULL)
+            return -1;
+
+        if (post_proc->threshold != 0)
+            threshold = post_proc->threshold;
+
+        array = (LabelsArray *)post_proc->labels->data;
+        for (i = 0; i < array->num; i++) {
+            if (blob_data[i] >= threshold)
+                strncat(classification->attributes, array->label[i], (strlen(array->label[i]) + 1));
+            if (blob_data[i] > confidence)
+                confidence = blob_data[i];
+        }
+
+        classification->detect_id = meta->index;
+        classification->name = post_proc->attribute_name;
+        classification->confidence = confidence;
+        VAII_LOGD("%s:%s\n", classification->name, classification->attributes);
+    } else if (method_index) {
+        int i;
+        LabelsArray *array;
+        classification->attributes = av_mallocz(4096);
+        if (classification->attributes == NULL)
+            return -1;
+
+        array = (LabelsArray *)post_proc->labels->data;
+        for (i = 0; i < array->num; i++) {
+            int value = blob_data[i];
+            if (value < 0 || value >= array->num)
+                break;
+            strncat(classification->attributes, array->label[value], (strlen(array->label[value]) + 1));
+        }
+
+        classification->detect_id = meta->index;
+        classification->name = post_proc->attribute_name;
+        VAII_LOGD("%s:%s\n", classification->name, classification->attributes);
+    }
+
+    return 0;
+}
+
+static int tensor_to_text(FFVideoRegionOfInterestMeta *meta, OutputPostproc *post_proc, void *data, Dimensions *dim,
+                          InferClassification *classification, InferClassificationMeta *classify_meta) {
+    // InferClassification *classify;
+    const float *blob_data = (const float *)data;
+    double scale = 1.0;
+    int value = 0;
+
+    if (!blob_data || !meta || !post_proc || !classification)
+        return -1;
+
+    if (post_proc->tensor_to_text_scale != 0)
+        scale = post_proc->tensor_to_text_scale;
+
+    classification->detect_id = meta->index;
+    classification->name = post_proc->attribute_name;
+
+    value = (int)(*blob_data * scale);
+    classification->attributes = malloc(16);
+    if (!classification->attributes)
+        return -1;
+    snprintf(classification->attributes, 16, "%d", value);
+
+    dump_tensor_value(classification->name, value);
+    return 0;
+}
+
+static void Blob2RoiMeta(const OutputBlobArray *blob_array, InferenceROIArray *infer_roi_array,
+                         ModelOutputPostproc *model_postproc, const char *model_name,
+                         const FFBaseInference *ff_base_inference) {
+    int batch_size = infer_roi_array->num_infer_ROIs;
+
+    for (int n = 0; n < blob_array->num_blobs; n++) {
+        OutputBlobContext *ctx = blob_array->output_blobs[n];
+        const OutputBlobMethod *blob;
+        const char *layer_name;
+        uint8_t *data = NULL;
+        int size;
+        OutputPostproc *post_proc = NULL;
+        Dimensions dimensions;
+        IILayout layout;
+        IEPrecision precision;
+
+        av_assert0(ctx);
+
+        blob = ctx->output_blob_method;
+        layer_name = blob->GetOutputLayerName(ctx);
+        data = (uint8_t *)blob->GetData(ctx);
+        dimensions = blob->GetDims(ctx);
+        layout = blob->GetLayout(ctx);
+        precision = blob->GetPrecision(ctx);
+        size = get_unbatched_size_in_bytes(ctx, batch_size);
+
+        if (model_postproc) {
+            int proc_idx = findModelPostProcByName(model_postproc, layer_name);
+            if (proc_idx != MAX_MODEL_OUTPUT)
+                post_proc = &model_postproc->procs[proc_idx];
+        }
+
+        for (int b = 0; b < batch_size; b++) {
+            FFVideoRegionOfInterestMeta *meta = &infer_roi_array->infer_ROIs[b]->roi;
+            AVFrame *av_frame = infer_roi_array->infer_ROIs[b]->frame;
+            AVFrameSideData *sd = NULL;
+            InferClassificationMeta *classify_meta = NULL;
+            InferClassification *classification = NULL;
+
+            sd = av_frame_get_side_data(av_frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION);
+            if (sd) {
+                // append to exsiting side data
+                classify_meta = (InferClassificationMeta *)sd->data;
+                av_assert0(classify_meta);
+            } else {
+                ClassifyArray *classify_array = NULL;
+                // new classification meta data
+                classify_meta = av_mallocz(sizeof(*classify_meta));
+                classify_array = av_mallocz(sizeof(*classify_array));
+                av_assert0(classify_meta && classify_array);
+                classify_meta->c_array = classify_array;
+                av_assert0(0 == CreateNewClassifySideData(av_frame, classify_meta));
+            }
+
+            classification = av_mallocz(sizeof(*classification));
+            av_assert0(classification);
+
+            if (post_proc && post_proc->converter) {
+                if (!strcmp(post_proc->converter, "tensor_to_label")) {
+                    attributes_to_text(meta, post_proc, (void *)(data + b * size), &dimensions, classification,
+                                       classify_meta);
+                } else if (!strcmp(post_proc->converter, "tensor_to_text")) {
+                    tensor_to_text(meta, post_proc, (void *)(data + b * size), &dimensions, classification,
+                                   classify_meta);
+                } else {
+                    VAII_LOGE("Undefined converter:%s\n", post_proc->converter);
+                    av_free(classification);
+                    break;
+                }
+            } else {
+                // copy data to tensor buffer
+                classification->detect_id = meta->index;
+                classification->name =
+                    (post_proc && post_proc->attribute_name) ? post_proc->attribute_name : (char *)"default";
+            }
+
+            fill_tensor_metadata(&classification->tensor, layer_name, model_name, precision, layout,
+                                 dimensions.num_dims, dimensions.dims, data + b * size, size);
+            av_dynarray_add(&classify_meta->c_array->classifications, &classify_meta->c_array->num, classification);
+        }
+    }
+}
+
+PostProcFunction getPostProcFunctionByName(const char *name, const char *model) {
+    if (name == NULL || model == NULL)
+        return NULL;
+
+    if (!strcmp(name, "detect")) {
+        if (strstr(model, "yolo"))
+            return (PostProcFunction)ExtractYOLOV3BoundingBoxes;
+        else
+            return (PostProcFunction)ExtractBoundingBoxes;
+    } else if (!strcmp(name, "classify")) {
+        return (PostProcFunction)Blob2RoiMeta;
+    }
+    return NULL;
+}
+
+int findModelPostProcByName(ModelOutputPostproc *model_postproc, const char *layer_name) {
+    int proc_id;
+    // search model postproc
+    for (proc_id = 0; proc_id < MAX_MODEL_OUTPUT; proc_id++) {
+        char *proc_layer_name = model_postproc->procs[proc_id].layer_name;
+        // skip this output process
+        if (!proc_layer_name)
+            continue;
+        if (!strcmp(layer_name, proc_layer_name))
+            return proc_id;
+    }
+
+    VAII_LOGD("Could not find proc:%s\n", layer_name);
+    return proc_id;
+}
diff --git a/libavfilter/inference_backend/ff_proc_factory.h b/libavfilter/inference_backend/ff_proc_factory.h
new file mode 100644
index 0000000..9419c1f
--- /dev/null
+++ b/libavfilter/inference_backend/ff_proc_factory.h
@@ -0,0 +1,27 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "ff_base_inference.h"
+
+PostProcFunction getPostProcFunctionByName(const char *name, const char *model);
+
+int findModelPostProcByName(ModelOutputPostproc *model_postproc, const char *layer_name);
\ No newline at end of file
diff --git a/libavfilter/inference_backend/image.c b/libavfilter/inference_backend/image.c
new file mode 100644
index 0000000..1bc6c5d
--- /dev/null
+++ b/libavfilter/inference_backend/image.c
@@ -0,0 +1,92 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "image.h"
+#include "config.h"
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+extern ImageMap image_map_vaapi;
+extern ImageMap image_map_mocker;
+
+static const ImageMap *const image_map_list[] = {
+#if CONFIG_VAAPI
+    &image_map_vaapi,
+#endif
+    &image_map_mocker, NULL};
+
+static const ImageMap *image_map_iterate(void **opaque) {
+    uintptr_t i = (uintptr_t)*opaque;
+    const ImageMap *im = image_map_list[i];
+
+    if (im != NULL)
+        *opaque = (void *)(i + 1);
+
+    return im;
+}
+
+const ImageMap *image_map_get_by_name(const char *name) {
+    const ImageMap *im = NULL;
+    void *opaque = 0;
+
+    if (name == NULL)
+        return NULL;
+
+    while ((im = image_map_iterate(&opaque)))
+        if (!strcmp(im->name, name))
+            return im;
+
+    return NULL;
+}
+
+ImageMapContext *image_map_alloc(const ImageMap *image_map) {
+    ImageMapContext *ret = NULL;
+
+    if (image_map == NULL)
+        return NULL;
+
+    ret = (ImageMapContext *)malloc(sizeof(*ret));
+    assert(ret);
+    memset(ret, 0, sizeof(*ret));
+
+    ret->mapper = image_map;
+    if (image_map->priv_size > 0) {
+        ret->priv = malloc(image_map->priv_size);
+        if (!ret->priv)
+            goto err;
+        memset(ret->priv, 0, image_map->priv_size);
+    }
+
+    return ret;
+err:
+    free(ret);
+    return NULL;
+}
+
+void image_map_free(ImageMapContext *context) {
+    if (context == NULL)
+        return;
+
+    if (context->priv)
+        free(context->priv);
+    free(context);
+}
diff --git a/libavfilter/inference_backend/image.h b/libavfilter/inference_backend/image.h
new file mode 100644
index 0000000..a65be54
--- /dev/null
+++ b/libavfilter/inference_backend/image.h
@@ -0,0 +1,90 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include <stdint.h>
+
+typedef enum MemoryType { MEM_TYPE_ANY = 0, MEM_TYPE_SYSTEM = 1, MEM_TYPE_OPENCL = 2, MEM_TYPE_VAAPI = 3 } MemoryType;
+
+typedef enum FourCC {
+    FOURCC_NV12 = 0x3231564E,
+    FOURCC_BGRA = 0x41524742,
+    FOURCC_BGRX = 0x58524742,
+    FOURCC_BGRP = 0x50524742,
+    FOURCC_BGR = 0x00524742,
+    FOURCC_RGBA = 0x41424752,
+    FOURCC_RGBX = 0x58424752,
+    FOURCC_RGBP = 0x50424752,
+    FOURCC_RGBP_F32 = 0x07282024,
+    FOURCC_I420 = 0x30323449,
+} FourCC;
+
+typedef struct Rectangle {
+    int x;
+    int y;
+    int width;
+    int height;
+} Rectangle;
+
+#define MAX_PLANES_NUMBER 4
+
+typedef struct Image {
+    MemoryType type;
+    union {
+        uint8_t *planes[MAX_PLANES_NUMBER]; // if type==SYSTEM
+        void *cl_mem;                       // if type==OPENCL
+        struct {                            // if type==VAAPI
+            uint32_t surface_id;
+            void *va_display;
+        };
+    };
+    int format; // FourCC
+    int colorspace;
+    int width;
+    int height;
+    int stride[MAX_PLANES_NUMBER];
+    Rectangle rect;
+} Image;
+
+typedef struct ImageMapContext ImageMapContext;
+
+// Map DMA/VAAPI image into system memory
+typedef struct ImageMap {
+    /* image mapper name. Must be non-NULL and unique among pre processing modules. */
+    const char *name;
+
+    Image (*Map)(ImageMapContext *context, const Image *image);
+
+    void (*Unmap)(ImageMapContext *context);
+
+    int priv_size;
+} ImageMap;
+
+struct ImageMapContext {
+    const ImageMap *mapper;
+    void *priv;
+};
+
+const ImageMap *image_map_get_by_name(const char *name);
+
+ImageMapContext *image_map_alloc(const ImageMap *image_map);
+
+void image_map_free(ImageMapContext *context);
\ No newline at end of file
diff --git a/libavfilter/inference_backend/image_inference.c b/libavfilter/inference_backend/image_inference.c
new file mode 100644
index 0000000..c3314bb
--- /dev/null
+++ b/libavfilter/inference_backend/image_inference.c
@@ -0,0 +1,199 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "image_inference.h"
+#include <limits.h>
+#include <string.h>
+
+extern OutputBlobMethod output_blob_method_openvino;
+extern ImageInference image_inference_openvino;
+extern ImageInference image_inference_async_preproc;
+
+static const ImageInference *const image_inference_list[] = {&image_inference_openvino, &image_inference_async_preproc,
+                                                             NULL};
+static const OutputBlobMethod *const output_blob_method_list[] = {&output_blob_method_openvino, NULL};
+
+static const ImageInference *image_inference_iterate(void **opaque) {
+    uintptr_t i = (uintptr_t)*opaque;
+    const ImageInference *ii = image_inference_list[i];
+
+    if (ii != NULL)
+        *opaque = (void *)(i + 1);
+
+    return ii;
+}
+
+static const OutputBlobMethod *output_blob_iterate(void **opaque) {
+    uintptr_t i = (uintptr_t)*opaque;
+    const OutputBlobMethod *obm = output_blob_method_list[i];
+
+    if (obm != NULL)
+        *opaque = (void *)(i + 1);
+
+    return obm;
+}
+
+const ImageInference *image_inference_get_by_name(const char *name) {
+    const ImageInference *ii = NULL;
+    void *opaque = 0;
+
+    if (name == NULL)
+        return NULL;
+
+    while ((ii = image_inference_iterate(&opaque)))
+        if (!strcmp(ii->name, name))
+            return (ImageInference *)ii;
+
+    return NULL;
+}
+
+ImageInferenceContext *image_inference_alloc(const ImageInference *infernce, const OutputBlobMethod *obm,
+                                             const char *instance_name) {
+    ImageInferenceContext *ret;
+
+    if (infernce == NULL)
+        return NULL;
+
+    ret = (ImageInferenceContext *)malloc(sizeof(*ret));
+    if (!ret)
+        return NULL;
+    memset(ret, 0, sizeof(*ret));
+
+    ret->inference = infernce;
+    ret->output_blob_method = obm;
+    ret->name = instance_name ? strdup(instance_name) : NULL;
+    if (infernce->priv_size > 0) {
+        ret->priv = malloc(infernce->priv_size);
+        if (!ret->priv)
+            goto err;
+        memset(ret->priv, 0, infernce->priv_size);
+    }
+
+    return ret;
+err:
+    free(ret->priv);
+    free(ret);
+    return NULL;
+}
+
+void image_inference_free(ImageInferenceContext *inference_context) {
+    if (inference_context == NULL)
+        return;
+
+    if (inference_context->priv)
+        free(inference_context->priv);
+    if (inference_context->name)
+        free(inference_context->name);
+    free(inference_context);
+}
+
+const OutputBlobMethod *output_blob_method_get_by_name(const char *name) {
+    const OutputBlobMethod *ob = NULL;
+    void *opaque = 0;
+
+    if (name == NULL)
+        return NULL;
+
+    while ((ob = output_blob_iterate(&opaque)))
+        if (!strcmp(ob->name, name))
+            return (OutputBlobMethod *)ob;
+
+    return NULL;
+}
+
+OutputBlobContext *output_blob_alloc(const OutputBlobMethod *obm) {
+    OutputBlobContext *ret;
+
+    if (obm == NULL)
+        return NULL;
+
+    ret = (OutputBlobContext *)malloc(sizeof(*ret));
+    if (!ret)
+        return NULL;
+    memset(ret, 0, sizeof(*ret));
+
+    ret->output_blob_method = obm;
+    if (obm->priv_size > 0) {
+        ret->priv = malloc(obm->priv_size);
+        if (!ret->priv)
+            goto err;
+        memset(ret->priv, 0, obm->priv_size);
+    }
+
+    return ret;
+err:
+    free(ret->priv);
+    free(ret);
+    return NULL;
+}
+
+void output_blob_free(OutputBlobContext *context) {
+    if (context == NULL)
+        return;
+
+    if (context->priv)
+        free(context->priv);
+    free(context);
+}
+
+#define FF_DYNARRAY_ADD(av_size_max, av_elt_size, av_array, av_size, av_success, av_failure)                           \
+    do {                                                                                                               \
+        size_t av_size_new = (av_size);                                                                                \
+        if (!((av_size) & ((av_size)-1))) {                                                                            \
+            av_size_new = (av_size) ? (av_size) << 1 : 1;                                                              \
+            if (av_size_new > (av_size_max) / (av_elt_size)) {                                                         \
+                av_size_new = 0;                                                                                       \
+            } else {                                                                                                   \
+                void *av_array_new = realloc((av_array), av_size_new * (av_elt_size));                                 \
+                if (!av_array_new)                                                                                     \
+                    av_size_new = 0;                                                                                   \
+                else                                                                                                   \
+                    (av_array) = (void **)av_array_new;                                                                \
+            }                                                                                                          \
+        }                                                                                                              \
+        if (av_size_new) {                                                                                             \
+            {av_success}(av_size)++;                                                                                   \
+        } else {                                                                                                       \
+            av_failure                                                                                                 \
+        }                                                                                                              \
+    } while (0)
+
+static void ii_freep(void *arg) {
+    void *val;
+
+    memcpy(&val, arg, sizeof(val));
+    memcpy(arg, &(void *){NULL}, sizeof(val));
+    free(val);
+}
+
+void image_inference_dynarray_add(void *tab_ptr, int *nb_ptr, void *elem) {
+    void **tab;
+    memcpy(&tab, tab_ptr, sizeof(tab));
+
+    FF_DYNARRAY_ADD(INT_MAX, sizeof(*tab), tab, *nb_ptr,
+                    {
+                        tab[*nb_ptr] = elem;
+                        memcpy(tab_ptr, &tab, sizeof(tab));
+                    },
+                    {
+                        *nb_ptr = 0;
+                        ii_freep(tab_ptr);
+                    });
+}
\ No newline at end of file
diff --git a/libavfilter/inference_backend/image_inference.h b/libavfilter/inference_backend/image_inference.h
new file mode 100644
index 0000000..5be91e6
--- /dev/null
+++ b/libavfilter/inference_backend/image_inference.h
@@ -0,0 +1,163 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "image.h"
+#include "pre_proc.h"
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+typedef enum {
+    II_LAYOUT_ANY = 0,
+    II_LAYOUT_NCHW = 1,
+    II_LAYOUT_NHWC = 2,
+} IILayout;
+
+typedef enum {
+    II_FP32 = 10,
+    II_U8 = 40,
+} IIPrecision;
+
+/* Don't change this structure */
+#define II_MAX_DIMENSIONS 8
+typedef struct Dimensions {
+    size_t num_dims;
+    size_t dims[II_MAX_DIMENSIONS];
+} Dimensions;
+
+typedef void *IFramePtr;
+
+typedef struct UserDataBuffers {
+    IFramePtr *frames;
+    int num_buffers;
+} UserDataBuffers;
+
+typedef struct OutputBlobMethod OutputBlobMethod;
+typedef struct OutputBlobArray OutputBlobArray;
+typedef struct OutputBlobContext OutputBlobContext;
+typedef struct ImageInference ImageInference;
+typedef struct ImageInferenceContext ImageInferenceContext;
+
+/**
+ * \brief Callback function when a inference request completed.
+ * Image inference backend takes charge of memory management for @param Blobs and @param frames
+ * Caller is responsible to every IFramePtr after reinterpreted as customed data structure
+ */
+typedef void (*CallbackFunc)(OutputBlobArray *Blobs, UserDataBuffers *frames);
+typedef void (*PreProcessor)(Image *image);
+
+struct ImageInference {
+    /* image inference backend name. Must be non-NULL and unique among backends. */
+    const char *name;
+
+    /* create image inference engine */
+    int (*Create)(ImageInferenceContext *ctx, MemoryType type, const char *devices, const char *model, int batch_size,
+                  int nireq, const char *config, void *allocator, CallbackFunc callback);
+
+    /* create image inference engine w/ asynchronous input preprocessing */
+    int (*CreateAsyncPreproc)(ImageInferenceContext *async_preproc_context, ImageInferenceContext *inference_context,
+                              PreProcContext *preproc_context, int image_queue_size, void *opaque);
+
+    /* submit image */
+    void (*SubmitImage)(ImageInferenceContext *ctx, const Image *image, IFramePtr user_data,
+                        PreProcessor pre_processor);
+
+    const char *(*GetModelName)(ImageInferenceContext *ctx);
+
+    void (*GetModelInputInfo)(ImageInferenceContext *ctx, int *width, int *height, int *format);
+
+    int (*IsQueueFull)(ImageInferenceContext *ctx);
+
+    int (*ResourceStatus)(ImageInferenceContext *ctx);
+
+    void (*Flush)(ImageInferenceContext *ctx);
+
+    void (*Close)(ImageInferenceContext *ctx);
+
+    int priv_size; ///< size of private data to allocate for the backend
+};
+
+struct ImageInferenceContext {
+    const ImageInference *inference;
+    const OutputBlobMethod *output_blob_method;
+    char *name;
+    void *priv;
+};
+
+struct OutputBlobMethod {
+    /* output blob method name. Must be non-NULL and unique among output blob methods. */
+    const char *name;
+
+    const char *(*GetOutputLayerName)(OutputBlobContext *ctx);
+
+    IILayout (*GetLayout)(OutputBlobContext *ctx);
+
+    IIPrecision (*GetPrecision)(OutputBlobContext *ctx);
+
+    Dimensions (*GetDims)(OutputBlobContext *ctx);
+
+    const void *(*GetData)(OutputBlobContext *ctx);
+
+    int priv_size; ///< size of private data to allocate for the output blob
+};
+
+struct OutputBlobContext {
+    const OutputBlobMethod *output_blob_method;
+    void *priv;
+};
+
+struct OutputBlobArray {
+    OutputBlobContext **output_blobs;
+    int num_blobs;
+};
+
+#define __STRING(x) #x
+
+#ifdef __cplusplus
+#define __CONFIG_KEY(name) KEY_##name
+#define __DECLARE_CONFIG_KEY(name) static const char *__CONFIG_KEY(name) = __STRING(name)
+__DECLARE_CONFIG_KEY(CPU_EXTENSION);          // library with implementation of custom layers
+__DECLARE_CONFIG_KEY(CPU_THREADS_NUM);        // threads number CPU plugin use for inference
+__DECLARE_CONFIG_KEY(CPU_THROUGHPUT_STREAMS); // number inference requests running in parallel
+__DECLARE_CONFIG_KEY(RESIZE_BY_INFERENCE);    // experimental, don't use
+#else
+#define KEY_CPU_EXTENSION __STRING(CPU_EXTENSION)                   // library with implementation of custom layers
+#define KEY_CPU_THREADS_NUM __STRING(CPU_THREADS_NUM)               // threads number CPU plugin use for inference
+#define KEY_CPU_THROUGHPUT_STREAMS __STRING(CPU_THROUGHPUT_STREAMS) // number inference requests running in parallel
+#define KEY_PRE_PROCESSOR_TYPE __STRING(PRE_PROCESSOR_TYPE)         // preprocessor, e.g. ie, ffmpeg, opencv, g-api etc.
+#define KEY_IMAGE_FORMAT __STRING(IMAGE_FORMAT)                     // image format, e.g. NV12, BGR, RGB etc.
+#endif
+
+const ImageInference *image_inference_get_by_name(const char *name);
+
+ImageInferenceContext *image_inference_alloc(const ImageInference *infernce, const OutputBlobMethod *blob,
+                                             const char *instance_name);
+
+void image_inference_free(ImageInferenceContext *inference_context);
+
+const OutputBlobMethod *output_blob_method_get_by_name(const char *name);
+
+OutputBlobContext *output_blob_alloc(const OutputBlobMethod *method);
+
+void output_blob_free(OutputBlobContext *context);
+
+void image_inference_dynarray_add(void *tab_ptr, int *nb_ptr, void *elem);
diff --git a/libavfilter/inference_backend/image_inference_async_preproc.c b/libavfilter/inference_backend/image_inference_async_preproc.c
new file mode 100644
index 0000000..5ab87ce
--- /dev/null
+++ b/libavfilter/inference_backend/image_inference_async_preproc.c
@@ -0,0 +1,245 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <assert.h>
+#include <string.h>
+
+#include "image_inference.h"
+#include "image_inference_async_preproc.h"
+#include "logger.h"
+
+#define MOCKER_PRE_PROC_MAGIC 0x47474747
+
+static void *AsyncPreprocWorkingFunction(void *arg);
+
+static void PreprocImagesFree(PreprocImage **imgs, size_t num_imgs) {
+    if (!imgs || !num_imgs)
+        return;
+
+    for (size_t i = 0; i < num_imgs; i++) {
+        if (imgs[i]) {
+            image_map_free(imgs[i]->img_map_ctx);
+            free(imgs[i]);
+        }
+    }
+    free(imgs);
+}
+
+static int ImageInferenceAsyncPreprocCreate(ImageInferenceContext *async_preproc_context,
+                                            ImageInferenceContext *inference_context, PreProcContext *preproc_context,
+                                            int image_queue_size, void *opaque) {
+    int ret = 0;
+    int width = 0, height = 0, format = 0;
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)async_preproc_context->priv;
+    PreProcInitParam pp_init_param = {};
+    assert(inference_context && preproc_context);
+
+    VAII_INFO("Using async preproc image inference.");
+
+    async_preproc->actual = inference_context;
+    async_preproc->pre_proc = preproc_context;
+
+    // TODO: create image pool
+    async_preproc->preproc_images = (PreprocImage **)malloc(image_queue_size * sizeof(*async_preproc->preproc_images));
+    if (!async_preproc->preproc_images) {
+        VAII_ERROR("Creat preproc images failed!");
+        goto err;
+    }
+    async_preproc->num_preproc_images = image_queue_size;
+
+    async_preproc->freeImages = SafeQueueCreate();
+    async_preproc->workingImages = SafeQueueCreate();
+    if (!async_preproc->freeImages || !async_preproc->workingImages) {
+        VAII_ERROR("Creat images queues failed!");
+        goto err;
+    }
+
+    inference_context->inference->GetModelInputInfo(inference_context, &width, &height, &format);
+    pp_init_param.va_display = opaque;
+    pp_init_param.num_surfaces = image_queue_size;
+    pp_init_param.width = width;
+    pp_init_param.height = height;
+    pp_init_param.format = format;
+
+    if (preproc_context->pre_proc->Init)
+        preproc_context->pre_proc->Init(preproc_context, &pp_init_param);
+
+    for (size_t n = 0; n < async_preproc->num_preproc_images; n++) {
+        PreprocImage *preproc_image = (PreprocImage *)malloc(sizeof(*preproc_image));
+        if (!preproc_image)
+            goto err;
+        memset(preproc_image, 0, sizeof(*preproc_image));
+
+        preproc_image->image.type = MEM_TYPE_ANY;
+        preproc_image->image.width = width;
+        preproc_image->image.height = height;
+        preproc_image->image.format = format;
+        if (MOCKER_PRE_PROC_MAGIC == (uint32_t)opaque)
+            preproc_image->img_map_ctx = image_map_alloc(image_map_get_by_name("mocker"));
+        else
+            preproc_image->img_map_ctx = image_map_alloc(image_map_get_by_name("vaapi"));
+        assert(preproc_image->img_map_ctx);
+        async_preproc->preproc_images[n] = preproc_image;
+        SafeQueuePush(async_preproc->freeImages, preproc_image);
+    }
+
+    ret = pthread_create(&async_preproc->async_thread, NULL, AsyncPreprocWorkingFunction, async_preproc_context);
+    if (ret != 0) {
+        VAII_ERROR("Create async preproc thread error!");
+        goto err;
+    }
+
+    return 0;
+err:
+    PreprocImagesFree(async_preproc->preproc_images, async_preproc->num_preproc_images);
+
+    if (async_preproc->freeImages)
+        SafeQueueDestroy(async_preproc->freeImages);
+    if (async_preproc->workingImages)
+        SafeQueueDestroy(async_preproc->workingImages);
+
+    return -1;
+}
+
+static int ImageInferenceAsyncPreprocCreateDummy(ImageInferenceContext *ctx, MemoryType type, const char *devices,
+                                                 const char *model, int batch_size, int nireq, const char *config,
+                                                 void *allocator, CallbackFunc callback) {
+    /* Leave empty */
+    return 0;
+}
+
+static void ImageInferenceAsyncPreprocSubmtImage(ImageInferenceContext *ctx, const Image *image, IFramePtr user_data,
+                                                 PreProcessor pre_processor) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+    PreProcContext *pp_ctx = async_preproc->pre_proc;
+    PreprocImage *pp_image = NULL;
+
+    pp_image = (PreprocImage *)SafeQueuePop(async_preproc->freeImages);
+    pp_ctx->pre_proc->Convert(pp_ctx, image, &pp_image->image, 1);
+    pp_image->user_data = user_data;
+    pp_image->pre_processor = pre_processor;
+    SafeQueuePush(async_preproc->workingImages, pp_image);
+}
+
+static const char *ImageInferenceAsyncPreprocGetModelName(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+    return infer->GetModelName(infer_ctx);
+}
+
+static int ImageInferenceAsyncPreprocIsQueueFull(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+    return infer->IsQueueFull(infer_ctx);
+}
+
+static int ImageInferenceResourceStatus(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    return infer_ctx->inference->ResourceStatus(infer_ctx);
+}
+
+static void ImageInferenceAsyncPreprocFlush(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+    // Since async image preproc is working in another independent thread,
+    // have to wait for all working images be processed and sent to inference engine,
+    // or the flushing cannot assure the un-batched frames can be used and released
+    SafeQueueWaitEmpty(async_preproc->workingImages);
+    return infer->Flush(infer_ctx);
+}
+
+static void ImageInferenceAsyncPreprocClose(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+    PreProcContext *pp_ctx = async_preproc->pre_proc;
+
+    if (async_preproc->async_thread) {
+        // add one empty request
+        PreprocImage pp_image = {};
+        SafeQueuePush(async_preproc->workingImages, &pp_image);
+        // wait for thread reaching empty request
+        pthread_join(async_preproc->async_thread, NULL);
+    }
+
+    for (size_t n = 0; n < async_preproc->num_preproc_images; n++) {
+        PreprocImage *pp_image = async_preproc->preproc_images[n];
+        pp_ctx->pre_proc->ReleaseImage(pp_ctx, &pp_image->image);
+    }
+
+    infer->Close(infer_ctx);
+    image_inference_free(infer_ctx);
+    pp_ctx->pre_proc->Destroy(pp_ctx);
+    pre_proc_free(pp_ctx);
+
+    PreprocImagesFree(async_preproc->preproc_images, async_preproc->num_preproc_images);
+
+    if (async_preproc->freeImages)
+        SafeQueueDestroy(async_preproc->freeImages);
+    if (async_preproc->workingImages)
+        SafeQueueDestroy(async_preproc->workingImages);
+}
+
+static void *AsyncPreprocWorkingFunction(void *arg) {
+    ImageInferenceContext *ctx = (ImageInferenceContext *)arg;
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+
+    while (1) {
+        PreprocImage *pp_image = (PreprocImage *)SafeQueueFront(async_preproc->workingImages);
+        assert(pp_image);
+
+        // empty ctx means ending
+        if (!pp_image->img_map_ctx)
+            break;
+
+        {
+            ImageMapContext *map_ctx = pp_image->img_map_ctx;
+            Image image_sys = map_ctx->mapper->Map(map_ctx, &pp_image->image);
+            infer->SubmitImage(infer_ctx, &image_sys, pp_image->user_data, pp_image->pre_processor);
+            map_ctx->mapper->Unmap(map_ctx);
+        }
+        pp_image = (PreprocImage *)SafeQueuePop(async_preproc->workingImages);
+        SafeQueuePush(async_preproc->freeImages, pp_image);
+    }
+
+    return NULL;
+}
+
+ImageInference image_inference_async_preproc = {
+    .name = "async_preproc",
+    .priv_size = sizeof(ImageInferenceAsyncPreproc),
+    .CreateAsyncPreproc = ImageInferenceAsyncPreprocCreate,
+    .Create = ImageInferenceAsyncPreprocCreateDummy,
+    .SubmitImage = ImageInferenceAsyncPreprocSubmtImage,
+    .GetModelName = ImageInferenceAsyncPreprocGetModelName,
+    .IsQueueFull = ImageInferenceAsyncPreprocIsQueueFull,
+    .ResourceStatus = ImageInferenceResourceStatus,
+    .Flush = ImageInferenceAsyncPreprocFlush,
+    .Close = ImageInferenceAsyncPreprocClose,
+};
diff --git a/libavfilter/inference_backend/image_inference_async_preproc.h b/libavfilter/inference_backend/image_inference_async_preproc.h
new file mode 100644
index 0000000..89ccd0c
--- /dev/null
+++ b/libavfilter/inference_backend/image_inference_async_preproc.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "image_inference.h"
+#include "pre_proc.h"
+#include "safe_queue.h"
+#include <pthread.h>
+
+typedef struct PreprocImage {
+    Image image;
+    ImageMapContext *img_map_ctx;
+    IFramePtr user_data;        // Pass through to wrapped inference backend
+    PreProcessor pre_processor; // Pass through to wrapped inference backend
+} PreprocImage;
+
+typedef struct ImageInferenceAsyncPreproc {
+    ImageInferenceContext *actual;
+    PreProcContext *pre_proc;
+
+    PreprocImage **preproc_images;
+    size_t num_preproc_images;
+
+    // Threading
+    pthread_t async_thread;
+    SafeQueueT *freeImages;    // PreprocImage queue
+    SafeQueueT *workingImages; // PreprocImage queue
+} ImageInferenceAsyncPreproc;
\ No newline at end of file
diff --git a/libavfilter/inference_backend/logger.c b/libavfilter/inference_backend/logger.c
new file mode 100644
index 0000000..576d8a9
--- /dev/null
+++ b/libavfilter/inference_backend/logger.c
@@ -0,0 +1,57 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "logger.h"
+#include <stdio.h>
+#include <string.h>
+
+static VAIILogFuncPtr inference_log_function = default_log_function;
+
+void set_log_function(VAIILogFuncPtr log_func) {
+    inference_log_function = log_func;
+}
+
+void debug_log(int level, const char *file, const char *function, int line, const char *message) {
+    (*inference_log_function)(level, file, function, line, message);
+}
+
+void default_log_function(int level, const char *file, const char *function, int line, const char *message) {
+    const char log_level[][16] = {"DEFAULT", "ERROR", "WARNING", "INFO", "VERBOSE", "DEBUG", "TRACE", "MEMDUMP"};
+    fprintf(stderr, "%s \t %s:%i : %s \t %s \n", (char *)&log_level[level], file, line, function, message);
+}
+
+static VAIITraceFuncPtr inference_trace_function = default_trace_function;
+
+void set_trace_function(VAIITraceFuncPtr trace_func) {
+    inference_trace_function = trace_func;
+}
+
+void trace_log(int level, const char *fmt, ...) {
+    va_list args;
+    va_start(args, fmt);
+    (*inference_trace_function)(level, fmt, args);
+    va_end(args);
+}
+
+void default_trace_function(int level, const char *fmt, va_list vl) {
+    const char log_level[][16] = {"DEFAULT", "ERROR", "WARNING", "INFO", "VERBOSE", "DEBUG", "TRACE", "MEMDUMP"};
+    fprintf(stderr, "%s \t", (char *)&log_level[level]);
+    vprintf(fmt, vl);
+}
\ No newline at end of file
diff --git a/libavfilter/inference_backend/logger.h b/libavfilter/inference_backend/logger.h
new file mode 100644
index 0000000..71b6d4c
--- /dev/null
+++ b/libavfilter/inference_backend/logger.h
@@ -0,0 +1,88 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include <stdarg.h>
+
+enum {
+    VAII_ERROR_LOG_LEVEL = 1,
+    VAII_WARNING_LOG_LEVEL,
+    VAII_INFO_LOG_LEVEL,
+    VAII_VERBOSE_LOG_LEVEL,
+    VAII_DEBUG_LOG_LEVEL,
+    VAII_TRACE_LOG_LEVEL,
+    VAII_MEMDUMP_LOG_LEVEL,
+};
+
+#define VAII_DEBUG_LOG(level, message) debug_log(level, __FILE__, __FUNCTION__, __LINE__, message);
+
+#define VAII_MEMDUMP(message) VAII_DEBUG_LOG(VAII_MEMDUMP_LOG_LEVEL, message);
+#define VAII_TRACE(message) VAII_DEBUG_LOG(VAII_TRACE_LOG_LEVEL, message);
+#define VAII_DEBUG(message) VAII_DEBUG_LOG(VAII_DEBUG_LOG_LEVEL, message);
+#define VAII_INFO(message) VAII_DEBUG_LOG(VAII_INFO_LOG_LEVEL, message);
+#define VAII_FIXME(message) VAII_DEBUG_LOG(VAII_FIXME_LOG_LEVEL, message);
+#define VAII_WARNING(message) VAII_DEBUG_LOG(VAII_WARNING_LOG_LEVEL, message);
+#define VAII_ERROR(message) VAII_DEBUG_LOG(VAII_ERROR_LOG_LEVEL, message);
+
+#define VAII_LOGE(f_, ...) trace_log(VAII_ERROR_LOG_LEVEL, (f_), ##__VA_ARGS__);
+#define VAII_LOGW(f_, ...) trace_log(VAII_WARNING_LOG_LEVEL, (f_), ##__VA_ARGS__);
+#define VAII_LOGI(f_, ...) trace_log(VAII_INFO_LOG_LEVEL, (f_), ##__VA_ARGS__);
+#define VAII_LOGV(f_, ...) trace_log(VAII_VERBOSE_LOG_LEVEL, (f_), ##__VA_ARGS__);
+#define VAII_LOGD(f_, ...) trace_log(VAII_DEBUG_LOG_LEVEL, (f_), ##__VA_ARGS__);
+
+typedef void (*VAIILogFuncPtr)(int level, const char *file, const char *function, int line, const char *message);
+
+void set_log_function(VAIILogFuncPtr log_func);
+
+void debug_log(int level, const char *file, const char *function, int line, const char *message);
+
+void default_log_function(int level, const char *file, const char *function, int line, const char *message);
+
+typedef void (*VAIITraceFuncPtr)(int, const char *, va_list);
+
+void set_trace_function(VAIITraceFuncPtr trace_func);
+
+void trace_log(int level, const char *fmt, ...);
+
+void default_trace_function(int level, const char *fmt, va_list vl);
+
+#if defined(HAVE_ITT)
+#include "ittnotify.h"
+#include <string>
+
+static __itt_domain *itt_domain = NULL;
+inline void taskBegin(const char *name) {
+    if (itt_domain == NULL) {
+        itt_domain = __itt_domain_create("video-analytics");
+    }
+    __itt_task_begin(itt_domain, __itt_null, __itt_null, __itt_string_handle_create(name));
+}
+
+inline void taskEnd(void) {
+    __itt_task_end(itt_domain);
+}
+
+#else
+
+#define taskBegin(x)
+#define taskEnd(x)
+
+#endif
diff --git a/libavfilter/inference_backend/metaconverter.c b/libavfilter/inference_backend/metaconverter.c
new file mode 100644
index 0000000..9373212
--- /dev/null
+++ b/libavfilter/inference_backend/metaconverter.c
@@ -0,0 +1,320 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "metaconverter.h"
+#include "ff_base_inference.h"
+#include "libavutil/avassert.h"
+#include "logger.h"
+
+static int convert_roi_detection(json_object *info_object, AVFrame *frame) {
+    AVFrameSideData *detect_sd, *classify_sd;
+    InferDetectionMeta *d_meta;
+    InferClassificationMeta *c_meta = NULL;
+    BBoxesArray *boxes = NULL;
+    json_object *objects;
+
+    detect_sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_DETECTION);
+    classify_sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION);
+
+    if (!detect_sd)
+        return 0;
+
+    d_meta = (InferDetectionMeta *)detect_sd->data;
+    if (!d_meta)
+        return 0;
+
+    boxes = d_meta->bboxes;
+    if (boxes == NULL)
+        return 0;
+
+    if (classify_sd)
+        c_meta = (InferClassificationMeta *)classify_sd->data;
+
+    // convert roi detection to json objects
+    objects = json_object_new_array();
+    for (size_t i = 0; i < boxes->num; i++) {
+        LabelsArray *labels = NULL;
+        int label_id = -1;
+        json_object *box_object, *detection_object, *assembled_object;
+
+        assembled_object = json_object_new_object();
+
+        label_id = boxes->bbox[i]->label_id;
+        if (boxes->bbox[i]->label_buf) {
+            labels = (LabelsArray *)boxes->bbox[i]->label_buf->data;
+        }
+
+        detection_object = json_object_new_object();
+
+        box_object = json_object_new_object();
+        json_object_object_add(box_object, "x_min", json_object_new_double(boxes->bbox[i]->x_min));
+        json_object_object_add(box_object, "y_min", json_object_new_double(boxes->bbox[i]->y_min));
+        json_object_object_add(box_object, "x_max", json_object_new_double(boxes->bbox[i]->x_max));
+        json_object_object_add(box_object, "y_max", json_object_new_double(boxes->bbox[i]->y_max));
+        json_object_object_add(detection_object, "bounding_box", box_object);
+        if (labels)
+            json_object_object_add(detection_object, "label", json_object_new_string(labels->label[label_id]));
+        json_object_object_add(detection_object, "confidence",
+                               json_object_new_double((double)boxes->bbox[i]->confidence));
+        json_object_object_add(detection_object, "label_id", json_object_new_int(label_id));
+
+        json_object_object_add(assembled_object, "detection", detection_object);
+
+        if (c_meta) {
+            for (size_t k = 0; k < c_meta->c_array->num; k++) {
+                InferClassification *c = c_meta->c_array->classifications[k];
+                json_object *new_object, *model_object;
+
+                if (c->detect_id != i) {
+                    continue;
+                }
+
+                // skip worthless classification meta
+                if (!strcmp(c->name, "default"))
+                    continue;
+
+                new_object = json_object_new_object();
+
+                if (c->attributes) {
+                    json_object_object_add(new_object, "label", json_object_new_string(c->attributes));
+                } else {
+                    if (c->label_buf) {
+                        LabelsArray *array = (LabelsArray *)c->label_buf->data;
+                        json_object_object_add(new_object, "label", json_object_new_string(array->label[c->label_id]));
+                    }
+                }
+
+                model_object = json_object_new_object();
+                json_object_object_add(model_object, "name", json_object_new_string(c->tensor.model_name));
+                json_object_object_add(new_object, "model", model_object);
+
+                json_object_object_add(assembled_object, c->name, new_object);
+            }
+        }
+
+        json_object_object_add(assembled_object, "h", json_object_new_int(boxes->bbox[i]->roi_meta.h));
+        json_object_object_add(assembled_object, "w", json_object_new_int(boxes->bbox[i]->roi_meta.w));
+        json_object_object_add(assembled_object, "x", json_object_new_int(boxes->bbox[i]->roi_meta.x));
+        json_object_object_add(assembled_object, "y", json_object_new_int(boxes->bbox[i]->roi_meta.y));
+        json_object_array_add(objects, assembled_object);
+    }
+    json_object_object_add(info_object, "objects", objects);
+
+    return 1;
+}
+
+static int convert_roi_tensor(json_object *info_object, AVFrame *frame) {
+    AVFrameSideData *detect_sd, *classify_sd;
+    InferDetectionMeta *d_meta;
+    InferClassificationMeta *c_meta = NULL;
+    BBoxesArray *boxes = NULL;
+    json_object *tensors_objects;
+
+    detect_sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_DETECTION);
+    classify_sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION);
+    if (!detect_sd)
+        return 0;
+
+    d_meta = (InferDetectionMeta *)detect_sd->data;
+    if (!d_meta)
+        return 0;
+
+    boxes = d_meta->bboxes;
+    if (boxes == NULL)
+        return 0;
+
+    if (classify_sd)
+        c_meta = (InferClassificationMeta *)classify_sd->data;
+
+    // convert roi tensors to json objects
+    tensors_objects = json_object_new_array();
+    for (size_t i = 0; i < boxes->num; i++) {
+        json_object *box_object;
+
+        box_object = json_object_new_object();
+        json_object_object_add(box_object, "confidence", json_object_new_double(boxes->bbox[i]->confidence));
+        json_object_object_add(box_object, "label_id", json_object_new_int(boxes->bbox[i]->label_id));
+        json_object_object_add(box_object, "layer_name", json_object_new_string(boxes->bbox[i]->tensor.layer_name));
+        json_object_object_add(box_object, "layout", json_object_new_string(boxes->bbox[i]->tensor.layout));
+        json_object_object_add(box_object, "model_name", json_object_new_string(boxes->bbox[i]->tensor.model_name));
+        json_object_object_add(box_object, "name", json_object_new_string("detection"));
+        json_object_object_add(box_object, "precision", json_object_new_string(boxes->bbox[i]->tensor.precision));
+        json_object_array_add(tensors_objects, box_object);
+
+        if (c_meta) {
+            for (size_t k = 0; k < c_meta->c_array->num; k++) {
+                InferClassification *c = c_meta->c_array->classifications[k];
+                json_object *assembled_object;
+
+                if (c->detect_id != i) {
+                    continue;
+                }
+
+                assembled_object = json_object_new_object();
+                if (c->confidence > 0)
+                    json_object_object_add(assembled_object, "confidence", json_object_new_double(c->confidence));
+
+                if (c->tensor.buffer) {
+                    json_object *data = json_object_new_array();
+                    if (!strcmp(c->tensor.precision, "FP32")) {
+                        float *p = (float *)c->tensor.buffer->data;
+                        for (int i = 0; i < c->tensor.buffer->size / sizeof(p[0]); i++) {
+                            json_object *item = json_object_new_double(*(p + i));
+                            json_object_array_add(data, item);
+                        }
+                    } else {
+                        uint8_t *p = (uint8_t *)c->tensor.buffer->data;
+                        for (int i = 0; i < c->tensor.buffer->size; i++) {
+                            json_object *item = json_object_new_int(*(p + i));
+                            json_object_array_add(data, item);
+                        }
+                    }
+                    json_object_object_add(assembled_object, "data", data);
+                }
+
+                if (c->attributes) {
+                    json_object_object_add(assembled_object, "label", json_object_new_string(c->attributes));
+                } else {
+                    if (c->label_buf) {
+                        LabelsArray *array = (LabelsArray *)c->label_buf->data;
+                        json_object_object_add(assembled_object, "label",
+                                               json_object_new_string(array->label[c->label_id]));
+                        json_object_object_add(assembled_object, "label_id", json_object_new_int(c->label_id));
+                    }
+                }
+                json_object_object_add(assembled_object, "layer_name", json_object_new_string(c->tensor.layer_name));
+                json_object_object_add(assembled_object, "layout", json_object_new_string(c->tensor.layout));
+                json_object_object_add(assembled_object, "model_name", json_object_new_string(c->tensor.model_name));
+                json_object_object_add(assembled_object, "name", json_object_new_string(c->name));
+                json_object_object_add(assembled_object, "precision", json_object_new_string(c->tensor.precision));
+                json_object_array_add(tensors_objects, assembled_object);
+            }
+        }
+    }
+    json_object_object_add(info_object, "tensors", tensors_objects);
+
+    return 1;
+}
+
+static int tensors_to_file(AVFilterContext *ctx, AVFrame *frame, json_object *info_object, const char *location,
+                           const char *tags, add_data_function add_frame_data) {
+    AVFrameSideData *sd;
+    InferClassificationMeta *c_meta;
+
+    static uint32_t frame_num = 0;
+
+    UNUSED(ctx);
+    UNUSED(info_object);
+    UNUSED(add_frame_data);
+
+    av_assert0(location && tags);
+
+    if (!(sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION)))
+        return 0;
+
+    c_meta = (InferClassificationMeta *)sd->data;
+
+    if (c_meta) {
+        int i;
+        uint32_t index = 0;
+        char filename[1024] = {0};
+        const int meta_num = c_meta->c_array->num;
+        for (i = 0; i < meta_num; i++) {
+            FILE *f = NULL;
+            InferClassification *c = c_meta->c_array->classifications[i];
+            if (!c->tensor.buffer || !c->tensor.buffer->data)
+                continue;
+
+            snprintf(filename, sizeof(filename), "%s/%s_frame_%u_idx_%u.tensor", location, tags ? tags : "default",
+                     frame_num, index);
+            f = fopen(filename, "wb");
+            if (!f) {
+                VAII_LOGW("Failed to open/create file: %s\n", filename);
+            } else {
+                fwrite(c->tensor.buffer->data, sizeof(float), c->tensor.buffer->size / sizeof(float), f);
+                fclose(f);
+            }
+            index++;
+        }
+    }
+
+    frame_num++;
+    return 0;
+}
+
+static int detection_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object, const char *location,
+                             const char *tags, add_data_function add_frame_data) {
+    int ret;
+    UNUSED(location);
+    UNUSED(tags);
+
+    if (add_frame_data)
+        add_frame_data(ctx, frame, info_object);
+
+    ret = convert_roi_detection(info_object, frame);
+    return ret;
+}
+
+static int tensor_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object, const char *location,
+                          const char *tags, add_data_function add_frame_data) {
+    int ret;
+    UNUSED(location);
+    UNUSED(tags);
+
+    if (add_frame_data)
+        add_frame_data(ctx, frame, info_object);
+
+    ret = convert_roi_tensor(info_object, frame);
+    return ret;
+}
+
+static int all_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object, const char *location,
+                       const char *tags, add_data_function add_frame_data) {
+    int ret1, ret2;
+    UNUSED(location);
+    UNUSED(tags);
+
+    if (add_frame_data)
+        add_frame_data(ctx, frame, info_object);
+    ret1 = convert_roi_detection(info_object, frame);
+    ret2 = convert_roi_tensor(info_object, frame);
+    return (ret1 + ret2);
+}
+
+convert_function_type ffva_metaconvert_get_convert_func(FFVAMetaconvertConverterType converter,
+                                                        FFVAMetaconvertMethodType method) {
+    switch (converter) {
+    case FFVA_METACONVERT_JSON: {
+        switch (method) {
+        case FFVA_METACONVERT_ALL:
+            return all_to_json;
+        case FFVA_METACONVERT_DETECTION:
+            return detection_to_json;
+        case FFVA_METACONVERT_TENSOR:
+            return tensor_to_json;
+        };
+        break;
+    }
+    case FFVA_METACONVERT_TENSORS_TO_FILE:
+        return tensors_to_file;
+    };
+
+    return NULL;
+}
diff --git a/libavfilter/inference_backend/metaconverter.h b/libavfilter/inference_backend/metaconverter.h
new file mode 100644
index 0000000..3370b83
--- /dev/null
+++ b/libavfilter/inference_backend/metaconverter.h
@@ -0,0 +1,52 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "libavfilter/avfilter.h"
+#include <json-c/json.h>
+#include <libavutil/frame.h>
+#include "ff_base_inference.h"
+
+typedef enum {
+    FFVA_METACONVERT_TENSOR2TEXT,
+    FFVA_METACONVERT_JSON,
+    FFVA_METACONVERT_DUMP_DETECTION,
+    FFVA_METACONVERT_DUMP_CLASSIFICATION,
+    FFVA_METACONVERT_DUMP_TENSORS,
+    FFVA_METACONVERT_TENSORS_TO_FILE,
+    FFVA_METACONVERT_ADD_FULL_FRAME_ROI
+} FFVAMetaconvertConverterType;
+
+typedef enum {
+    FFVA_METACONVERT_ALL,
+    FFVA_METACONVERT_DETECTION,
+    FFVA_METACONVERT_TENSOR,
+    // FFVA_METACONVERT_MAX,
+    // FFVA_METACONVERT_INDEX,
+    // FFVA_METACONVERT_COMPOUND
+} FFVAMetaconvertMethodType;
+
+typedef void (*add_data_function)(AVFilterContext *ctx, AVFrame *frame, json_object *output_json);
+
+typedef int (*convert_function_type)(AVFilterContext *ctx, AVFrame *frame, json_object *info_object,
+                                     const char *location, const char *tags, add_data_function add_frame_data);
+
+convert_function_type ffva_metaconvert_get_convert_func(FFVAMetaconvertConverterType, FFVAMetaconvertMethodType);
diff --git a/libavfilter/inference_backend/model_proc.c b/libavfilter/inference_backend/model_proc.c
new file mode 100644
index 0000000..88490f4
--- /dev/null
+++ b/libavfilter/inference_backend/model_proc.c
@@ -0,0 +1,275 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "model_proc.h"
+#include "libavutil/avassert.h"
+#include "logger.h"
+#include <json-c/json.h>
+
+// helper functions
+static void infer_labels_dump(uint8_t *data) {
+    int i;
+    LabelsArray *labels = (LabelsArray *)data;
+    printf("labels: ");
+    for (i = 0; i < labels->num; i++)
+        printf("%s ", labels->label[i]);
+    printf("\n");
+}
+
+void infer_labels_buffer_free(void *opaque, uint8_t *data) {
+    int i;
+    LabelsArray *labels = (LabelsArray *)data;
+
+    for (i = 0; i < labels->num; i++)
+        av_freep(&labels->label[i]);
+
+    av_free(labels->label);
+
+    av_free(data);
+}
+
+int model_proc_get_file_size(FILE *fp) {
+    int file_size, current_pos;
+
+    if (!fp)
+        return -1;
+
+    current_pos = ftell(fp);
+
+    if (fseek(fp, 0, SEEK_END)) {
+        fprintf(stderr, "Couldn't seek to the end of feature file.\n");
+        return -1;
+    }
+
+    file_size = ftell(fp);
+
+    fseek(fp, current_pos, SEEK_SET);
+
+    return file_size;
+}
+/*
+ * model proc parsing functions using JSON-c
+ */
+void *model_proc_read_config_file(const char *path) {
+    int n, file_size;
+    json_object *proc_config = NULL;
+    uint8_t *proc_json = NULL;
+    json_tokener *tok = NULL;
+
+    FILE *fp = fopen(path, "rb");
+    if (!fp) {
+        fprintf(stderr, "File open error:%s\n", path);
+        return NULL;
+    }
+
+    file_size = model_proc_get_file_size(fp);
+
+    proc_json = av_mallocz(file_size + 1);
+    if (!proc_json)
+        goto end;
+
+    n = fread(proc_json, file_size, 1, fp);
+
+    UNUSED(n);
+
+    tok = json_tokener_new();
+    proc_config = json_tokener_parse_ex(tok, proc_json, file_size);
+    if (proc_config == NULL) {
+        enum json_tokener_error jerr;
+        jerr = json_tokener_get_error(tok);
+        fprintf(stderr, "Error before: %s\n", json_tokener_error_desc(jerr));
+        goto end;
+    }
+
+end:
+    if (proc_json)
+        av_freep(&proc_json);
+    if (tok)
+        json_tokener_free(tok);
+    fclose(fp);
+    return proc_config;
+}
+
+void model_proc_load_default_config_file(ModelInputPreproc *preproc, ModelOutputPostproc *postproc) {
+    if (preproc) {
+        /*
+         * format is a little tricky, an ideal input format for IE is BGR planer
+         * however, neither soft csc nor hardware vpp could support that format.
+         * Here, we set a close soft format. The actual one coverted before sent
+         * to IE will be decided by user config and hardware vpp used or not.
+         */
+        preproc->color_format = AV_PIX_FMT_BGR24;
+        preproc->layer_name = NULL;
+    }
+
+    if (postproc) {
+        // do nothing
+    }
+}
+
+int model_proc_parse_input_preproc(const void *json, ModelInputPreproc *m_preproc) {
+    json_object *jvalue, *preproc, *color, *layer, *object_class;
+    int ret;
+
+    ret = json_object_object_get_ex((json_object *)json, "input_preproc", &preproc);
+    if (!ret) {
+        VAII_DEBUG("No input_preproc.\n");
+        return 0;
+    }
+
+    // not support multiple inputs yet
+    av_assert0(json_object_array_length(preproc) <= 1);
+
+    jvalue = json_object_array_get_idx(preproc, 0);
+
+    ret = json_object_object_get_ex(jvalue, "color_format", &color);
+    if (ret) {
+        if (json_object_get_string(color) == NULL)
+            return -1;
+
+        VAII_LOGI("Color Format:\"%s\"\n", json_object_get_string(color));
+
+        if (!strcmp(json_object_get_string(color), "BGR"))
+            m_preproc->color_format = AV_PIX_FMT_BGR24;
+        else if (!strcmp(json_object_get_string(color), "RGB"))
+            m_preproc->color_format = AV_PIX_FMT_RGB24;
+        else
+            return -1;
+    }
+
+    ret = json_object_object_get_ex(jvalue, "object_class", &object_class);
+    if (ret) {
+        if (json_object_get_string(object_class) == NULL)
+            return -1;
+
+        VAII_LOGI("Object_class:\"%s\"\n", json_object_get_string(object_class));
+
+        m_preproc->object_class = (char *)json_object_get_string(object_class);
+    }
+
+    ret = json_object_object_get_ex(jvalue, "layer_name", &layer);
+    UNUSED(layer);
+
+    return 0;
+}
+
+// For detection, we now care labels only.
+// Layer name and type can be got from output blob.
+int model_proc_parse_output_postproc(const void *json, ModelOutputPostproc *m_postproc) {
+    json_object *jvalue, *postproc;
+    json_object *attribute, *converter, *labels, *layer, *method, *threshold;
+    json_object *tensor_to_text_scale, *tensor_to_text_precision;
+    int ret;
+    size_t jarraylen;
+
+    ret = json_object_object_get_ex((json_object *)json, "output_postproc", &postproc);
+    if (!ret) {
+        VAII_DEBUG("No output_postproc.\n");
+        return 0;
+    }
+
+    jarraylen = json_object_array_length(postproc);
+    av_assert0(jarraylen <= MAX_MODEL_OUTPUT);
+
+    for (int i = 0; i < jarraylen; i++) {
+        OutputPostproc *proc = &m_postproc->procs[i];
+        jvalue = json_object_array_get_idx(postproc, i);
+
+#define FETCH_STRING(var, name)                                                                                        \
+    do {                                                                                                               \
+        ret = json_object_object_get_ex(jvalue, #name, &var);                                                          \
+        if (ret)                                                                                                       \
+            proc->name = (char *)json_object_get_string(var);                                                          \
+    } while (0)
+#define FETCH_DOUBLE(var, name)                                                                                        \
+    do {                                                                                                               \
+        ret = json_object_object_get_ex(jvalue, #name, &var);                                                          \
+        if (ret)                                                                                                       \
+            proc->name = (double)json_object_get_double(var);                                                          \
+    } while (0)
+#define FETCH_INTEGER(var, name)                                                                                       \
+    do {                                                                                                               \
+        ret = json_object_object_get_ex(jvalue, #name, &var);                                                          \
+        if (ret)                                                                                                       \
+            proc->name = (int)json_object_get_int(var);                                                                \
+    } while (0)
+
+        FETCH_STRING(layer, layer_name);
+        FETCH_STRING(method, method);
+        FETCH_STRING(attribute, attribute_name);
+        FETCH_STRING(converter, converter);
+
+        FETCH_DOUBLE(threshold, threshold);
+        FETCH_DOUBLE(tensor_to_text_scale, tensor_to_text_scale);
+
+        FETCH_INTEGER(tensor_to_text_precision, tensor_to_text_precision);
+
+        // handle labels
+        ret = json_object_object_get_ex(jvalue, "labels", &labels);
+        if (ret) {
+            json_object *label;
+            size_t labels_num = json_object_array_length(labels);
+
+            if (labels_num > 0) {
+                AVBufferRef *ref = NULL;
+                LabelsArray *larray = av_mallocz(sizeof(*larray));
+
+                if (!larray)
+                    return AVERROR(ENOMEM);
+
+                for (int i = 0; i < labels_num; i++) {
+                    char *copy = NULL;
+                    label = json_object_array_get_idx(labels, i);
+                    copy = av_strdup(json_object_get_string(label));
+                    av_dynarray_add(&larray->label, &larray->num, copy);
+                }
+
+                ref = av_buffer_create((uint8_t *)larray, sizeof(*larray), &infer_labels_buffer_free, NULL, 0);
+
+                proc->labels = ref;
+
+                if (ref)
+                    infer_labels_dump(ref->data);
+            }
+        }
+    }
+
+#undef FETCH_STRING
+#undef FETCH_DOUBLE
+#undef FETCH_INTEGER
+
+    return 0;
+}
+
+void model_proc_release_model_proc(const void *json, ModelInputPreproc *preproc, ModelOutputPostproc *postproc) {
+    size_t index = 0;
+
+    if (!json)
+        return;
+
+    if (postproc) {
+        for (index = 0; index < MAX_MODEL_OUTPUT; index++) {
+            if (postproc->procs[index].labels)
+                av_buffer_unref(&postproc->procs[index].labels);
+        }
+    }
+
+    json_object_put((json_object *)json);
+}
\ No newline at end of file
diff --git a/libavfilter/inference_backend/model_proc.h b/libavfilter/inference_backend/model_proc.h
new file mode 100644
index 0000000..e4289d4
--- /dev/null
+++ b/libavfilter/inference_backend/model_proc.h
@@ -0,0 +1,37 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "ff_base_inference.h"
+
+void *model_proc_read_config_file(const char *path);
+
+void model_proc_load_default_config_file(ModelInputPreproc *preproc, ModelOutputPostproc *postproc);
+
+int model_proc_parse_input_preproc(const void *json, ModelInputPreproc *m_preproc);
+
+int model_proc_parse_output_postproc(const void *json, ModelOutputPostproc *m_postproc);
+
+void model_proc_release_model_proc(const void *json, ModelInputPreproc *preproc, ModelOutputPostproc *postproc);
+
+int model_proc_get_file_size(FILE *fp);
+
+void infer_labels_buffer_free(void *opaque, uint8_t *data);
\ No newline at end of file
diff --git a/libavfilter/inference_backend/openvino_image_inference.c b/libavfilter/inference_backend/openvino_image_inference.c
new file mode 100644
index 0000000..ed77eda
--- /dev/null
+++ b/libavfilter/inference_backend/openvino_image_inference.c
@@ -0,0 +1,835 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <assert.h>
+#include <string.h>
+
+#include "image_inference.h"
+#include "logger.h"
+#include "openvino_image_inference.h"
+
+#define II_MAX(a, b) ((a) > (b) ? (a) : (b))
+#define II_MIN(a, b) ((a) > (b) ? (b) : (a))
+
+typedef enum { VPP_DEVICE_HW, VPP_DEVICE_SW } DEVICE_TYPE;
+
+static inline void* mallocz(size_t size) {
+    void *ptr = malloc(size);
+    if (ptr)
+        memset(ptr, 0, size);
+    return ptr;
+}
+
+static ie_config_t *StringToIEConfig(const char *configs, char **pre_processor_name, char **multi_device_list,
+                                     char **hetero_device_list, char **cpu_exetension, char**image_format) {
+
+    ie_config_t *config_res = NULL, *cfg_tmp = NULL;
+    char *key = NULL, *value = NULL, *configs_temp = NULL;
+
+    if (!configs) return NULL;
+
+    configs_temp = (char *)mallocz(strlen(configs) + 1);
+    assert(configs_temp);
+    strcpy(configs_temp, configs);
+
+    key = strtok(configs_temp, "=|\n");
+    value = strtok(NULL, "=|\n");
+    while (key && value) {
+        char *list = NULL;
+        if (strlen(key) == 0) {
+            continue;
+        }
+        if (!strcmp(key, "MULTI_DEVICE_PRIORITIES")) {
+            list = (char *)mallocz(strlen(value) + 1);
+            assert(list);
+            strcpy(list, value);
+            *multi_device_list = list;
+        } else if (!strcmp(key, "TARGET_FALLBACK")) {
+            list = (char *)mallocz(strlen(value) + 1);
+            assert(list);
+            strcpy(list, value);
+            *hetero_device_list = list;
+        } else if (!strcmp(key, "PRE_PROCESSOR_TYPE")) {
+            list = (char *)mallocz(strlen(value) + 1);
+            assert(list);
+            strcpy(list, value);
+            *pre_processor_name = list;
+        } else if (!strcmp(key, "CPU_EXTENSION")) {
+            list = (char *)mallocz(strlen(value) + 1);
+            assert(list);
+            strcpy(list, value);
+            *cpu_exetension = list;
+        } else if (!strcmp(key, "IMAGE_FORMAT")) {
+            list = (char *)mallocz(strlen(value) + 1);
+            assert(list);
+            strcpy(list, value);
+            *image_format = list;
+        } else {
+            ie_config_t *cfg_t = (ie_config_t *)malloc(sizeof(ie_config_t));
+            char *cfg_name = (char *)mallocz(strlen(key) + 1);
+            char *cfg_value = (char *)mallocz(strlen(value) + 1);
+            assert(cfg_t && cfg_name && cfg_value);
+            strcpy(cfg_name, key);
+            strcpy(cfg_value, value);
+
+            cfg_t->name = cfg_name;
+            cfg_t->value = cfg_value;
+            cfg_t->next = NULL;
+            if (!config_res) {
+                config_res = cfg_t;
+                cfg_tmp = config_res;
+            } else {
+                cfg_tmp->next = cfg_t;
+                cfg_tmp = cfg_tmp->next;
+            }
+        }
+        key = strtok(NULL, "=|\n");
+        value = strtok(NULL, "=|\n");
+    }
+
+    free(configs_temp);
+    configs_temp = NULL;
+    return config_res;
+}
+
+static void ie_config_free(ie_config_t *config) {
+    while (config) {
+        ie_config_t *_tmp = config;
+        config = _tmp->next;
+        free((char *)_tmp->name),
+        free((char *)_tmp->value);
+        _tmp->name = NULL, _tmp->value = NULL, _tmp->next = NULL;
+        free(_tmp);
+        _tmp = NULL;
+    }
+}
+
+static void completion_callback(void *args);
+
+static inline int getNumberChannels(int format) {
+    switch (format) {
+    case FOURCC_BGRA:
+    case FOURCC_BGRX:
+    case FOURCC_RGBA:
+    case FOURCC_RGBX:
+        return 4;
+    case FOURCC_BGR:
+        return 3;
+    }
+    return 0;
+}
+
+static colorformat_e FormatNameToIEColorFormat(const char *format) {
+    static const char *formats[] = {"NV12", "RGB", "BGR", "RGBX", "BGRX", "RGBA", "BGRA"};
+    const colorformat_e ie_color_formats[] = {NV12, RGB, BGR, RGBX, BGRX, RGBX, BGRX};
+
+    int num_formats = sizeof(formats) / sizeof(formats[0]);
+    for (int i = 0; i < num_formats; i++) {
+        if (!strcmp(format, formats[i]))
+            return ie_color_formats[i];
+    }
+
+    VAII_ERROR("Unsupported color format by Inference Engine preprocessing");
+    return RAW;
+}
+
+static inline void RectToIERoi(roi_t *roi, const Rectangle *rect) {
+    roi->id = 0;
+    roi->posX = rect->x;
+    roi->posY = rect->y;
+    roi->sizeX = rect->width;
+    roi->sizeY = rect->height;
+}
+
+static void GetNextImageBuffer(ImageInferenceContext *ctx, const BatchRequest *request, Image *image) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    dimensions_t blob_dims = {};
+    int batchIndex, plane_size;
+    ie_blob_t *input_blob = NULL;
+    ie_blob_buffer_t blob_buffer;
+
+    VAII_DEBUG(__FUNCTION__);
+
+    ie_infer_request_get_blob(request->infer_request, vino->input_name, &input_blob);
+    ie_blob_get_dims(input_blob, &blob_dims);
+
+    memset(image, 0, sizeof(*image));
+    image->width = blob_dims.dims[3];  // W
+    image->height = blob_dims.dims[2]; // H
+    image->format = FOURCC_RGBP;
+    batchIndex = request->buffers.num_buffers;
+    plane_size = image->width * image->height;
+
+    ie_blob_get_buffer(input_blob, &blob_buffer);
+    image->planes[0] = (uint8_t *)blob_buffer.buffer + batchIndex * plane_size * blob_dims.dims[1];
+    image->planes[1] = image->planes[0] + plane_size;
+    image->planes[2] = image->planes[1] + plane_size;
+    image->stride[0] = image->width;
+    image->stride[1] = image->width;
+    image->stride[2] = image->width;
+    ie_blob_destroy(&input_blob);
+}
+
+static inline Image ApplyCrop(const Image *src) {
+    int planes_count;
+    int rect_x, rect_y, rect_width, rect_height;
+    Image dst = *src;
+    dst.rect = (Rectangle){0};
+
+    VAII_DEBUG(__FUNCTION__);
+
+    planes_count = GetPlanesCount(src->format);
+    if (!src->rect.width && !src->rect.height) {
+        dst = *src;
+        for (int i = 0; i < planes_count; i++)
+            dst.planes[i] = src->planes[i];
+        return dst;
+    }
+
+    if (src->rect.x >= src->width || src->rect.y >= src->height || src->rect.x + src->rect.width <= 0 ||
+        src->rect.y + src->rect.height <= 0) {
+        fprintf(stderr, "ERROR: ApplyCrop: Requested rectangle is out of image boundaries\n");
+        assert(0);
+    }
+
+    rect_x = II_MAX(src->rect.x, 0);
+    rect_y = II_MAX(src->rect.y, 0);
+    rect_width = II_MIN(src->rect.width - (rect_x - src->rect.x), src->width - rect_x);
+    rect_height = II_MIN(src->rect.height - (rect_y - src->rect.y), src->height - rect_y);
+
+    switch (src->format) {
+    case FOURCC_NV12: {
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x;
+        dst.planes[1] = src->planes[1] + (rect_y / 2) * src->stride[1] + rect_x;
+        break;
+    }
+    case FOURCC_I420: {
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x;
+        dst.planes[1] = src->planes[1] + (rect_y / 2) * src->stride[1] + (rect_x / 2);
+        dst.planes[2] = src->planes[2] + (rect_y / 2) * src->stride[2] + (rect_x / 2);
+        break;
+    }
+    case FOURCC_RGBP: {
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x;
+        dst.planes[1] = src->planes[1] + rect_y * src->stride[1] + rect_x;
+        dst.planes[2] = src->planes[2] + rect_y * src->stride[2] + rect_x;
+        break;
+    }
+    case FOURCC_BGR: {
+        int channels = 3;
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x * channels;
+        break;
+    }
+    default: {
+        int channels = 4;
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x * channels;
+        break;
+    }
+    }
+
+    if (rect_width)
+        dst.width = rect_width;
+    if (rect_height)
+        dst.height = rect_height;
+
+    return dst;
+}
+
+static void SubmitImagePreProcess(ImageInferenceContext *ctx, const BatchRequest *request, const Image *pSrc,
+                                  PreProcessor preProcessor) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+
+    if (vino->resize_by_inference) {
+
+        // ie preprocess can only support system memory right now
+        assert(pSrc->type == MEM_TYPE_SYSTEM);
+        if (pSrc->format != FOURCC_NV12) {
+            roi_t roi, *_roi = NULL;
+            ie_blob_t *input_blob = NULL;
+            tensor_desc_t tensor = {NHWC, {4, {1, getNumberChannels(pSrc->format), pSrc->height, pSrc->width}}, U8};
+            if (pSrc->rect.width != 0 && pSrc->rect.height != 0) {
+                RectToIERoi(&roi, &pSrc->rect);
+                _roi = &roi;
+            }
+
+            ie_blob_make_memory_from_preallocated(&tensor, pSrc->planes[0], 0, &input_blob);
+            if (_roi) {
+                ie_blob_t *input_blob_roi = NULL;
+                ie_blob_make_memory_with_roi(input_blob, _roi, &input_blob_roi);
+                ie_infer_request_set_blob(request->infer_request, vino->input_name, input_blob_roi);
+                ie_blob_destroy(&input_blob_roi);
+            } else {
+                ie_infer_request_set_blob(request->infer_request, vino->input_name, input_blob);
+                ie_blob_destroy(&input_blob);
+            }
+        } else {
+            Image src = {};
+            src = ApplyCrop(pSrc);
+
+            ie_blob_t *y_blob = NULL, *uv_blob = NULL, *nv12_blob = NULL;
+            tensor_desc_t y_tensor = {NHWC, {4, {1, 1, src.height - src.height % 2, src.width - src.width % 2}}, U8};
+            tensor_desc_t uv_tensor = {NHWC, {4, {1, 2, src.height / 2, src.width / 2}}, U8};
+            ie_blob_make_memory_from_preallocated(&y_tensor, src.planes[0], 0, &y_blob);
+            ie_blob_make_memory_from_preallocated(&uv_tensor, src.planes[1], 0, &uv_blob);
+            ie_blob_make_memory_nv12(y_blob, uv_blob, &nv12_blob);
+
+            ie_infer_request_set_blob(request->infer_request, vino->input_name, nv12_blob);
+            ie_blob_destroy(&y_blob);
+            ie_blob_destroy(&uv_blob);
+            ie_blob_destroy(&nv12_blob);
+        }
+    } else {
+        Image src = {};
+        Image dst = {};
+
+        dst.type = pSrc->type;
+        GetNextImageBuffer(ctx, request, &dst);
+
+        if (pSrc->planes[0] != dst.planes[0]) { // only convert if different buffers
+            if (!vino->vpp_ctx) {
+                vino->vpp_ctx = pre_proc_alloc(pre_proc_get_by_type(MEM_TYPE_SYSTEM));
+                assert(vino->vpp_ctx);
+            }
+#ifdef HAVE_GAPI
+            vino->vpp_ctx->pre_proc->Convert(vino->vpp_ctx, &src, &dst, 0);
+#else
+            if (pSrc->type == MEM_TYPE_SYSTEM)
+                src = ApplyCrop(pSrc);
+            else
+                src = *pSrc;
+            vino->vpp_ctx->pre_proc->Convert(vino->vpp_ctx, &src, &dst, 0);
+#endif
+            // model specific pre-processing
+            if (preProcessor)
+                preProcessor(&dst);
+        }
+    }
+}
+
+static int OpenVINOImageInferenceCreate(ImageInferenceContext *ctx, MemoryType type, const char *devices,
+                                        const char *model, int batch_size, int nireq, const char *configs,
+                                        void *allocator, CallbackFunc callback) {
+    int cpu_extension_needed = 0, input_num = 0;
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    char *cpu_exetension = NULL, *image_format = NULL;
+    char *pre_processor_name = NULL, *multi_device_list = NULL, *hetero_device_list = NULL;
+    char *_devices = NULL, *weight = NULL;
+    input_shapes_t network_input_shapes = {0};
+    ie_config_t net_config = {NULL, NULL, NULL};
+    VAII_DEBUG("Create");
+
+    if (!model || !devices) {
+        VAII_ERROR("No model or device!");
+        return -1;
+    }
+
+    if (!callback) {
+        VAII_ERROR("Callback function is not assigned!");
+        return -1;
+    }
+
+    ie_core_create("", &vino->core);
+    if (!vino->core) {
+        VAII_ERROR("Create ie core failed!");
+        return -1;
+    }
+
+    if (configs) {
+        ie_config_t *_configs = StringToIEConfig(configs, &pre_processor_name, &multi_device_list,
+                                                &hetero_device_list, &cpu_exetension, &image_format);
+        ie_core_set_config(vino->core, _configs, devices);
+        vino->resize_by_inference = (pre_processor_name && !strcmp(pre_processor_name, "ie")) ? 1 : 0;
+
+        if (multi_device_list && strstr(multi_device_list, "CPU") ||
+            hetero_device_list && strstr(hetero_device_list, "CPU")) {
+            cpu_extension_needed = 1;
+        }
+
+        if (!strcmp(devices, "MULTI")) {
+            if (multi_device_list) {
+                _devices = (char *)malloc(strlen(devices) + strlen(multi_device_list) + 2);
+                if (!_devices) {
+                    VAII_ERROR("Not enough memory!");
+                    ie_config_free(_configs);
+                    goto err;
+                }
+                memset(_devices, 0, sizeof(*_devices));
+                strcpy(_devices, devices);
+                strcat(_devices, ":");
+                strcat(_devices, multi_device_list);
+            }
+        } else if (!strcmp(devices, "HETERO")) {
+            if (hetero_device_list) {
+                _devices = (char *)malloc(strlen(devices) + strlen(hetero_device_list) + 2);
+                if (!_devices) {
+                    VAII_ERROR("Not enough memory!");
+                    ie_config_free(_configs);
+                    goto err;
+                }
+                memset(_devices, 0, sizeof(*_devices));
+                strcpy(_devices, devices);
+                strcat(_devices, ":");
+                strcat(_devices, hetero_device_list);
+            }
+        }
+
+        ie_config_free(_configs);
+
+        if (pre_processor_name)
+            free(pre_processor_name);
+        if (hetero_device_list)
+            free(hetero_device_list);
+        if (multi_device_list)
+            free(multi_device_list);
+        pre_processor_name = NULL, hetero_device_list = NULL, multi_device_list = NULL;
+    }
+
+    // Extension for custom layers
+    if (cpu_extension_needed || strstr(devices, "CPU")) {
+        ie_core_add_extension(vino->core, cpu_exetension, "CPU");
+        VAII_DEBUG("Cpu extension loaded!");
+        if (cpu_exetension)
+            free(cpu_exetension);
+        cpu_exetension = NULL;
+    }
+
+    // Read network
+    weight = (char *)malloc(strlen(model) + 1);
+    assert(weight);
+    strncpy(weight, model, strlen(model) - 4);
+    weight[strlen(model) - 4] = '\0';
+    strcat(weight, ".bin");
+    ie_network_read(model, weight, &vino->network);
+    free(weight);
+    if (!vino->network) {
+        VAII_ERROR("Create network failed!");
+        goto err;
+    }
+
+    vino->batch_size = batch_size;
+
+    // Check model input
+    ie_network_get_inputs_number(vino->network, &input_num);
+    if (!input_num) {
+        VAII_ERROR("Input layer not found!");
+        goto err;
+    }
+
+    ie_network_get_input_shapes(vino->network, &network_input_shapes);
+    if (batch_size > 1 && network_input_shapes.shapes) {
+        for (int i = 0; i < network_input_shapes.shape_num; i++)
+            network_input_shapes.shapes[i].shape.dims[0] = batch_size;
+        ie_network_reshape(vino->network, network_input_shapes);
+    }
+    ie_network_input_shapes_free(&network_input_shapes);
+    network_input_shapes.shape_num = 0;
+
+    ie_network_get_input_name(vino->network, 0, &vino->input_name);
+    if (!vino->input_name) {
+        VAII_ERROR("Get network input name failed!");
+        goto err;
+    }
+
+    ie_network_set_input_precision(vino->network, vino->input_name, U8);
+    ie_network_set_input_layout(vino->network, vino->input_name, NCHW);
+
+    if (image_format) {
+        vino->ie_color_format = FormatNameToIEColorFormat(image_format);
+        ie_network_set_color_format(vino->network, vino->input_name, vino->ie_color_format);
+        free(image_format);
+        image_format = NULL;
+    }
+
+    if (vino->resize_by_inference) {
+        ie_network_set_input_resize_algorithm(vino->network, vino->input_name, RESIZE_BILINEAR);
+    }
+
+    // Load network
+    if (_devices)
+        ie_core_load_network(vino->core, vino->network, _devices, &net_config, &vino->exe_network);
+    else
+        ie_core_load_network(vino->core, vino->network, devices, &net_config, &vino->exe_network);
+    if (!vino->exe_network) {
+        VAII_ERROR("Creat executable network failed!");
+        goto err;
+    }
+    if (_devices)
+        free(_devices);
+
+    // Create infer requests
+    if (nireq == 0) {
+        VAII_ERROR("Input layer not found!");
+        goto err;
+    }
+
+    vino->infer_requests = (ie_infer_request_t **)malloc(nireq * sizeof(ie_infer_request_t *));
+    if (!vino->infer_requests) {
+        VAII_ERROR("Creat infer requests failed!");
+        goto err;
+    }
+    vino->num_reqs = nireq;
+    for (size_t i = 0 ; i < vino->num_reqs; ++i) {
+        ie_exec_network_create_infer_request(vino->exe_network, &vino->infer_requests[i]);
+        if (!vino->infer_requests[i]) {
+            VAII_ERROR("Creat infer requests failed!");
+            goto err;
+        }
+    }
+
+    vino->batch_requests = (BatchRequest **)malloc(nireq * sizeof(*vino->batch_requests));
+    if (!vino->batch_requests) {
+        VAII_ERROR("Creat batch requests failed!");
+        goto err;
+    }
+    vino->num_batch_requests = nireq;
+
+    vino->freeRequests = SafeQueueCreate();
+    if (!vino->freeRequests) {
+        VAII_ERROR("Creat request queues failed!");
+        goto err;
+    }
+
+    for (size_t n = 0; n < vino->num_reqs; ++n) {
+        BatchRequest *batch_request = (BatchRequest *)malloc(sizeof(*batch_request));
+        if (!batch_request)
+            goto err;
+        memset(batch_request, 0, sizeof(*batch_request));
+        batch_request->infer_request = vino->infer_requests[n];
+        vino->batch_requests[n] = batch_request;
+        SafeQueuePush(vino->freeRequests, batch_request);
+    }
+
+    // TODO: handle allocator
+    ie_network_get_name(vino->network, &vino->model_name);
+    if (!vino->model_name) {
+        VAII_ERROR("Copy model name failed!");
+        goto err;
+    }
+
+    vino->callback = callback;
+
+    pthread_mutex_init(&vino->flush_mutex, NULL);
+    pthread_mutex_init(&vino->callback_mutex, NULL);
+    pthread_mutex_init(&vino->count_mutex, NULL);
+    pthread_cond_init(&vino->request_processed, NULL);
+
+    if (cpu_exetension)
+        free(cpu_exetension);
+    return 0;
+err:
+    if (pre_processor_name)
+        free(pre_processor_name);
+    if (hetero_device_list)
+        free(hetero_device_list);
+    if (multi_device_list)
+        free(multi_device_list);
+    if (cpu_exetension)
+        free(cpu_exetension);
+    if (image_format)
+        free(image_format);
+    if (_devices)
+        free(_devices);
+    if (network_input_shapes.shapes)
+        ie_network_input_shapes_free(&network_input_shapes);
+    if (vino->model_name)
+        ie_network_name_free(&vino->model_name);
+    if (vino->infer_requests) {
+        for (size_t i = 0; i < vino->num_reqs; ++i)
+            if (vino->infer_requests[i])
+                ie_infer_request_free(&vino->infer_requests[i]);
+        free(vino->infer_requests);
+        vino->num_reqs = 0;
+    }
+    if (vino->batch_requests) {
+        for (size_t i = 0; i < vino->num_batch_requests; i++)
+            if (vino->batch_requests[i])
+                free(vino->batch_requests[i]);
+        free(vino->batch_requests);
+    }
+    if (vino->freeRequests)
+        SafeQueueDestroy(vino->freeRequests);
+    if (vino->exe_network)
+        ie_exec_network_free (&vino->exe_network);
+    if (vino->network)
+        ie_network_free(&vino->network);
+    if (vino->core)
+        ie_core_free(&vino->core);
+    return -1;
+}
+
+static void OpenVINOImageInferenceSubmtImage(ImageInferenceContext *ctx, const Image *image, IFramePtr user_data,
+                                             PreProcessor pre_processor) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    const Image *pSrc = image;
+    BatchRequest *request = NULL;
+
+    VAII_DEBUG(__FUNCTION__);
+
+    pthread_mutex_lock(&vino->count_mutex);
+    ++vino->requests_processing;
+    pthread_mutex_unlock(&vino->count_mutex);
+
+    request = (BatchRequest *)SafeQueuePop(vino->freeRequests);
+
+    SubmitImagePreProcess(ctx, request, pSrc, pre_processor);
+
+    image_inference_dynarray_add(&request->buffers.frames, &request->buffers.num_buffers, user_data);
+
+    // start inference asynchronously if enough buffers for batching
+    if (request->buffers.num_buffers >= vino->batch_size) {
+#if 1 // TODO: remove when license-plate-recognition-barrier model will take one input
+        size_t num_inputs;
+        ie_network_get_inputs_number(vino->network, &num_inputs);
+        if (num_inputs > 1) {
+            char *input_name = NULL;
+            ie_network_get_input_name(vino->network, 1, &input_name);
+            if (!strcmp(input_name, "seq_ind")) {
+                // 'seq_ind' input layer is some relic from the training
+                // it should have the leading 0.0f and rest 1.0f
+                dimensions_t dims = {};
+                float *blob_data;
+                int maxSequenceSizePerPlate;
+                ie_blob_t *input_blob = NULL;
+                ie_blob_buffer_t blob_buffer;
+                ie_infer_request_get_blob(request->infer_request, input_name, &input_blob);
+                ie_blob_get_dims(input_blob, &dims);
+                maxSequenceSizePerPlate = dims.dims[0];
+                ie_blob_get_buffer(input_blob, &blob_buffer);
+                blob_data = (float *)(blob_buffer.buffer);
+                blob_data[0] = 0.0f;
+                for (int n = 1; n < maxSequenceSizePerPlate; n++)
+                    blob_data[n] = 1.0f;
+                ie_blob_destroy(&input_blob);
+            }
+            ie_network_name_free(&input_name);
+        }
+#endif
+        request->callback.completeCallBackFunc = completion_callback;
+        request->callback.args = request;
+        request->inference_ctx = ctx;
+        ie_infer_set_completion_callback(request->infer_request, &request->callback);
+
+        ie_infer_request_infer_async(request->infer_request);
+    } else {
+        SafeQueuePushFront(vino->freeRequests, request);
+    }
+}
+
+static const char *OpenVINOImageInferenceGetModelName(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    return vino->model_name;
+}
+
+static void OpenVINOImageInferenceGetModelInputInfo(ImageInferenceContext *ctx, int *width, int *height, int *format) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    dimensions_t input_blob_dims = {};
+
+    ie_network_get_input_dims(vino->network, vino->input_name, &input_blob_dims);
+    assert(input_blob_dims.ranks > 2);
+    *width = input_blob_dims.dims[3];  // W
+    *height = input_blob_dims.dims[2]; // H
+    *format = FOURCC_RGBP;
+}
+
+static int OpenVINOImageInferenceIsQueueFull(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    return SafeQueueEmpty(vino->freeRequests);
+}
+
+static int OpenVINOImageInferenceResourceStatus(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    return SafeQueueSize(vino->freeRequests) * vino->batch_size;
+}
+
+static void OpenVINOImageInferenceFlush(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+
+    pthread_mutex_lock(&vino->flush_mutex);
+
+    if (vino->already_flushed) {
+        pthread_mutex_unlock(&vino->flush_mutex);
+        return;
+    }
+
+    vino->already_flushed = 1;
+
+    while (vino->requests_processing != 0) {
+        pthread_cond_wait(&vino->request_processed, &vino->flush_mutex);
+    }
+
+    pthread_mutex_unlock(&vino->flush_mutex);
+}
+
+static void OpenVINOImageInferenceClose(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    if (vino->infer_requests) {
+        for (size_t i = 0; i < vino->num_reqs; ++i)
+            if(vino->infer_requests[i])
+                ie_infer_request_free(&vino->infer_requests[i]);
+        free(vino->infer_requests);
+    }
+    if (vino->batch_requests) {
+        for (size_t i = 0; i < vino->num_batch_requests; i++)
+            if (vino->batch_requests[i])
+                free(vino->batch_requests[i]);
+        free(vino->batch_requests);
+    }
+    if (vino->freeRequests)
+        SafeQueueDestroy(vino->freeRequests);
+
+    if (vino->model_name)
+        ie_network_name_free(&vino->model_name);
+
+    if (vino->input_name)
+        ie_network_name_free(&vino->input_name);
+
+    pthread_mutex_destroy(&vino->flush_mutex);
+    pthread_mutex_destroy(&vino->callback_mutex);
+    pthread_mutex_destroy(&vino->count_mutex);
+    pthread_cond_destroy(&vino->request_processed);
+
+    if (vino->vpp_ctx) {
+        vino->vpp_ctx->pre_proc->Destroy(vino->vpp_ctx);
+        pre_proc_free(vino->vpp_ctx);
+    }
+
+    ie_exec_network_free(&vino->exe_network);
+    ie_network_free(&vino->network);
+    ie_core_free(&vino->core);
+}
+
+static void completion_callback(void *args) {
+    BatchRequest *request = (BatchRequest *)args;
+    ImageInferenceContext *ctx = request->inference_ctx;
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    OutputBlobArray blob_array = {};
+    size_t num_outputs;
+
+    VAII_DEBUG(__FUNCTION__);
+
+    pthread_mutex_lock(&vino->callback_mutex);
+
+    ie_network_get_outputs_number(vino->network, &num_outputs);
+    for (size_t i = 0; i < num_outputs; i++) {
+        char *output_name = NULL;
+        OpenVINOOutputBlob *vino_blob;
+        OutputBlobContext *blob_ctx = output_blob_alloc(ctx->output_blob_method);
+        assert(blob_ctx);
+        vino_blob = (OpenVINOOutputBlob *)blob_ctx->priv;
+        ie_network_get_output_name(vino->network, i, &output_name);
+        vino_blob->name = output_name;
+        ie_infer_request_get_blob(request->infer_request, vino_blob->name, &vino_blob->blob);
+        image_inference_dynarray_add(&blob_array.output_blobs, &blob_array.num_blobs, blob_ctx);
+    }
+
+    vino->callback(&blob_array, &request->buffers);
+
+    for (int n = 0; n < blob_array.num_blobs; n++) {
+        OutputBlobContext *blob_ctx = blob_array.output_blobs[n];
+        OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)blob_ctx->priv;
+        ie_infer_request_set_blob(request->infer_request, vino_blob->name, vino_blob->blob);
+        char *output_name = (char *)vino_blob->name;
+        ie_network_name_free(&output_name);
+        ie_blob_destroy(&vino_blob->blob);
+        output_blob_free(blob_ctx);
+    }
+    blob_array.num_blobs = 0;
+    free(blob_array.output_blobs);
+
+    // clear buffers
+    if (request->buffers.frames) {
+        free(request->buffers.frames);
+        request->buffers.frames = NULL;
+    }
+    request->buffers.num_buffers = 0;
+
+    SafeQueuePush(vino->freeRequests, request);
+
+    pthread_mutex_lock(&vino->count_mutex);
+    vino->requests_processing -= vino->batch_size;
+    pthread_mutex_unlock(&vino->count_mutex);
+
+    pthread_cond_broadcast(&vino->request_processed);
+
+    pthread_mutex_unlock(&vino->callback_mutex);
+
+    VAII_DEBUG("EXIT");
+}
+
+static const char *OpenVINOOutputBlobGetOutputLayerName(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    return vino_blob->name;
+}
+
+static Dimensions OpenVINOOutputBlobGetDims(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    dimensions_t dims = {};
+    Dimensions dims_res = {};
+
+    ie_blob_get_dims(vino_blob->blob, &dims);
+    dims_res.num_dims = dims.ranks;
+    for (size_t i = 0; i< dims_res.num_dims; ++i)
+        dims_res.dims[i] = dims.dims[i];
+    return dims_res;
+}
+
+static IILayout OpenVINOOutputBlobGetLayout(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    layout_e l;
+    ie_blob_get_layout(vino_blob->blob, &l);
+    return (IILayout)l;
+}
+
+static IIPrecision OpenVINOOutputBlobGetPrecision(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    precision_e p;
+    ie_blob_get_precision(vino_blob->blob, &p);
+    return (IIPrecision)p;
+}
+
+static const void *OpenVINOOutputBlobGetData(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    ie_blob_buffer_t blob_data;
+    ie_blob_get_buffer(vino_blob->blob, &blob_data);
+    return blob_data.buffer;
+}
+
+OutputBlobMethod output_blob_method_openvino = {
+    .name = "openvino",
+    .priv_size = sizeof(OpenVINOOutputBlob),
+    .GetOutputLayerName = OpenVINOOutputBlobGetOutputLayerName,
+    .GetDims = OpenVINOOutputBlobGetDims,
+    .GetLayout = OpenVINOOutputBlobGetLayout,
+    .GetPrecision = OpenVINOOutputBlobGetPrecision,
+    .GetData = OpenVINOOutputBlobGetData,
+};
+
+ImageInference image_inference_openvino = {
+    .name = "openvino",
+    .priv_size = sizeof(OpenVINOImageInference),
+    .Create = OpenVINOImageInferenceCreate,
+    .SubmitImage = OpenVINOImageInferenceSubmtImage,
+    .GetModelName = OpenVINOImageInferenceGetModelName,
+    .GetModelInputInfo = OpenVINOImageInferenceGetModelInputInfo,
+    .IsQueueFull = OpenVINOImageInferenceIsQueueFull,
+    .ResourceStatus = OpenVINOImageInferenceResourceStatus,
+    .Flush = OpenVINOImageInferenceFlush,
+    .Close = OpenVINOImageInferenceClose,
+};
diff --git a/libavfilter/inference_backend/openvino_image_inference.h b/libavfilter/inference_backend/openvino_image_inference.h
new file mode 100644
index 0000000..3e06396
--- /dev/null
+++ b/libavfilter/inference_backend/openvino_image_inference.h
@@ -0,0 +1,77 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include <ie_c_api.h>
+#include "image_inference.h"
+#include "pre_proc.h"
+#include "safe_queue.h"
+#include <pthread.h>
+
+typedef struct BatchRequest {
+    ie_infer_request_t *infer_request;
+    UserDataBuffers buffers;
+    // TODO: alloc_context
+
+    ie_complete_call_back_t callback;
+    ImageInferenceContext *inference_ctx;
+} BatchRequest;
+
+typedef struct OpenVINOImageInference {
+    int resize_by_inference;
+    colorformat_e ie_color_format;
+
+    CallbackFunc callback;
+
+    // Inference Engine
+    ie_core_t *core;
+    ie_network_t *network;
+    char *model_name;
+    char *input_name;
+    ie_executable_network_t *exe_network;
+    ie_infer_request_t **infer_requests;
+    size_t num_reqs;
+
+    BatchRequest **batch_requests;
+    size_t num_batch_requests;
+
+    // Threading
+    int batch_size;
+    pthread_t working_thread;
+    SafeQueueT *freeRequests;    // BatchRequest queue
+
+    // VPP
+    PreProcContext *vpp_ctx;
+
+    int already_flushed;
+    unsigned int requests_processing;
+
+    pthread_mutex_t flush_mutex;
+    pthread_mutex_t callback_mutex;
+    pthread_mutex_t count_mutex;
+    pthread_cond_t request_processed;
+
+} OpenVINOImageInference;
+
+typedef struct OpenVINOOutputBlob {
+    const char *name;
+    ie_blob_t *blob;
+} OpenVINOOutputBlob;
diff --git a/libavfilter/inference_backend/pre_proc.c b/libavfilter/inference_backend/pre_proc.c
new file mode 100644
index 0000000..623c111
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc.c
@@ -0,0 +1,239 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "pre_proc.h"
+#include <stdlib.h>
+#include <string.h>
+
+extern PreProc pre_proc_swscale;
+extern PreProc pre_proc_opencv;
+extern PreProc pre_proc_gapi;
+extern PreProc pre_proc_vaapi;
+extern PreProc pre_proc_mocker;
+
+static const PreProc *const pre_proc_list[] = {
+#if HAVE_FFMPEG || CONFIG_SWSCALE
+    &pre_proc_swscale,
+#endif
+#if HAVE_OPENCV
+    &pre_proc_opencv,
+#endif
+#if HAVE_GAPI
+    &pre_proc_gapi,
+#endif
+#if CONFIG_VAAPI
+    &pre_proc_vaapi,
+#endif
+    &pre_proc_mocker,  NULL};
+
+int GetPlanesCount(int fourcc) {
+    switch (fourcc) {
+    case FOURCC_BGRA:
+    case FOURCC_BGRX:
+    case FOURCC_BGR:
+    case FOURCC_RGBA:
+    case FOURCC_RGBX:
+        return 1;
+    case FOURCC_NV12:
+        return 2;
+    case FOURCC_BGRP:
+    case FOURCC_RGBP:
+    case FOURCC_I420:
+        return 3;
+    }
+
+    return 0;
+}
+
+static const PreProc *pre_proc_iterate(void **opaque) {
+    uintptr_t i = (uintptr_t)*opaque;
+    const PreProc *pp = pre_proc_list[i];
+
+    if (pp != NULL)
+        *opaque = (void *)(i + 1);
+
+    return pp;
+}
+
+const PreProc *pre_proc_get_by_name(const char *name) {
+    const PreProc *pp = NULL;
+    void *opaque = 0;
+
+    if (name == NULL)
+        return NULL;
+
+    while ((pp = pre_proc_iterate(&opaque)))
+        if (!strcmp(pp->name, name))
+            return pp;
+
+    return NULL;
+}
+
+const PreProc *pre_proc_get_by_type(MemoryType type) {
+    const PreProc *ret = NULL;
+
+    if (type == MEM_TYPE_SYSTEM) {
+        ret = pre_proc_get_by_name("swscale");
+        if (!ret)
+            ret = pre_proc_get_by_name("gapi");
+        if (!ret)
+            ret = pre_proc_get_by_name("opencv");
+    } else if (type == MEM_TYPE_VAAPI) {
+        ret = pre_proc_get_by_name("vaapi");
+    }
+
+    return ret;
+}
+
+PreProcContext *pre_proc_alloc(const PreProc *pre_proc) {
+    PreProcContext *ret;
+
+    if (pre_proc == NULL)
+        return NULL;
+
+    ret = (PreProcContext *)malloc(sizeof(*ret));
+    if (!ret)
+        return NULL;
+    memset(ret, 0, sizeof(*ret));
+
+    ret->pre_proc = pre_proc;
+    if (pre_proc->priv_size > 0) {
+        ret->priv = malloc(pre_proc->priv_size);
+        if (!ret->priv)
+            goto err;
+        memset(ret->priv, 0, pre_proc->priv_size);
+    }
+
+    return ret;
+err:
+    free(ret);
+    return NULL;
+}
+
+void pre_proc_free(PreProcContext *context) {
+    if (context == NULL)
+        return;
+
+    if (context->priv)
+        free(context->priv);
+    free(context);
+}
+
+#ifdef DEBUG
+#include "logger.h"
+#include <assert.h>
+#include <stdio.h>
+void DumpBGRpToRgb24File(const Image *out_image) {
+    FILE *fp;
+    char file_name[256] = {};
+    static int dump_frame_num = 0;
+
+    sprintf(file_name, "ff_bgrp_to_rgb24_%03d.rgb", dump_frame_num++);
+    fp = fopen(file_name, "w+b");
+    assert(fp);
+
+    const uint8_t *b_channel = out_image->planes[0];
+    const uint8_t *g_channel = out_image->planes[1];
+    const uint8_t *r_channel = out_image->planes[2];
+
+    int size = out_image->height * out_image->width * 3;
+    uint8_t *data = (uint8_t *)malloc(size);
+    memset(data, 0, size);
+
+    for (int i = 0; i < out_image->height; i++) {
+        for (int j = 0; j < out_image->width; j++) {
+            data[3 * j + i * 3 * out_image->width] = r_channel[j + i * out_image->stride[2]];
+            data[3 * j + i * 3 * out_image->width + 1] = g_channel[j + i * out_image->stride[1]];
+            data[3 * j + i * 3 * out_image->width + 2] = b_channel[j + i * out_image->stride[0]];
+        }
+    }
+    fwrite(data, out_image->height * out_image->width * 3, 1, fp);
+    free(data);
+    fclose(fp);
+}
+
+void DumpRGBpToRgb24File(const Image *out_image) {
+    FILE *fp;
+    char file_name[256] = {};
+    static int dump_frame_num = 0;
+
+    sprintf(file_name, "ff_rgbp_to_rgb24_%03d.rgb", dump_frame_num++);
+    fp = fopen(file_name, "w+b");
+    assert(fp);
+
+    const uint8_t *b_channel = out_image->planes[2];
+    const uint8_t *g_channel = out_image->planes[1];
+    const uint8_t *r_channel = out_image->planes[0];
+
+    int size = out_image->height * out_image->width * 3;
+    uint8_t *data = (uint8_t *)malloc(size);
+    memset(data, 0, size);
+
+    for (int i = 0; i < out_image->height; i++) {
+        for (int j = 0; j < out_image->width; j++) {
+            data[3 * j + i * 3 * out_image->width] = r_channel[j + i * out_image->stride[0]];
+            data[3 * j + i * 3 * out_image->width + 1] = g_channel[j + i * out_image->stride[1]];
+            data[3 * j + i * 3 * out_image->width + 2] = b_channel[j + i * out_image->stride[2]];
+        }
+    }
+    fwrite(data, out_image->height * out_image->width * 3, 1, fp);
+    free(data);
+    fclose(fp);
+}
+
+void DumpRGBpToFile(const Image *out_image) {
+    FILE *fp;
+    char file_name[256] = {};
+    static int dump_frame_num = 0;
+
+    sprintf(file_name, "ff_rgbp_orig_%03d.rgb", dump_frame_num++);
+    fp = fopen(file_name, "w+b");
+    assert(fp);
+
+    const uint8_t *r_channel = out_image->planes[0];
+    const uint8_t *g_channel = out_image->planes[1];
+    const uint8_t *b_channel = out_image->planes[2];
+
+    fwrite(r_channel, out_image->height * out_image->stride[0], 1, fp);
+    fwrite(g_channel, out_image->height * out_image->stride[1], 1, fp);
+    fwrite(b_channel, out_image->height * out_image->stride[2], 1, fp);
+
+    fclose(fp);
+}
+
+void DumpBGRAToFile(const Image *out_image) {
+    FILE *fp;
+    char file_name[256] = {};
+    static int dump_frame_num = 0;
+
+    sprintf(file_name, "ff_bgra_orig_%03d.rgb", dump_frame_num++);
+    fp = fopen(file_name, "w+b");
+    assert(fp);
+
+    fwrite(out_image->planes[0], out_image->height * out_image->stride[0], 1, fp);
+
+    fclose(fp);
+}
+
+void DumpImageInfo(const Image *p) {
+    VAII_LOGI("Image w:%d h:%d f:%x, plane: %p %p %p  stride: %d %d %d \n", p->width, p->height, p->format,
+              p->planes[0], p->planes[1], p->planes[2], p->stride[0], p->stride[1], p->stride[2]);
+}
+#endif
\ No newline at end of file
diff --git a/libavfilter/inference_backend/pre_proc.h b/libavfilter/inference_backend/pre_proc.h
new file mode 100644
index 0000000..805767b
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc.h
@@ -0,0 +1,81 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "config.h"
+#include "image.h"
+// #define DEBUG
+
+typedef struct PreProcContext PreProcContext;
+
+typedef struct PreProcInitParam {
+    union {
+        struct { // VAAPI
+            void *va_display;
+            int num_surfaces;
+            int width;
+            int height;
+            int format; // FourCC
+        };
+        void *reserved; // Others
+    };
+} PreProcInitParam;
+
+typedef struct PreProc {
+    /* image pre processing module name. Must be non-NULL and unique among pre processing modules. */
+    const char *name;
+
+    int (*Init)(PreProcContext *context, PreProcInitParam *param);
+
+    void (*Destroy)(PreProcContext *context);
+
+    void (*Convert)(PreProcContext *context, const Image *src, Image *dst, int bAllocateDestination);
+
+    // to be called if Convert called with bAllocateDestination = true
+    void (*ReleaseImage)(PreProcContext *context, Image *dst);
+
+    MemoryType mem_type;
+
+    int priv_size; ///< size of private data to allocate for pre processing
+} PreProc;
+
+struct PreProcContext {
+    const PreProc *pre_proc;
+    void *priv;
+};
+
+int GetPlanesCount(int fourcc);
+
+const PreProc *pre_proc_get_by_name(const char *name);
+
+const PreProc *pre_proc_get_by_type(MemoryType type);
+
+PreProcContext *pre_proc_alloc(const PreProc *pre_proc);
+
+void pre_proc_free(PreProcContext *context);
+
+#ifdef DEBUG
+void DumpBGRpToRgb24File(const Image *out_image);
+void DumpRGBpToRgb24File(const Image *out_image);
+void DumpRGBpToFile(const Image *out_image);
+void DumpBGRAToFile(const Image *out_image);
+inline void DumpImageInfo(const Image *p);
+#endif
\ No newline at end of file
diff --git a/libavfilter/inference_backend/pre_proc_mocker.c b/libavfilter/inference_backend/pre_proc_mocker.c
new file mode 100644
index 0000000..c6686e2
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc_mocker.c
@@ -0,0 +1,61 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "pre_proc.h"
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+static void MockerPreProcConvert(PreProcContext *context, const Image *src, Image *dst, int bAllocateDestination) {
+    assert(src->type == MEM_TYPE_SYSTEM);
+    memcpy(dst, src, sizeof(*src));
+}
+
+static void MockerPreProcDestroy(PreProcContext *context) {
+    // empty
+}
+
+static void MockerPreProcReleaseImage(PreProcContext *context, Image *image) {
+    // empty
+}
+
+static Image MockerMap(ImageMapContext *context, const Image *image) {
+    assert(image);
+    return *image;
+}
+
+static void MockerUnmap(ImageMapContext *context) {
+    // empty
+}
+
+ImageMap image_map_mocker = {
+    .name = "mocker",
+    .Map = MockerMap,
+    .Unmap = MockerUnmap,
+};
+
+PreProc pre_proc_mocker = {
+    .name = "mocker",
+    .mem_type = MEM_TYPE_SYSTEM,
+    .Convert = MockerPreProcConvert,
+    .ReleaseImage = MockerPreProcReleaseImage,
+    .Destroy = MockerPreProcDestroy,
+};
diff --git a/libavfilter/inference_backend/pre_proc_swscale.c b/libavfilter/inference_backend/pre_proc_swscale.c
new file mode 100644
index 0000000..a891630
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc_swscale.c
@@ -0,0 +1,201 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "logger.h"
+#include "pre_proc.h"
+#include <assert.h>
+#include <libavutil/imgutils.h>
+#include <libswscale/swscale.h>
+
+#if CONFIG_SWSCALE || HAVE_FFMPEG
+
+static inline enum AVPixelFormat FOURCC2FFmpegFormat(int format) {
+    switch (format) {
+    case FOURCC_NV12:
+        return AV_PIX_FMT_NV12;
+    case FOURCC_BGRA:
+        return AV_PIX_FMT_BGRA;
+    case FOURCC_BGRX:
+        return AV_PIX_FMT_BGRA;
+    case FOURCC_BGR:
+        return AV_PIX_FMT_BGR24;
+    case FOURCC_RGBP:
+        return AV_PIX_FMT_RGBP;
+    case FOURCC_I420:
+        return AV_PIX_FMT_YUV420P;
+    }
+    return AV_PIX_FMT_NONE;
+}
+
+typedef struct FFPreProc {
+    struct SwsContext *sws_context[3];
+    Image image_yuv;
+    Image image_bgr;
+} FFPreProc;
+
+static void FFPreProcConvert(PreProcContext *context, const Image *src, Image *dst, int bAllocateDestination) {
+    FFPreProc *ff_pre_proc = (FFPreProc *)context->priv;
+    struct SwsContext **sws_context = ff_pre_proc->sws_context;
+    Image *image_yuv = &ff_pre_proc->image_yuv;
+    Image *image_bgr = &ff_pre_proc->image_bgr;
+    uint8_t *gbr_planes[4] = {};
+
+    // if identical format and resolution
+    if (src->format == dst->format && src->width == dst->width && src->height == dst->height) {
+        int planes_count = GetPlanesCount(src->format);
+
+        if (src->format == FOURCC_RGBP) {
+            // RGB->BGR
+            Image src_bgr = *src;
+            src_bgr.planes[0] = src->planes[2];
+            src_bgr.planes[2] = src->planes[0];
+            for (int i = 0; i < planes_count; i++) {
+                if (src_bgr.width == src_bgr.stride[i]) {
+                    memcpy(dst->planes[i], src_bgr.planes[i], src_bgr.width * src_bgr.height * sizeof(uint8_t));
+                } else {
+                    int dst_stride = dst->stride[i] * sizeof(uint8_t);
+                    int src_stride = src_bgr.stride[i] * sizeof(uint8_t);
+                    for (int r = 0; r < src_bgr.height; r++) {
+                        memcpy(dst->planes[i] + r * dst_stride, src_bgr.planes[i] + r * src_stride, dst->width);
+                    }
+                }
+            }
+
+            return;
+        }
+
+        if (src->format == FOURCC_BGRA) {
+            int src_stride = src->stride[0] * sizeof(uint8_t);
+            const uint8_t *data = src->planes[0];
+            for (int i = 0; i < dst->height; i++) {
+                for (int j = 0; j < dst->width; j++) {
+                    *(dst->planes[0] + j + i * dst->stride[0]) = *(data + 0 + 4 * j + src_stride * i);
+                    *(dst->planes[1] + j + i * dst->stride[1]) = *(data + 1 + 4 * j + src_stride * i);
+                    *(dst->planes[2] + j + i * dst->stride[2]) = *(data + 2 + 4 * j + src_stride * i);
+                }
+            }
+
+            return;
+        }
+    }
+#define PLANE_NUM 3
+    // init image YUV
+    if (image_yuv->width != dst->width || image_yuv->height != dst->height) {
+        int ret = 0;
+        image_yuv->width = dst->width;
+        image_yuv->height = dst->height;
+        image_yuv->format = src->format; // no CSC for the 1st stage
+
+        if (image_yuv->planes[0])
+            av_freep(&image_yuv->planes[0]);
+        ret = av_image_alloc(image_yuv->planes, image_yuv->stride, image_yuv->width, image_yuv->height,
+                             FOURCC2FFmpegFormat(image_yuv->format), 16);
+        if (ret < 0) {
+            fprintf(stderr, "Alloc yuv image buffer error!\n");
+            assert(0);
+        }
+    }
+
+    // init image BGR24
+    if (image_bgr->width != image_yuv->width || image_bgr->height != image_yuv->height) {
+        int ret = 0;
+        image_bgr->width = image_yuv->width;
+        image_bgr->height = image_yuv->height;
+        image_bgr->format = FOURCC_BGR; // YUV -> BGR packed for the 2nd stage
+
+        if (image_bgr->planes[0])
+            av_freep(&image_bgr->planes[0]);
+        ret = av_image_alloc(image_bgr->planes, image_bgr->stride, image_bgr->width, image_bgr->height,
+                             FOURCC2FFmpegFormat(image_bgr->format), 16);
+        if (ret < 0) {
+            fprintf(stderr, "Alloc bgr image buffer error!\n");
+            assert(0);
+        }
+    }
+
+    sws_context[0] = sws_getCachedContext(sws_context[0], src->width, src->height, FOURCC2FFmpegFormat(src->format),
+                                          image_yuv->width, image_yuv->height, FOURCC2FFmpegFormat(image_yuv->format),
+                                          SWS_FAST_BILINEAR, NULL, NULL, NULL);
+    sws_context[1] = sws_getCachedContext(sws_context[1], image_yuv->width, image_yuv->height,
+                                          FOURCC2FFmpegFormat(image_yuv->format), image_bgr->width, image_bgr->height,
+                                          FOURCC2FFmpegFormat(image_bgr->format), SWS_FAST_BILINEAR, NULL, NULL, NULL);
+    sws_context[2] = sws_getCachedContext(sws_context[2], image_bgr->width, image_bgr->height,
+                                          FOURCC2FFmpegFormat(image_bgr->format), dst->width, dst->height,
+                                          AV_PIX_FMT_GBRP, SWS_FAST_BILINEAR, NULL, NULL, NULL);
+
+    for (int i = 0; i < 3; i++)
+        assert(sws_context[i]);
+
+    // BGR->GBR
+    gbr_planes[0] = dst->planes[1];
+    gbr_planes[1] = dst->planes[0];
+    gbr_planes[2] = dst->planes[2];
+
+    // stage 1: yuv -> yuv, resize to dst size
+    if (!sws_scale(sws_context[0], (const uint8_t *const *)src->planes, src->stride, 0, src->height, image_yuv->planes,
+                   image_yuv->stride)) {
+        fprintf(stderr, "Error on FFMPEG sws_scale stage 1\n");
+        assert(0);
+    }
+    // stage 2: yuv -> bgr packed, no resize
+    if (!sws_scale(sws_context[1], (const uint8_t *const *)image_yuv->planes, image_yuv->stride, 0, image_yuv->height,
+                   image_bgr->planes, image_bgr->stride)) {
+        fprintf(stderr, "Error on FFMPEG sws_scale stage 2\n");
+        assert(0);
+    }
+    // stage 3: bgr -> gbr planer, no resize
+    if (!sws_scale(sws_context[2], (const uint8_t *const *)image_bgr->planes, image_bgr->stride, 0, image_bgr->height,
+                   gbr_planes, dst->stride)) {
+        fprintf(stderr, "Error on FFMPEG sws_scale stage 3\n");
+        assert(0);
+    }
+
+    /* dump pre-processed image to file */
+    // DumpBGRpToFile(dst);
+}
+
+static void FFPreProcDestroy(PreProcContext *context) {
+    FFPreProc *ff_pre_proc = (FFPreProc *)context->priv;
+
+    for (int i = 0; i < 3; i++) {
+        if (ff_pre_proc->sws_context[i]) {
+            sws_freeContext(ff_pre_proc->sws_context[i]);
+            ff_pre_proc->sws_context[i] = NULL;
+        }
+    }
+    if (ff_pre_proc->image_yuv.planes[0]) {
+        av_freep(&ff_pre_proc->image_yuv.planes[0]);
+        ff_pre_proc->image_yuv.planes[0] = NULL;
+    }
+    if (ff_pre_proc->image_bgr.planes[0]) {
+        av_freep(&ff_pre_proc->image_bgr.planes[0]);
+        ff_pre_proc->image_bgr.planes[0] = NULL;
+    }
+}
+
+PreProc pre_proc_swscale = {
+    .name = "swscale",
+    .priv_size = sizeof(FFPreProc),
+    .mem_type = MEM_TYPE_SYSTEM,
+    .Convert = FFPreProcConvert,
+    .Destroy = FFPreProcDestroy,
+};
+
+#endif
diff --git a/libavfilter/inference_backend/pre_proc_vaapi.c b/libavfilter/inference_backend/pre_proc_vaapi.c
new file mode 100644
index 0000000..222ecef
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc_vaapi.c
@@ -0,0 +1,279 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "pre_proc.h"
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#if CONFIG_VAAPI
+#include <va/va.h>
+#include <va/va_vpp.h>
+#endif
+
+#if CONFIG_VAAPI
+
+#define VA_CALL(_FUNC)                                                                                                 \
+    do {                                                                                                               \
+        VAStatus _status = _FUNC;                                                                                      \
+        if (_status != VA_STATUS_SUCCESS) {                                                                            \
+            printf(#_FUNC " failed, sts = %d (%s).\n", _status, vaErrorStr(_status));                                  \
+            assert(0);                                                                                                 \
+        }                                                                                                              \
+    } while (0)
+
+typedef struct _VAAPIPreProc {
+    VADisplay display;
+    VAConfigID va_config;
+    VAContextID va_context;
+    VAImageFormat *format_list; //!< Surface formats which can be used with this device.
+    int nb_formats;
+    VAImage va_image;
+    VAImageFormat va_format_selected;
+} VAAPIPreProc;
+
+static uint32_t Fourcc2RTFormat(int format_fourcc) {
+    switch (format_fourcc) {
+#if VA_MAJOR_VERSION >= 1
+    case VA_FOURCC_I420:
+        return VA_FOURCC_I420;
+#endif
+    case VA_FOURCC_NV12:
+        return VA_RT_FORMAT_YUV420;
+    case VA_FOURCC_RGBP:
+        return VA_RT_FORMAT_RGBP;
+    default:
+        return VA_RT_FORMAT_RGB32;
+    }
+}
+
+static VASurfaceID CreateVASurface(VADisplay va_display, const Image *src) {
+    unsigned int rtformat = Fourcc2RTFormat(src->format);
+    VASurfaceID va_surface_id;
+    VASurfaceAttrib surface_attrib;
+    surface_attrib.type = VASurfaceAttribPixelFormat;
+    surface_attrib.flags = VA_SURFACE_ATTRIB_SETTABLE;
+    surface_attrib.value.type = VAGenericValueTypeInteger;
+    surface_attrib.value.value.i = src->format;
+
+    VA_CALL(vaCreateSurfaces(va_display, rtformat, src->width, src->height, &va_surface_id, 1, &surface_attrib, 1));
+    return va_surface_id;
+}
+
+static int VAAPIPreProcInit(PreProcContext *context, PreProcInitParam *param) {
+    VAAPIPreProc *vaapi_pre_proc = (VAAPIPreProc *)context->priv;
+    VADisplay va_display = (VADisplay)param->va_display;
+    VAConfigID va_config = VA_INVALID_ID;
+    VAContextID va_context = VA_INVALID_ID;
+    VAImageFormat *image_list = NULL;
+    int image_count;
+
+    image_count = vaMaxNumImageFormats(va_display);
+    if (image_count <= 0) {
+        return -1;
+    }
+    image_list = malloc(image_count * sizeof(*image_list));
+    if (!image_list) {
+        return -1;
+    }
+
+    VA_CALL(vaQueryImageFormats(va_display, image_list, &image_count));
+    VA_CALL(vaCreateConfig(va_display, VAProfileNone, VAEntrypointVideoProc, NULL, 0, &va_config));
+    VA_CALL(vaCreateContext(va_display, va_config, 0, 0, VA_PROGRESSIVE, 0, 0, &va_context));
+
+    vaapi_pre_proc->display = va_display;
+    vaapi_pre_proc->format_list = image_list;
+    vaapi_pre_proc->nb_formats = image_count;
+    vaapi_pre_proc->va_config = va_config;
+    vaapi_pre_proc->va_context = va_context;
+
+    for (int i = 0; i < vaapi_pre_proc->nb_formats; i++) {
+        if (vaapi_pre_proc->format_list[i].fourcc == VA_FOURCC_RGBP) {
+            vaapi_pre_proc->va_format_selected = vaapi_pre_proc->format_list[i];
+            break;
+        }
+    }
+    return VA_STATUS_SUCCESS;
+}
+
+static void VAAPIPreProcConvert(PreProcContext *context, const Image *src, Image *dst, int bAllocateDestination) {
+    VAAPIPreProc *vaapi_pre_proc = (VAAPIPreProc *)context->priv;
+    VAProcPipelineParameterBuffer pipeline_param = {};
+    VARectangle surface_region = {};
+    VABufferID pipeline_param_buf_id = VA_INVALID_ID;
+
+    VADisplay va_display = vaapi_pre_proc->display;
+    VAContextID va_context = vaapi_pre_proc->va_context;
+    VASurfaceID src_surface = (VASurfaceID)src->surface_id;
+
+    if (dst->type == MEM_TYPE_ANY) {
+        dst->surface_id = CreateVASurface(vaapi_pre_proc->display, dst);
+        dst->va_display = va_display;
+        dst->type = MEM_TYPE_VAAPI;
+    }
+
+    pipeline_param.surface = src_surface;
+    surface_region = (VARectangle){.x = (int16_t)src->rect.x,
+                                   .y = (int16_t)src->rect.y,
+                                   .width = (uint16_t)src->rect.width,
+                                   .height = (uint16_t)src->rect.height};
+    if (surface_region.width > 0 && surface_region.height > 0)
+        pipeline_param.surface_region = &surface_region;
+
+    // pipeline_param.filter_flags = VA_FILTER_SCALING_HQ; // High-quality scaling method
+    pipeline_param.filter_flags = VA_FILTER_SCALING_DEFAULT;
+
+    VA_CALL(vaCreateBuffer(va_display, va_context, VAProcPipelineParameterBufferType, sizeof(pipeline_param), 1,
+                           &pipeline_param, &pipeline_param_buf_id));
+
+    VA_CALL(vaBeginPicture(va_display, va_context, (VASurfaceID)dst->surface_id));
+
+    VA_CALL(vaRenderPicture(va_display, va_context, &pipeline_param_buf_id, 1));
+
+    VA_CALL(vaEndPicture(va_display, va_context));
+
+    VA_CALL(vaDestroyBuffer(va_display, pipeline_param_buf_id));
+}
+
+static void VAAPIPreProcReleaseImage(PreProcContext *context, Image *image) {
+    VAAPIPreProc *vaapi_pre_proc = (VAAPIPreProc *)context->priv;
+
+    if (!vaapi_pre_proc)
+        return;
+
+    if (image->type == MEM_TYPE_VAAPI && image->surface_id && image->surface_id != VA_INVALID_ID) {
+        VA_CALL(vaDestroySurfaces(vaapi_pre_proc->display, (uint32_t *)&image->surface_id, 1));
+        image->type = MEM_TYPE_ANY;
+    }
+}
+
+static void VAAPIPreProcDestroy(PreProcContext *context) {
+    VAAPIPreProc *vaapi_pre_proc = (VAAPIPreProc *)context->priv;
+
+    if (!vaapi_pre_proc)
+        return;
+
+    if (vaapi_pre_proc->va_context != VA_INVALID_ID) {
+        vaDestroyContext(vaapi_pre_proc->display, vaapi_pre_proc->va_context);
+        vaapi_pre_proc->va_context = VA_INVALID_ID;
+    }
+
+    if (vaapi_pre_proc->va_config != VA_INVALID_ID) {
+        vaDestroyConfig(vaapi_pre_proc->display, vaapi_pre_proc->va_config);
+        vaapi_pre_proc->va_config = VA_INVALID_ID;
+    }
+
+    if (vaapi_pre_proc->format_list) {
+        free(vaapi_pre_proc->format_list);
+        vaapi_pre_proc->format_list = NULL;
+    }
+}
+
+typedef struct VAAPIImageMap {
+    VADisplay va_display;
+    VAImage va_image;
+} VAAPIImageMap;
+
+static Image VAAPIMap(ImageMapContext *context, const Image *image) {
+    VAAPIImageMap *m = (VAAPIImageMap *)context->priv;
+    VADisplay va_display = image->va_display;
+    VAImage va_image = {};
+    VAImageFormat va_format = {};
+    void *surface_p = NULL;
+    Image image_sys = {};
+
+    assert(image->type == MEM_TYPE_VAAPI);
+
+    if (image->format == VA_FOURCC_RGBP) {
+        va_format = (VAImageFormat){.fourcc = (uint32_t)image->format,
+                                    .byte_order = VA_LSB_FIRST,
+                                    .bits_per_pixel = 24,
+                                    .depth = 24,
+                                    .red_mask = 0xff0000,
+                                    .green_mask = 0xff00,
+                                    .blue_mask = 0xff,
+                                    .alpha_mask = 0,
+                                    .va_reserved = {}};
+    } else if (image->format == VA_FOURCC_BGRA) {
+        va_format = (VAImageFormat){.fourcc = (uint32_t)image->format,
+                                    .byte_order = VA_LSB_FIRST,
+                                    .bits_per_pixel = 32,
+                                    .depth = 32,
+                                    .red_mask = 0xff0000,
+                                    .green_mask = 0xff00,
+                                    .blue_mask = 0xff,
+                                    .alpha_mask = 0xff000000,
+                                    .va_reserved = {}};
+    }
+
+    VA_CALL(vaSyncSurface(va_display, image->surface_id));
+
+    if (va_format.fourcc &&
+        vaCreateImage(va_display, &va_format, image->width, image->height, &va_image) == VA_STATUS_SUCCESS) {
+        VA_CALL(vaGetImage(va_display, image->surface_id, 0, 0, image->width, image->height, va_image.image_id));
+    } else {
+        VA_CALL(vaDeriveImage(va_display, image->surface_id, &va_image));
+    }
+
+    VA_CALL(vaMapBuffer(va_display, va_image.buf, &surface_p));
+
+    image_sys.type = MEM_TYPE_SYSTEM;
+    image_sys.width = image->width;
+    image_sys.height = image->height;
+    image_sys.format = image->format;
+    for (uint32_t i = 0; i < va_image.num_planes; i++) {
+        image_sys.planes[i] = (uint8_t *)surface_p + va_image.offsets[i];
+        image_sys.stride[i] = va_image.pitches[i];
+    }
+
+    m->va_display = va_display;
+    m->va_image = va_image;
+
+    //  DumpRGBpToFile(&image_sys);
+    return image_sys;
+}
+
+static void VAAPIUnmap(ImageMapContext *context) {
+    VAAPIImageMap *m = (VAAPIImageMap *)context->priv;
+    if (m->va_display) {
+        VA_CALL(vaUnmapBuffer(m->va_display, m->va_image.buf));
+        VA_CALL(vaDestroyImage(m->va_display, m->va_image.image_id));
+    }
+}
+
+ImageMap image_map_vaapi = {
+    .name = "vaapi",
+    .priv_size = sizeof(VAAPIImageMap),
+    .Map = VAAPIMap,
+    .Unmap = VAAPIUnmap,
+};
+
+PreProc pre_proc_vaapi = {
+    .name = "vaapi",
+    .priv_size = sizeof(VAAPIPreProc),
+    .mem_type = MEM_TYPE_VAAPI,
+    .Init = VAAPIPreProcInit,
+    .Convert = VAAPIPreProcConvert,
+    .ReleaseImage = VAAPIPreProcReleaseImage,
+    .Destroy = VAAPIPreProcDestroy,
+};
+
+#endif // #if CONFIG_VAAPI
\ No newline at end of file
diff --git a/libavfilter/inference_backend/queue.c b/libavfilter/inference_backend/queue.c
new file mode 100644
index 0000000..263e76b
--- /dev/null
+++ b/libavfilter/inference_backend/queue.c
@@ -0,0 +1,171 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+typedef struct _queue_entry queue_entry_t;
+typedef struct _queue queue_t;
+
+struct _queue_entry {
+    void *value;
+    queue_entry_t *prev;
+    queue_entry_t *next;
+    queue_t *queue;
+};
+
+struct _queue {
+    queue_entry_t *head;
+    queue_entry_t *tail;
+    size_t length;
+};
+
+static inline queue_entry_t *create_entry(queue_t *q) {
+    queue_entry_t *new_entry = (queue_entry_t *)calloc(1, sizeof(queue_entry_t));
+
+    assert(new_entry != NULL);
+
+    new_entry->queue = q;
+    return new_entry;
+}
+
+static queue_entry_t *queue_iterate(queue_t *q) {
+    queue_entry_t *it = q->head->next;
+    return it == q->tail ? NULL : it;
+}
+
+static queue_entry_t *queue_iterate_next(queue_t *q, queue_entry_t *it) {
+    queue_entry_t *next = it->next;
+    return next == q->tail ? NULL : next;
+}
+
+static void *queue_iterate_value(queue_entry_t *it) {
+    return it->value;
+}
+
+static queue_t *queue_create(void) {
+    queue_t *q = (queue_t *)malloc(sizeof(queue_t));
+    if (!q)
+        return NULL;
+
+    memset(q, 0, sizeof(queue_t));
+    q->head = create_entry(q);
+    q->tail = create_entry(q);
+    q->head->next = q->tail;
+    q->tail->prev = q->head;
+    q->head->prev = NULL;
+    q->tail->next = NULL;
+
+    return q;
+}
+
+static void queue_destroy(queue_t *q) {
+    queue_entry_t *entry;
+    if (!q)
+        return;
+
+    entry = q->head;
+    while (entry != NULL) {
+        queue_entry_t *temp = entry;
+        entry = entry->next;
+        free(temp);
+    }
+
+    q->head = NULL;
+    q->tail = NULL;
+    q->length = 0;
+    free(q);
+}
+
+static size_t queue_count(queue_t *q) {
+    return q ? q->length : 0;
+}
+
+static void queue_push_front(queue_t *q, void *val) {
+    queue_entry_t *new_node = create_entry(q);
+    queue_entry_t *original_next = q->head->next;
+
+    new_node->value = val;
+
+    q->head->next = new_node;
+    original_next->prev = new_node;
+    new_node->prev = q->head;
+    new_node->next = original_next;
+    q->length++;
+}
+
+static void queue_push_back(queue_t *q, void *val) {
+    queue_entry_t *new_node = create_entry(q);
+    queue_entry_t *original_prev = q->tail->prev;
+
+    new_node->value = val;
+
+    q->tail->prev = new_node;
+    original_prev->next = new_node;
+    new_node->next = q->tail;
+    new_node->prev = original_prev;
+    q->length++;
+}
+
+static void *queue_pop_front(queue_t *q) {
+    queue_entry_t *front = q->head->next;
+    queue_entry_t *new_head_next = front->next;
+    void *ret = front->value;
+
+    if (q->length == 0)
+        return NULL;
+
+    q->head->next = new_head_next;
+    new_head_next->prev = q->head;
+    free(front);
+    q->length--;
+    return ret;
+}
+
+static void *queue_pop_back(queue_t *q) {
+    queue_entry_t *back = q->tail->prev;
+    queue_entry_t *new_tail_prev = back->prev;
+    void *ret = back->value;
+
+    if (q->length == 0)
+        return NULL;
+
+    q->tail->prev = new_tail_prev;
+    new_tail_prev->next = q->tail;
+    free(back);
+    q->length--;
+    return ret;
+}
+
+static void *queue_peek_front(queue_t *q) {
+    if (!q || q->length == 0)
+        return NULL;
+
+    return q->head->next->value;
+}
+
+static void *queue_peek_back(queue_t *q) {
+    if (!q || q->length == 0)
+        return NULL;
+
+    return q->tail->prev->value;
+}
diff --git a/libavfilter/inference_backend/safe_queue.c b/libavfilter/inference_backend/safe_queue.c
new file mode 100644
index 0000000..cd6101d
--- /dev/null
+++ b/libavfilter/inference_backend/safe_queue.c
@@ -0,0 +1,167 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "safe_queue.h"
+#include <assert.h>
+#include <pthread.h>
+#include <stdlib.h>
+
+#include "queue.c"
+
+#define mutex_t pthread_mutex_t
+#define cond_t pthread_cond_t
+
+#define mutex_init(m) pthread_mutex_init((m), NULL)
+
+#define mutex_lock pthread_mutex_lock
+#define mutex_unlock pthread_mutex_unlock
+#define mutex_destroy pthread_mutex_destroy
+
+#define cond_init(c) pthread_cond_init((c), NULL)
+#define cond_signal pthread_cond_signal
+#define cond_broadcast pthread_cond_broadcast
+#define cond_wait pthread_cond_wait
+#define cond_destroy pthread_cond_destroy
+
+struct _SafeQueue {
+    queue_t *q;
+
+    mutex_t mutex;
+    cond_t cond;
+};
+
+SafeQueueT *SafeQueueCreate() {
+    SafeQueueT *sq = (SafeQueueT *)malloc(sizeof(SafeQueueT));
+    if (!sq)
+        return NULL;
+
+    sq->q = queue_create();
+    assert(sq->q);
+
+    mutex_init(&sq->mutex);
+    cond_init(&sq->cond);
+    return sq;
+}
+
+void SafeQueueDestroy(SafeQueueT *sq) {
+    if (!sq)
+        return;
+
+    mutex_lock(&sq->mutex);
+    queue_destroy(sq->q);
+    mutex_unlock(&sq->mutex);
+
+    mutex_destroy(&sq->mutex);
+    cond_destroy(&sq->cond);
+    free(sq);
+}
+
+void SafeQueuePush(SafeQueueT *sq, void *t) {
+    mutex_lock(&sq->mutex);
+    queue_push_back(sq->q, t);
+    cond_signal(&sq->cond);
+    mutex_unlock(&sq->mutex);
+}
+
+void SafeQueuePushFront(SafeQueueT *sq, void *t) {
+    mutex_lock(&sq->mutex);
+    queue_push_front(sq->q, t);
+    cond_signal(&sq->cond);
+    mutex_unlock(&sq->mutex);
+}
+
+void *SafeQueueFront(SafeQueueT *sq) {
+    void *value;
+    mutex_lock(&sq->mutex);
+    while (queue_count(sq->q) == 0) {
+        cond_wait(&sq->cond, &sq->mutex);
+    }
+    value = queue_peek_front(sq->q);
+    mutex_unlock(&sq->mutex);
+    return value;
+}
+
+void *SafeQueuePop(SafeQueueT *sq) {
+    void *value;
+    mutex_lock(&sq->mutex);
+    while (queue_count(sq->q) == 0) {
+        cond_wait(&sq->cond, &sq->mutex);
+    }
+    value = queue_pop_front(sq->q);
+    cond_signal(&sq->cond);
+    mutex_unlock(&sq->mutex);
+    return value;
+}
+
+int SafeQueueSize(SafeQueueT *sq) {
+    int size = 0;
+    mutex_lock(&sq->mutex);
+    size = queue_count(sq->q);
+    mutex_unlock(&sq->mutex);
+    return size;
+}
+
+int SafeQueueEmpty(SafeQueueT *sq) {
+    int empty = 0;
+    mutex_lock(&sq->mutex);
+    empty = (queue_count(sq->q) == 0);
+    mutex_unlock(&sq->mutex);
+    return empty;
+}
+
+void SafeQueueWaitEmpty(SafeQueueT *sq) {
+    mutex_lock(&sq->mutex);
+    while (queue_count(sq->q) != 0) {
+        cond_wait(&sq->cond, &sq->mutex);
+    }
+    mutex_unlock(&sq->mutex);
+}
+
+#if 0
+
+#include <stdio.h>
+
+int main(int argc, char *argv[])
+{
+    SafeQueueT *queue = SafeQueueCreate();
+
+    char TEST[] = {'A', 'B', 'C', 'D'};
+
+    SafeQueuePush(queue, (void *)&TEST[0]);
+    SafeQueuePush(queue, (void *)&TEST[1]);
+    SafeQueuePush(queue, (void *)&TEST[2]);
+    SafeQueuePush(queue, (void *)&TEST[3]);
+
+    SafeQueuePushFront(queue, (void *)&TEST[0]);
+    SafeQueuePushFront(queue, (void *)&TEST[1]);
+    SafeQueuePushFront(queue, (void *)&TEST[2]);
+    SafeQueuePushFront(queue, (void *)&TEST[3]);
+
+    while (!SafeQueueEmpty(queue))
+    {
+        char *c = (char *)SafeQueueFront(queue);
+        printf("%c\n", *c);
+        c = SafeQueuePop(queue);
+    }
+
+    SafeQueueDestroy(queue);
+    return 0;
+}
+#endif
diff --git a/libavfilter/inference_backend/safe_queue.h b/libavfilter/inference_backend/safe_queue.h
new file mode 100644
index 0000000..c6b8d93
--- /dev/null
+++ b/libavfilter/inference_backend/safe_queue.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef __SAFE_QUEUE_H
+#define __SAFE_QUEUE_H
+
+typedef struct _SafeQueue SafeQueueT;
+
+SafeQueueT *SafeQueueCreate(void);
+
+void SafeQueueDestroy(SafeQueueT *sq);
+
+void SafeQueuePush(SafeQueueT *sq, void *t);
+
+void SafeQueuePushFront(SafeQueueT *sq, void *t);
+
+void *SafeQueueFront(SafeQueueT *sq);
+
+void *SafeQueuePop(SafeQueueT *sq);
+
+int SafeQueueEmpty(SafeQueueT *sq);
+
+void SafeQueueWaitEmpty(SafeQueueT *sq);
+
+// Debug only
+int SafeQueueSize(SafeQueueT *sq);
+
+#endif // __SAFE_QUEUE_H
diff --git a/libavfilter/vf_inference_classify.c b/libavfilter/vf_inference_classify.c
new file mode 100644
index 0000000..3fd2296
--- /dev/null
+++ b/libavfilter/vf_inference_classify.c
@@ -0,0 +1,370 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * image inference filter used for object classification
+ */
+
+#include "libavutil/opt.h"
+#include "libavutil/mem.h"
+#include "libavutil/eval.h"
+#include "libavutil/avassert.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/mathematics.h"
+
+#include "formats.h"
+#include "internal.h"
+#include "avfilter.h"
+#include "filters.h"
+#include "libavcodec/avcodec.h"
+#include "libavformat/avformat.h"
+#include "libavutil/time.h"
+
+#include "inference_backend/ff_base_inference.h"
+
+#define OFFSET(x) offsetof(IEClassifyContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+static int flush_frame(AVFilterContext *ctx, AVFilterLink *outlink, int64_t pts, int64_t *out_pts);
+
+typedef struct IEClassifyContext {
+    const AVClass *class;
+
+    FFBaseInference *base;
+
+    FF_INFERENCE_OPTIONS
+
+    int    async_preproc;
+    int    backend_type;
+    int    already_flushed;
+} IEClassifyContext;
+
+static int query_formats(AVFilterContext *context)
+{
+    AVFilterFormats *formats_list;
+    const enum AVPixelFormat pixel_formats[] = {
+        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_NV12,
+        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,
+        AV_PIX_FMT_BGR0,     AV_PIX_FMT_RGBP,
+        AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
+        AV_PIX_FMT_NONE};
+
+    formats_list = ff_make_format_list(pixel_formats);
+    if (!formats_list) {
+        av_log(context, AV_LOG_ERROR, "Could not create formats list\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return ff_set_common_formats(context, formats_list);
+}
+
+static int config_input(AVFilterLink *inlink)
+{
+    int ret = 0;
+    AVFilterContext *ctx = inlink->dst;
+    IEClassifyContext *s = ctx->priv;
+    const AVPixFmtDescriptor *desc   = av_pix_fmt_desc_get(inlink->format);
+    if (desc == NULL)
+        return AVERROR(EINVAL);
+
+    FFInferenceParam param = { };
+    param = s->base->param;
+
+    if (desc->flags & AV_PIX_FMT_FLAG_HWACCEL) {
+        AVHWFramesContext *hw_frm_ctx = (AVHWFramesContext *)inlink->hw_frames_ctx->data;
+        AVHWDeviceContext *dev_ctx = (AVHWDeviceContext *)hw_frm_ctx->device_ref->data;
+#if CONFIG_VAAPI
+        param.vpp_device = VPP_DEVICE_HW;
+        param.opaque = (void *)((AVVAAPIDeviceContext *)dev_ctx->hwctx)->display;
+#endif
+        for (int i = 0; i < ctx->nb_outputs; i++) {
+            if (!ctx->outputs[i]->hw_frames_ctx)
+                ctx->outputs[i]->hw_frames_ctx = av_buffer_ref(inlink->hw_frames_ctx);
+        }
+    }
+
+    ret = av_base_inference_set_params(s->base, &param);
+
+    return ret;
+}
+
+static av_cold int classify_init(AVFilterContext *ctx)
+{
+    int ret;
+    IEClassifyContext  *s = ctx->priv;
+    av_assert0(s->model);
+    FFInferenceParam param = { };
+
+    param.model           = s->model;
+    param.device          = s->device;
+    param.nireq           = s->nireq;
+    param.batch_size      = s->batch_size;
+    param.every_nth_frame = s->every_nth_frame;
+    param.threshold       = s->threshold;
+    param.is_full_frame   = 0;
+    param.infer_config    = s->infer_config;
+    param.model_proc      = s->model_proc;
+    param.opaque          = s->async_preproc ? (void *)MOCKER_PRE_PROC_MAGIC : 0;
+
+    s->base = av_base_inference_create(ctx->filter->name);
+    if (!s->base) {
+        av_log(ctx, AV_LOG_ERROR, "Could not create inference.\n");
+        return AVERROR(EINVAL);
+    }
+    ret = av_base_inference_init(s->base, &param);
+
+    return ret;
+}
+
+static av_cold void classify_uninit(AVFilterContext *ctx)
+{
+    IEClassifyContext *s = ctx->priv;
+
+    flush_frame(ctx, NULL, 0LL, NULL);
+
+    av_base_inference_release(s->base);
+}
+
+static int flush_frame(AVFilterContext *ctx, AVFilterLink *outlink, int64_t pts, int64_t *out_pts)
+{
+    int ret = 0;
+    IEClassifyContext *s = ctx->priv;
+
+    if (s->already_flushed)
+        return ret;
+
+    while (!av_base_inference_frame_queue_empty(ctx, s->base)) {
+        AVFrame *output = NULL;
+        av_base_inference_get_frame(ctx, s->base, &output);
+        if (output) {
+            if (outlink) {
+                ret = ff_filter_frame(outlink, output);
+                if (out_pts)
+                    *out_pts = output->pts + pts;
+            } else {
+                av_frame_free(&output);
+            }
+        }
+
+        av_base_inference_send_event(ctx, s->base, INFERENCE_EVENT_EOS);
+        av_usleep(5000);
+    }
+
+    s->already_flushed = 1;
+    return ret;
+}
+
+static int classify_num_of_roi(AVFrame *frame) {
+    int roi_num = 0;
+    BBoxesArray *bboxes = NULL;
+    InferDetectionMeta *detect_meta = NULL;
+    AVFrameSideData *side_data;
+
+    if (!frame)
+        return 0;
+
+    side_data = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_DETECTION);
+    if (side_data) {
+        detect_meta = (InferDetectionMeta *)(side_data->data);
+        av_assert0(detect_meta);
+        bboxes = detect_meta->bboxes;
+        roi_num = bboxes ? bboxes->num : 0;
+    }
+
+    return roi_num;
+}
+
+static int load_balance(AVFilterContext *ctx)
+{
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVFilterLink *outlink = ctx->outputs[0];
+    IEClassifyContext *s = ctx->priv;
+    AVFrame *in = NULL, *output = NULL;
+    int64_t pts;
+    int i, ret, status, idx;
+    int needed, resource, prepared, got_frames = 0;
+
+    FF_FILTER_FORWARD_STATUS_BACK(outlink, inlink);
+
+    while (av_base_inference_get_frame(ctx, s->base, &output) == 0) {
+        if (output) {
+            status = ff_filter_frame(outlink, output);
+            if (status < 0)
+                return status;
+            got_frames = 1;
+            output = NULL;
+        }
+    }
+
+    status = ff_outlink_get_status(inlink);
+    if (status)
+        needed = ff_inlink_queued_frames(inlink);
+    else {
+        needed = 0;
+        prepared = 0;
+        idx = ff_inlink_queued_frames(inlink);
+        resource = av_base_inference_resource_status(ctx, s->base);
+        for (i = 0; i < idx; i++) {
+            int num_roi;
+            in = ff_inlink_peek_frame(inlink, i);
+            num_roi = classify_num_of_roi(in);
+            if (num_roi > s->nireq * s->batch_size) {
+                needed = resource < s->nireq ? 0 : 1;
+                break;
+            }
+            else {
+                if (num_roi == 0 || (resource - prepared) >= num_roi) {
+                    needed++;
+                    prepared += num_roi;
+                    continue;
+                } else
+                    break;
+            }
+        }
+    }
+
+    while (needed > 0) {
+        ret = ff_inlink_consume_frame(inlink, &in);
+        if (ret < 0)
+            return ret;
+        if (ret > 0) {
+            av_base_inference_send_frame(ctx, s->base, in);
+        }
+        needed--;
+    }
+
+    if (!status && got_frames)
+        return 0;
+
+    if (ff_inlink_acknowledge_status(inlink, &status, &pts)) {
+        if (status == AVERROR_EOF) {
+            int64_t out_pts = pts;
+
+            av_log(ctx, AV_LOG_INFO, "Get EOS.\n");
+            ret = flush_frame(ctx, outlink, pts, &out_pts);
+            ff_outlink_set_status(outlink, status, out_pts);
+            return ret;
+        }
+    }
+
+    FF_FILTER_FORWARD_WANTED(outlink, inlink);
+
+    return FFERROR_NOT_READY;
+}
+
+static int activate(AVFilterContext *ctx)
+{
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVFilterLink *outlink = ctx->outputs[0];
+    IEClassifyContext *s = ctx->priv;
+    AVFrame *in = NULL, *output = NULL;
+    int64_t pts;
+    int ret, status;
+    int got_frames = 0;
+
+    if (av_load_balance_get())
+        return load_balance(ctx);
+
+    FF_FILTER_FORWARD_STATUS_BACK(outlink, inlink);
+
+    // drain all frames from inlink
+    do {
+        ret = ff_inlink_consume_frame(inlink, &in);
+        if (ret < 0)
+            return ret;
+        if (ret > 0)
+            av_base_inference_send_frame(ctx, s->base, in);
+
+        while (av_base_inference_get_frame(ctx, s->base, &output) == 0) {
+            if (output) {
+                status = ff_filter_frame(outlink, output);
+                if (status < 0)
+                    return status;
+                got_frames = 1;
+                output = NULL;
+            }
+        }
+    } while (ret > 0);
+
+    if (got_frames)
+        return 0;
+
+    if (ff_inlink_acknowledge_status(inlink, &status, &pts)) {
+        if (status == AVERROR_EOF) {
+            int64_t out_pts = pts;
+
+            av_log(ctx, AV_LOG_INFO, "Get EOS.\n");
+            ret = flush_frame(ctx, outlink, pts, &out_pts);
+            ff_outlink_set_status(outlink, status, out_pts);
+            return ret;
+        }
+    }
+
+    FF_FILTER_FORWARD_WANTED(outlink, inlink);
+
+    return FFERROR_NOT_READY;
+}
+
+static const AVOption inference_classify_options[] = {
+    { "dnn_backend",  "DNN backend for model execution", OFFSET(backend_type),    AV_OPT_TYPE_FLAGS,  { .i64 = 1},          0, 2,  FLAGS },
+    { "model",        "path to model file for network",  OFFSET(model),           AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "model_proc",   "model preproc and postproc",      OFFSET(model_proc),      AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "object_class", "objective class",                 OFFSET(object_class),    AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "device",       "running on device name",          OFFSET(device),          AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "configs",      "configurations to backend",       OFFSET(infer_config),    AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "interval",     "detect every Nth frame",          OFFSET(every_nth_frame), AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 1024, FLAGS},
+    { "nireq",        "inference request number",        OFFSET(nireq),           AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 128,  FLAGS},
+    { "batch_size",   "batch size per infer",            OFFSET(batch_size),      AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 1000, FLAGS},
+    { "threshold",    "threshod to filter output data",  OFFSET(threshold),       AV_OPT_TYPE_FLOAT,  { .dbl = 0.5}, 0, 1,    FLAGS},
+    { "async_preproc", "do asynchronous preproc in inference backend", OFFSET(async_preproc), AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, FLAGS },
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(inference_classify);
+
+static const AVFilterPad classify_inputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .config_props  = config_input,
+    },
+    { NULL }
+};
+
+static const AVFilterPad classify_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_inference_classify = {
+    .name          = "classify",
+    .description   = NULL_IF_CONFIG_SMALL("Image Inference classify filter."),
+    .priv_size     = sizeof(IEClassifyContext),
+    .query_formats = query_formats,
+    .activate      = activate,
+    .init          = classify_init,
+    .uninit        = classify_uninit,
+    .inputs        = classify_inputs,
+    .outputs       = classify_outputs,
+    .priv_class    = &inference_classify_class,
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+};
diff --git a/libavfilter/vf_inference_detect.c b/libavfilter/vf_inference_detect.c
new file mode 100644
index 0000000..2742f35
--- /dev/null
+++ b/libavfilter/vf_inference_detect.c
@@ -0,0 +1,357 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * image inference filter used for object detection
+ */
+
+#include "libavutil/opt.h"
+#include "libavutil/mem.h"
+#include "libavutil/eval.h"
+#include "libavutil/avassert.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/mathematics.h"
+
+#include "formats.h"
+#include "internal.h"
+#include "avfilter.h"
+#include "filters.h"
+#include "libavcodec/avcodec.h"
+#include "libavformat/avformat.h"
+#include "libavutil/time.h"
+
+#include "inference_backend/ff_base_inference.h"
+
+#define OFFSET(x) offsetof(IEDetectContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+static int flush_frame(AVFilterContext *ctx, AVFilterLink *outlink, int64_t pts, int64_t *out_pts);
+
+
+typedef struct IEDetectContext {
+    const AVClass *class;
+
+    FFBaseInference *base;
+
+    FF_INFERENCE_OPTIONS
+
+    int    async_preproc;
+    int    backend_type;
+    int    already_flushed;
+    char  *crop_params;
+} IEDetectContext;
+
+static int query_formats(AVFilterContext *context)
+{
+    AVFilterFormats *formats_list;
+    const enum AVPixelFormat pixel_formats[] = {
+        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_NV12,
+        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,
+        AV_PIX_FMT_BGR0,     AV_PIX_FMT_RGBP,
+        AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
+        AV_PIX_FMT_NONE};
+
+    formats_list = ff_make_format_list(pixel_formats);
+    if (!formats_list) {
+        av_log(context, AV_LOG_ERROR, "Could not create formats list\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return ff_set_common_formats(context, formats_list);
+}
+
+static int config_input(AVFilterLink *inlink)
+{
+    int ret = 0;
+    AVFilterContext *ctx = inlink->dst;
+    IEDetectContext *s = ctx->priv;
+    const AVPixFmtDescriptor *desc   = av_pix_fmt_desc_get(inlink->format);
+    if (desc == NULL)
+        return AVERROR(EINVAL);
+
+    FFInferenceParam param = { };
+    param = s->base->param;
+
+    if (desc->flags & AV_PIX_FMT_FLAG_HWACCEL) {
+        AVHWFramesContext *hw_frm_ctx = (AVHWFramesContext *)inlink->hw_frames_ctx->data;
+        AVHWDeviceContext *dev_ctx = (AVHWDeviceContext *)hw_frm_ctx->device_ref->data;
+#if CONFIG_VAAPI
+        param.vpp_device = VPP_DEVICE_HW;
+        param.opaque = (void *)((AVVAAPIDeviceContext *)dev_ctx->hwctx)->display;
+#endif
+        for (int i = 0; i < ctx->nb_outputs; i++) {
+            if (!ctx->outputs[i]->hw_frames_ctx)
+                ctx->outputs[i]->hw_frames_ctx = av_buffer_ref(inlink->hw_frames_ctx);
+        }
+    }
+
+    if (s->crop_params) {
+        sscanf(s->crop_params, "%d|%d|%d|%d",
+                &param.crop_rect.x, &param.crop_rect.y, &param.crop_rect.width, &param.crop_rect.height);
+        if (param.crop_rect.x < 0         || param.crop_rect.y < 0      ||
+                param.crop_rect.width < 0 || param.crop_rect.height < 0 ||
+                param.crop_rect.width  + param.crop_rect.x > inlink->w  ||
+                param.crop_rect.height + param.crop_rect.y > inlink->h) {
+            av_log(ctx, AV_LOG_ERROR, "Invalid cropping parameters.\n");
+            return AVERROR(EINVAL);
+        }
+    }
+
+    ret = av_base_inference_set_params(s->base, &param);
+
+    return ret;
+}
+
+static av_cold int detect_init(AVFilterContext *ctx)
+{
+    int ret;
+    IEDetectContext *s = ctx->priv;
+    av_assert0(s->model);
+    FFInferenceParam param = { };
+
+    param.model           = s->model;
+    param.device          = s->device;
+    param.nireq           = s->nireq;
+    param.batch_size      = s->batch_size;
+    param.every_nth_frame = s->every_nth_frame;
+    param.threshold       = s->threshold;
+    param.is_full_frame   = 1;
+    param.infer_config    = s->infer_config;
+    param.model_proc      = s->model_proc;
+    param.opaque          = s->async_preproc ? (void *)MOCKER_PRE_PROC_MAGIC : 0;
+
+    s->base = av_base_inference_create(ctx->filter->name);
+    if (!s->base) {
+        av_log(ctx, AV_LOG_ERROR, "Could not create inference.\n");
+        return AVERROR(EINVAL);
+    }
+    ret = av_base_inference_init(s->base, &param);
+
+    return ret;
+}
+
+static av_cold void detect_uninit(AVFilterContext *ctx)
+{
+    IEDetectContext *s = ctx->priv;
+
+    flush_frame(ctx, NULL, 0LL, NULL);
+
+    av_base_inference_release(s->base);
+}
+
+static int flush_frame(AVFilterContext *ctx, AVFilterLink *outlink, int64_t pts, int64_t *out_pts)
+{
+    int ret = 0;
+    IEDetectContext *s = ctx->priv;
+
+    if (s->already_flushed)
+        return ret;
+
+    while (!av_base_inference_frame_queue_empty(ctx, s->base)) {
+        AVFrame *output = NULL;
+        av_base_inference_get_frame(ctx, s->base, &output);
+        if (output) {
+            if (outlink) {
+                ret = ff_filter_frame(outlink, output);
+                if (out_pts)
+                    *out_pts = output->pts + pts;
+            } else {
+                av_frame_free(&output);
+            }
+        }
+
+        av_base_inference_send_event(ctx, s->base, INFERENCE_EVENT_EOS);
+        av_usleep(5000);
+    }
+
+    s->already_flushed = 1;
+    return ret;
+}
+
+static int load_balance(AVFilterContext *ctx)
+{
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVFilterLink *outlink = ctx->outputs[0];
+    IEDetectContext *s = ctx->priv;
+    AVFrame *in = NULL, *output = NULL;
+    int64_t pts;
+    int ret, status;
+    int resource, got_frames = 0;
+    int get_frame_status;
+
+    FF_FILTER_FORWARD_STATUS_BACK(outlink, inlink);
+
+    // drain all processed frames
+    do {
+        get_frame_status = av_base_inference_get_frame(ctx, s->base, &output);
+        if (output) {
+            int ret_val = ff_filter_frame(outlink, output);
+            if (ret_val < 0)
+                return ret_val;
+
+            got_frames = 1;
+            output = NULL;
+        }
+    } while (get_frame_status == 0);
+
+    status = ff_outlink_get_status(inlink);
+    if (status)
+        resource = ff_inlink_queued_frames(inlink);
+    else
+        resource = av_base_inference_resource_status(ctx, s->base);
+
+    while (resource > 0) {
+        ret = ff_inlink_consume_frame(inlink, &in);
+        if (ret < 0)
+            return ret;
+        if (ret == 0)
+            break;
+        if (ret > 0) {
+            av_base_inference_send_frame(ctx, s->base, in);
+        }
+        resource--;
+    }
+
+    if (!status && got_frames)
+        return 0;
+
+    if (ff_inlink_acknowledge_status(inlink, &status, &pts)) {
+        if (status == AVERROR_EOF) {
+            int64_t out_pts = pts;
+
+            av_log(ctx, AV_LOG_INFO, "Get EOS.\n");
+            ret = flush_frame(ctx, outlink, pts, &out_pts);
+            ff_outlink_set_status(outlink, status, out_pts);
+            return ret;
+        }
+    }
+
+    FF_FILTER_FORWARD_WANTED(outlink, inlink);
+
+    return FFERROR_NOT_READY;
+}
+
+static int activate(AVFilterContext *ctx)
+{
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVFilterLink *outlink = ctx->outputs[0];
+    IEDetectContext *s = ctx->priv;
+    AVFrame *in = NULL, *output = NULL;
+    int64_t pts;
+    int ret, status;
+    int got_frame = 0;
+
+    if (av_load_balance_get())
+        return load_balance(ctx);
+
+    FF_FILTER_FORWARD_STATUS_BACK(outlink, inlink);
+
+    do {
+        int get_frame_status;
+        // drain all input frames
+        ret = ff_inlink_consume_frame(inlink, &in);
+        if (ret < 0)
+            return ret;
+        if (ret > 0)
+            av_base_inference_send_frame(ctx, s->base, in);
+
+        // drain all processed frames
+        do {
+            get_frame_status = av_base_inference_get_frame(ctx, s->base, &output);
+            if (output) {
+                int ret_val = ff_filter_frame(outlink, output);
+                if (ret_val < 0)
+                    return ret_val;
+
+                got_frame = 1;
+                output = NULL;
+            }
+        } while (get_frame_status == 0);
+    } while (ret > 0);
+
+    // if frame got, schedule to next filter
+    if (got_frame)
+        return 0;
+
+    if (ff_inlink_acknowledge_status(inlink, &status, &pts)) {
+        if (status == AVERROR_EOF) {
+            int64_t out_pts = pts;
+
+            av_log(ctx, AV_LOG_INFO, "Get EOS.\n");
+            ret = flush_frame(ctx, outlink, pts, &out_pts);
+            ff_outlink_set_status(outlink, status, out_pts);
+            return ret;
+        }
+    }
+
+    FF_FILTER_FORWARD_WANTED(outlink, inlink);
+
+    return FFERROR_NOT_READY;
+}
+
+static const AVOption inference_detect_options[] = {
+    { "dnn_backend",  "DNN backend for model execution", OFFSET(backend_type),    AV_OPT_TYPE_FLAGS,  { .i64 = 1},          0, 2,  FLAGS },
+    { "model",        "path to model file for network",  OFFSET(model),           AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "model_proc",   "model preproc and postproc",      OFFSET(model_proc),      AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "object_class", "objective class",                 OFFSET(object_class),    AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "device",       "running on device name",          OFFSET(device),          AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "configs",      "configurations to backend",       OFFSET(infer_config),    AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "interval",     "detect every Nth frame",          OFFSET(every_nth_frame), AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 1024, FLAGS},
+    { "nireq",        "inference request number",        OFFSET(nireq),           AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 128,  FLAGS},
+    { "batch_size",   "batch size per infer",            OFFSET(batch_size),      AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 1000, FLAGS},
+    { "threshold",    "threshod to filter output data",  OFFSET(threshold),       AV_OPT_TYPE_FLOAT,  { .dbl = 0.5}, 0, 1,    FLAGS},
+    { "square_bbox",  "optimizing bbox for face detect",  OFFSET(square_bbox),    AV_OPT_TYPE_BOOL,   { .i64 = 0 },  0, 1,    FLAGS},
+    { "crop_params",  "cropping rectangle format x|y|w|h", OFFSET(crop_params),   AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "async_preproc", "do asynchronous preproc in inference backend", OFFSET(async_preproc), AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, FLAGS },
+
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(inference_detect);
+
+static const AVFilterPad detect_inputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .config_props  = config_input,
+    },
+    { NULL }
+};
+
+static const AVFilterPad detect_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_inference_detect = {
+    .name          = "detect",
+    .description   = NULL_IF_CONFIG_SMALL("Image Inference Detect Filter."),
+    .priv_size     = sizeof(IEDetectContext),
+    .query_formats = query_formats,
+    .activate      = activate,
+    .init          = detect_init,
+    .uninit        = detect_uninit,
+    .inputs        = detect_inputs,
+    .outputs       = detect_outputs,
+    .priv_class    = &inference_detect_class,
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+};
diff --git a/libavfilter/vf_inference_identify.c b/libavfilter/vf_inference_identify.c
new file mode 100644
index 0000000..23e307d
--- /dev/null
+++ b/libavfilter/vf_inference_identify.c
@@ -0,0 +1,357 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * dnn inference identify filter
+ */
+#include "libavutil/opt.h"
+#include "libavutil/mem.h"
+#include "libavutil/eval.h"
+#include "libavutil/avassert.h"
+#include "libavutil/avstring.h"
+#include "libavutil/pixdesc.h"
+#include "libavformat/avformat.h"
+
+#include "formats.h"
+#include "internal.h"
+#include "avfilter.h"
+
+#include "inference_backend/ff_base_inference.h"
+#include "inference_backend/model_proc.h"
+
+#include <json-c/json.h>
+
+#define OFFSET(x) offsetof(InferenceIdentifyContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+#define UNUSED(x) (void)(x)
+
+#define PI 3.1415926
+#define FACE_FEATURE_VECTOR_LEN 256
+
+typedef struct FeatureLabelPair {
+    float *feature;
+    size_t label_id;
+} FeatureLabelPair;
+
+typedef struct InferenceIdentifyContext {
+    const AVClass *class;
+
+    char   *gallery;      ///<< gallery for identify features
+    double *norm_std;
+
+    AVBufferRef *labels;
+    FeatureLabelPair **features;
+    int features_num;
+} InferenceIdentifyContext;
+
+static const char *get_filename_ext(const char *filename) {
+    const char *dot = strrchr(filename, '.');
+    if (!dot || dot == filename)
+        return NULL;
+
+    return dot + 1;
+}
+
+const char *gallery_file_suffix = "json";
+
+static double av_norm(float vec[], size_t num) {
+    size_t i;
+    double result = 0.0;
+
+    for (i = 0; i < num; i++)
+        result += vec[i] * vec[i];
+
+    return sqrt(result);
+}
+
+static double av_dot(float vec1[], float vec2[], size_t num) {
+    size_t i;
+    double result = 0.0;
+
+    for (i = 0; i < num; i++)
+        result += vec1[i] * vec2[i];
+
+    return result;
+}
+
+static int query_formats(AVFilterContext *context) {
+    AVFilterFormats *formats_list;
+    const enum AVPixelFormat pixel_formats[] = {
+        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_YUV422P,  AV_PIX_FMT_YUV444P,
+        AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_YUVJ422P, AV_PIX_FMT_YUVJ444P,
+        AV_PIX_FMT_YUV410P,  AV_PIX_FMT_YUV411P,  AV_PIX_FMT_GRAY8,
+        AV_PIX_FMT_RGBP,
+        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
+        AV_PIX_FMT_NONE};
+
+    formats_list = ff_make_format_list(pixel_formats);
+    if (!formats_list) {
+        av_log(context, AV_LOG_ERROR, "Could not create formats list\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return ff_set_common_formats(context, formats_list);
+}
+
+static av_cold int identify_init(AVFilterContext *ctx) {
+    size_t i, index = 1;
+    char *dup, *unknown;
+    const char *dirname, *suffix;
+    json_object *entry;
+    LabelsArray *larray = NULL;
+    AVBufferRef *ref    = NULL;
+    InferenceIdentifyContext *s = ctx->priv;
+    size_t vec_size_in_bytes = sizeof(float) * FACE_FEATURE_VECTOR_LEN;
+    int ret;
+
+    av_assert0(s->gallery);
+
+    suffix = get_filename_ext(s->gallery);
+    if (!suffix) {
+        av_log(ctx, AV_LOG_ERROR, "Unrecognized gallery file '%s' \n", s->gallery);
+        return AVERROR(EINVAL);
+    }
+
+    if (0 != strcmp(suffix, gallery_file_suffix)) {
+        av_log(ctx, AV_LOG_ERROR, "Gallery '%s' is not a json file\n", s->gallery);
+        return AVERROR(EINVAL);
+    }
+
+    entry = model_proc_read_config_file(s->gallery);
+    if (!entry) {
+        av_log(ctx, AV_LOG_ERROR, "Could not open gallery file:%s\n", s->gallery);
+        return AVERROR(EIO);
+    }
+
+    dup = av_strdup(s->gallery);
+    dirname = av_dirname(dup);
+
+    larray = av_mallocz(sizeof(*larray));
+    if (!larray)
+        return AVERROR(ENOMEM);
+
+    // label id 0 reserved for unknown person
+    unknown = av_strdup("Unknown_Person");
+    av_dynarray_add(&larray->label, &larray->num, unknown);
+
+    json_object_object_foreach(entry, key, jvalue){
+        char *l = av_strdup(key);
+        json_object *features, *feature;
+
+        av_dynarray_add(&larray->label, &larray->num, l);
+
+        ret = json_object_object_get_ex(jvalue, "features", &features);
+        if (ret) {
+            size_t features_num = json_object_array_length(features);
+
+            for(int i = 0; i < features_num; i++) {
+                FILE *vec_fp;
+                FeatureLabelPair *pair;
+                char path[4096];
+
+                feature = json_object_array_get_idx(features, i);
+                if (json_object_get_string(feature) == NULL)
+                    continue;
+
+                av_assert0((strlen(dirname) + strlen(json_object_get_string(feature)) + 1) < sizeof(path));
+                strncpy(path, dirname, sizeof(path));
+                strncat(path, "/", 1);
+                strncat(path, json_object_get_string(feature), strlen(json_object_get_string(feature)));
+
+                vec_fp = fopen(path, "rb");
+                if (!vec_fp) {
+                    av_log(ctx, AV_LOG_ERROR, "Could not open feature file:%s\n", path);
+                    continue;
+                }
+
+                pair = av_mallocz(sizeof(FeatureLabelPair));
+                if (!pair){
+                    fclose(vec_fp);
+                    goto error;
+                }
+
+                pair->feature = av_malloc(vec_size_in_bytes);
+                if (!pair->feature){
+                    fclose(vec_fp);
+                    av_free(pair);
+                    goto error;
+                }
+
+                if (fread(pair->feature, vec_size_in_bytes, 1, vec_fp) != 1) {
+                    av_log(ctx, AV_LOG_ERROR, "Feature vector size mismatch:%s\n", path);
+                    fclose(vec_fp);
+                    av_free(pair->feature);
+                    av_free(pair);
+                    goto error;
+                }
+
+                fclose(vec_fp);
+
+                pair->label_id = index;
+                av_dynarray_add(&s->features, &s->features_num, pair);
+            }
+        }
+        index++;
+    }
+
+    s->norm_std = av_mallocz(sizeof(double) * s->features_num);
+    if (!s->norm_std)
+        goto error;
+
+    for (i = 0; i < s->features_num; i++)
+        s->norm_std[i] = av_norm(s->features[i]->feature, FACE_FEATURE_VECTOR_LEN);
+
+    ref = av_buffer_create((uint8_t *)larray, sizeof(*larray),
+            &infer_labels_buffer_free, NULL, 0);
+
+    s->labels = ref;
+    av_free(dup);
+    json_object_put(entry);
+
+    return 0;
+error:
+    if (larray)
+        av_free(larray);
+    return AVERROR(ENOMEM);
+}
+
+static av_cold void identify_uninit(AVFilterContext *ctx) {
+    int i;
+    InferenceIdentifyContext *s = ctx->priv;
+
+    av_buffer_unref(&s->labels);
+
+    if (s->features) {
+        for (i = 0; i < s->features_num; i++) {
+            av_freep(&s->features[i]->feature);
+            av_freep(&s->features[i]);
+        }
+        av_free(s->features);
+    }
+    if (s->norm_std)
+        av_free(s->norm_std);
+}
+
+static av_cold void dump_object_id(AVFilterContext *ctx, const char *name,
+                                   int label_id, float conf, AVBufferRef *label_buf) {
+    LabelsArray *array = (LabelsArray *)label_buf->data;
+
+    av_log(ctx, AV_LOG_DEBUG,"CLASSIFY META - %s:%d Label:%s Conf:%1.2f\n",
+           name, label_id, array->label[label_id], conf);
+}
+
+static int object_identify(AVFilterContext *ctx, AVFrame *frame) {
+    int i;
+    InferenceIdentifyContext *s = ctx->priv;
+    AVFrameSideData *side_data;
+    ClassifyArray *c_array;
+    InferClassificationMeta *meta;
+
+    side_data = av_frame_get_side_data(frame,
+            AV_FRAME_DATA_INFERENCE_CLASSIFICATION);
+
+    if (!side_data)
+        return 0;
+
+    meta = (InferClassificationMeta *)side_data->data;
+    if (!meta)
+        return 0;
+
+    c_array = meta->c_array;
+    for (i = 0; i < c_array->num; i++) {
+        int n, label = 0;
+        float *vector;
+        InferClassification *c;
+        double dot_product, norm_feature, confidence, angle;
+        double min_angle = 180.0f;
+
+        c = c_array->classifications[i];
+        vector = (float *)c->tensor.buffer->data;
+        norm_feature = av_norm(vector, FACE_FEATURE_VECTOR_LEN);
+        for (n = 0; n < s->features_num; n++) {
+            dot_product = av_dot(vector, s->features[n]->feature, FACE_FEATURE_VECTOR_LEN);
+
+            angle = acos((dot_product - 0.0001f) / (s->norm_std[n] * norm_feature))
+                    /
+                    PI * 180.0;
+            if (angle < 70 && angle < min_angle) {
+                label = s->features[n]->label_id;
+                min_angle = angle;
+            }
+        }
+
+        confidence = (90.0f - min_angle) / 90.0f;
+
+        c->label_id   = label;
+        c->confidence = (float)confidence;
+        c->label_buf  = av_buffer_ref(s->labels);
+
+        dump_object_id(ctx, c->name, label, confidence, s->labels);
+    }
+
+    return 0;
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in) {
+    AVFilterContext *ctx  = inlink->dst;
+    AVFilterLink *outlink = inlink->dst->outputs[0];
+
+    object_identify(ctx, in);
+
+    return ff_filter_frame(outlink, in);
+}
+
+static const AVOption inference_identify_options[] = {
+    { "gallery", "JSON file with list of image examples for each known object/face/person",
+        OFFSET(gallery), AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(inference_identify);
+
+static const AVFilterPad identify_inputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .filter_frame  = filter_frame,
+    },
+    { NULL }
+};
+
+static const AVFilterPad identify_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_inference_identify= {
+    .name          = "identify",
+    .description   = NULL_IF_CONFIG_SMALL("DNN Inference identification."),
+    .priv_size     = sizeof(InferenceIdentifyContext),
+    .query_formats = query_formats,
+    .init          = identify_init,
+    .uninit        = identify_uninit,
+    .inputs        = identify_inputs,
+    .outputs       = identify_outputs,
+    .priv_class    = &inference_identify_class,
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+};
diff --git a/libavfilter/vf_inference_metaconvert.c b/libavfilter/vf_inference_metaconvert.c
new file mode 100644
index 0000000..2828d4b
--- /dev/null
+++ b/libavfilter/vf_inference_metaconvert.c
@@ -0,0 +1,272 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * DNN inference metadata convert filter
+ */
+
+#include "libavutil/opt.h"
+#include "libavutil/mem.h"
+#include "libavutil/avassert.h"
+
+#include "formats.h"
+#include "internal.h"
+#include "avfilter.h"
+#include "libavcodec/avcodec.h"
+#include "libavformat/avformat.h"
+
+#include "inference_backend/ff_base_inference.h"
+#include "inference_backend/metaconverter.h"
+
+#define OFFSET(x) offsetof(MetaConvertContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+typedef struct MetaConvertContext {
+    const AVClass *class;
+
+    char *converter;
+    char *method;
+
+    char *tags;
+    char *custom_json_tag;
+    char *source;
+    char *location;
+
+    FILE *output_file;
+    int frame_number;
+
+    convert_function_type convert_func;
+} MetaConvertContext;
+
+static int query_formats(AVFilterContext *ctx) {
+    AVFilterFormats *formats_list;
+    const enum AVPixelFormat pixel_formats[] = {
+        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_YUV422P,  AV_PIX_FMT_YUV444P,
+        AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_YUVJ422P, AV_PIX_FMT_YUVJ444P,
+        AV_PIX_FMT_YUV410P,  AV_PIX_FMT_YUV411P,  AV_PIX_FMT_GRAY8,
+        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
+        AV_PIX_FMT_NONE};
+
+    formats_list = ff_make_format_list(pixel_formats);
+    if (!formats_list) {
+        av_log(ctx, AV_LOG_ERROR, "Could not create formats list\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return ff_set_common_formats(ctx, formats_list);
+}
+
+static inline FFVAMetaconvertConverterType get_converter_type(const char *converter) {
+    if (!strcmp(converter, "json")) {
+        return FFVA_METACONVERT_JSON;
+    } else if (!strcmp(converter, "tensors-to-file")) {
+        return FFVA_METACONVERT_TENSORS_TO_FILE;
+    } // TODO: add more converters
+
+    return FFVA_METACONVERT_JSON;
+}
+
+static inline FFVAMetaconvertMethodType get_method_type(const char *method) {
+    if (!method)
+        return FFVA_METACONVERT_ALL;
+
+    if (!strcmp(method, "all")) {
+        return FFVA_METACONVERT_ALL;
+    } else if (!strcmp(method, "detection")) {
+        return FFVA_METACONVERT_DETECTION;
+    } else if (!strcmp(method, "tensor")) {
+        return FFVA_METACONVERT_TENSOR;
+    }
+
+    return FFVA_METACONVERT_ALL;
+}
+
+static void add_frame_data_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *output_json) {
+    AVFilterLink *inlink = ctx->inputs[0];
+    MetaConvertContext *s = ctx->priv;
+    int64_t nano_ts = 1000000000LL;
+    json_object *resolution = NULL, *tags = NULL;
+
+    // resolution
+    resolution = json_object_new_object();
+    json_object_object_add(resolution, "width", json_object_new_int(frame->width));
+    json_object_object_add(resolution, "height", json_object_new_int(frame->height));
+    json_object_object_add(output_json, "resolution", resolution);
+
+    // source
+    if (s->source)
+        json_object_object_add(output_json, "source", json_object_new_string(s->source));
+
+    // tags
+    if (s->custom_json_tag) {
+        tags = json_tokener_parse(s->custom_json_tag);
+        if (tags)
+            json_object_object_add(output_json, "tags", tags);
+    }
+
+    // timestamp
+    nano_ts = frame->pts * (nano_ts * inlink->time_base.num / inlink->time_base.den);
+    json_object_object_add(output_json, "timestamp", json_object_new_int64(nano_ts));
+}
+
+static av_cold int metaconvert_init(AVFilterContext *ctx) {
+    MetaConvertContext *s = ctx->priv;
+    char filename[1024] = {0};
+
+    av_log(ctx, AV_LOG_INFO, "converter:%s method:%s location:%s \n", s->converter, s->method, s->location);
+
+    if (!s->converter) {
+        av_log(ctx, AV_LOG_ERROR, "Missing key parameters!!\n");
+        return AVERROR(EINVAL);
+    }
+
+    if (s->tags && strstr(s->tags, "file|")) {
+        FILE *fp = NULL;
+        char custom_tag_file[256] = "";
+        if (strlen(s->tags) > 256) {
+            av_log(ctx, AV_LOG_WARNING, "Tags length is not allow to longer than 256!\n");
+        }
+        sscanf(s->tags, "file|%255s", custom_tag_file);
+        av_log(ctx, AV_LOG_INFO, "custom_tag_file:%s\n", custom_tag_file);
+        fp = fopen(custom_tag_file, "rb");
+        if (fp) {
+            long size;
+            size_t result;
+
+            // obtain file size:
+            fseek(fp, 0 , SEEK_END);
+            size = ftell(fp);
+            rewind(fp);
+
+            s->custom_json_tag = (char *)av_mallocz(size + 1);
+            av_assert0(s->custom_json_tag);
+            result = fread(s->custom_json_tag, size, 1, fp);
+            av_assert0(result == 1);
+            av_log(ctx, AV_LOG_INFO, "%s\n", s->custom_json_tag);
+            fclose(fp);
+        }
+    }
+
+    // option to write metadata to json file by current filter
+    // *** NOT recommend: use metapublish instead! ***
+    if (s->location && !strcmp(s->converter, "json")) {
+        snprintf(filename, sizeof(filename), "%s/%s.json", s->location, "output");
+        s->output_file = fopen(filename, "wb");
+        if (!s->output_file) {
+            av_log(ctx, AV_LOG_ERROR, "Failed to open/create file: %s\n", filename);
+            return AVERROR(EINVAL);
+        }
+    }
+
+    s->convert_func = ffva_metaconvert_get_convert_func(get_converter_type(s->converter),
+                                                        get_method_type(s->method));
+    if (!s->convert_func) {
+        av_log(ctx, AV_LOG_ERROR, "Please check the parameters!");
+        return AVERROR(ERANGE);
+    }
+
+    return 0;
+}
+
+static av_cold void metaconvert_uninit(AVFilterContext *ctx) {
+    MetaConvertContext *s = ctx->priv;
+    if (s->output_file)
+        fclose(s->output_file);
+    if (s->custom_json_tag)
+        av_freep(&s->custom_json_tag);
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in) {
+    AVFilterContext *ctx  = inlink->dst;
+    MetaConvertContext *s = ctx->priv;
+    AVFilterLink *outlink = inlink->dst->outputs[0];
+    const char *proc_json = NULL;
+    json_object *root = json_object_new_object();
+
+    int ret = s->convert_func(ctx, in, root, s->location, s->tags, add_frame_data_to_json);
+    if (ret) {
+        proc_json = json_object_to_json_string_ext(root, JSON_C_TO_STRING_PLAIN);
+        if (s->output_file) {
+            fwrite(proc_json, strlen(proc_json), 1,  s->output_file);
+            fwrite("\n", 1, 1, s->output_file);
+        } else {
+            AVBufferRef *ref_buff = NULL;
+            size_t json_string_len = strlen(proc_json);
+            if (in->opaque_ref) {
+                av_log(ctx, AV_LOG_WARNING, "Somebody has used this field!");
+                av_buffer_unref(&in->opaque_ref);
+            }
+
+            if (json_string_len > 0) {
+                ref_buff = av_buffer_alloc(json_string_len);
+                if (!ref_buff)
+                    return AVERROR(ENOMEM);
+                memcpy(ref_buff->data, proc_json, json_string_len);
+                in->opaque_ref = ref_buff;
+            }
+        }
+        s->frame_number++;
+    }
+    json_object_put(root);
+
+    return ff_filter_frame(outlink, in);
+}
+
+static const AVOption inference_metaconvert_options[] = {
+    { "converter", "metadata conversion group",   OFFSET(converter), AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { "method",    "metadata conversion method",  OFFSET(method),    AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { "location",  "location for the output files, path to folder",   OFFSET(location),  AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { "tags",      "custom tags file with format \"file|<json_file_path>\": the JSON object of custom values added to json message",
+        OFFSET(tags),    AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { "source",    "Source URI",  OFFSET(source),    AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(inference_metaconvert);
+
+static const AVFilterPad metaconvert_inputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .filter_frame  = filter_frame,
+    },
+    { NULL }
+};
+
+static const AVFilterPad metaconvert_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_inference_metaconvert = {
+    .name          = "metaconvert",
+    .description   = NULL_IF_CONFIG_SMALL("DNN Inference metaconvert."),
+    .priv_size     = sizeof(MetaConvertContext),
+    .query_formats = query_formats,
+    .init          = metaconvert_init,
+    .uninit        = metaconvert_uninit,
+    .inputs        = metaconvert_inputs,
+    .outputs       = metaconvert_outputs,
+    .priv_class    = &inference_metaconvert_class,
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+};
diff --git a/libavfilter/vf_ocv_overlay.c b/libavfilter/vf_ocv_overlay.c
new file mode 100644
index 0000000..2ff1305
--- /dev/null
+++ b/libavfilter/vf_ocv_overlay.c
@@ -0,0 +1,319 @@
+/*
+ * Copyright (c) 2018 Pengfei Qu
+ * Copyright (c) 2019 Lin Xie
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * libopencv wrapper functions to overlay
+ */
+
+#include "config.h"
+
+#pragma GCC diagnostic ignored "-Wunused-function"
+#pragma GCC diagnostic push
+#if HAVE_OPENCV2_CORE_CORE_C_H
+#include <opencv2/core/core_c.h>
+#include <opencv2/imgproc/imgproc_c.h>
+#else
+#include <opencv/cv.h>
+#include <opencv/cxcore.h>
+#endif
+#pragma GCC diagnostic pop
+
+#include <json-c/json.h>
+
+#include "avfilter.h"
+#include "formats.h"
+#include "inference_backend/ff_base_inference.h"
+#include "inference_backend/metaconverter.h"
+#include "internal.h"
+#include "libavutil/avassert.h"
+#include "libavutil/avstring.h"
+#include "libavutil/common.h"
+#include "libavutil/file.h"
+#include "libavutil/opt.h"
+#include "video.h"
+
+static void fill_iplimage_from_frame(IplImage *img, const AVFrame *frame, enum AVPixelFormat pixfmt) {
+    IplImage *tmpimg;
+    int depth, channels_nb;
+
+    if (pixfmt == AV_PIX_FMT_GRAY8) {
+        depth = IPL_DEPTH_8U;
+        channels_nb = 1;
+    } else if (pixfmt == AV_PIX_FMT_BGRA) {
+        depth = IPL_DEPTH_8U;
+        channels_nb = 4;
+    } else if (pixfmt == AV_PIX_FMT_BGR24) {
+        depth = IPL_DEPTH_8U;
+        channels_nb = 3;
+    } else
+        return;
+
+    tmpimg = cvCreateImageHeader((CvSize){frame->width, frame->height}, depth, channels_nb);
+    *img = *tmpimg;
+    img->imageData = img->imageDataOrigin = frame->data[0];
+    img->dataOrder = IPL_DATA_ORDER_PIXEL;
+    img->origin = IPL_ORIGIN_TL;
+    img->widthStep = frame->linesize[0];
+}
+
+static void fill_frame_from_iplimage(AVFrame *frame, const IplImage *img, enum AVPixelFormat pixfmt) {
+    frame->linesize[0] = img->widthStep;
+    frame->data[0] = img->imageData;
+}
+
+static int query_formats(AVFilterContext *ctx) {
+    const enum AVPixelFormat pix_fmts[] = {AV_PIX_FMT_BGR24, AV_PIX_FMT_BGRA, AV_PIX_FMT_GRAY8, AV_PIX_FMT_NONE};
+    AVFilterFormats *fmts_list = ff_make_format_list(pix_fmts);
+    if (!fmts_list)
+        return AVERROR(ENOMEM);
+    return ff_set_common_formats(ctx, fmts_list);
+}
+
+typedef struct OCVOverlayContext {
+    const AVClass *class;
+
+    char *content_mode;
+
+    CvScalar color;
+    CvFont font;
+    int thickness;
+    int line_type;
+    int shift;
+
+    convert_function_type convert;
+} OCVOverlayContext;
+
+static av_cold int init(AVFilterContext *ctx) {
+    OCVOverlayContext *s = ctx->priv;
+
+    s->color = cvScalar(0, 255, 0, 255); // green
+    s->thickness = 1;
+    s->line_type = 8;
+    s->shift = 0;
+    cvInitFont(&s->font, CV_FONT_HERSHEY_SIMPLEX, 0.8, 0.8, 0, 2, s->line_type);
+
+    if (!strcmp(s->content_mode, "detailed"))
+        s->convert = ffva_metaconvert_get_convert_func(FFVA_METACONVERT_JSON, FFVA_METACONVERT_ALL);
+    return 0;
+}
+
+static void draw_rectangle(AVFilterContext *ctx, IplImage *img, CvScalar color, CvPoint pt1, CvPoint pt2) {
+    OCVOverlayContext *s = ctx->priv;
+    cvRectangle(img, pt1, pt2, color, s->thickness, s->line_type, s->shift);
+}
+
+static void put_text(AVFilterContext *ctx, IplImage *img, CvScalar color, CvPoint pt1, const char *text) {
+    OCVOverlayContext *s = ctx->priv;
+    cvPutText(img, text, pt1, &s->font, color);
+}
+
+static void inline drawPhotoFrameCorner(IplImage *img, CvScalar color, CvPoint p, int dx, int dy) {
+    const int photoFrameThickness = 2;
+
+    cvLine(img, p, (CvPoint){.x = p.x, .y = p.y + dy}, color, photoFrameThickness, 8, 0);
+    cvLine(img, p, (CvPoint){.x = p.x + dx, .y = p.y}, color, photoFrameThickness, 8, 0);
+};
+
+static void draw_photo_frame(IplImage *img, CvScalar color, CvPoint pt1, CvPoint pt2, int width, int height) {
+    const int bbThickness = 1;
+    const float photoFrameLength = 0.1;
+
+    int dx = (int)(photoFrameLength * width);
+    int dy = (int)(photoFrameLength * height);
+
+    cvRectangle(img, pt1, pt2, color, bbThickness, 8, 0);
+
+    drawPhotoFrameCorner(img, color, pt1, dx, dy);
+    drawPhotoFrameCorner(img, color, (CvPoint){.x = pt1.x + width - 1, .y = pt1.y}, -dx, dy);
+    drawPhotoFrameCorner(img, color, (CvPoint){.x = pt1.x, .y = pt1.y + height - 1}, dx, -dy);
+    drawPhotoFrameCorner(img, color, pt2, -dx, -dy);
+}
+
+static void draw_results_from_metadata(AVFilterContext *ctx, AVFrame *in, IplImage *img) {
+    OCVOverlayContext *s = ctx->priv;
+    json_object *root = json_object_new_object();
+    json_object *objects;
+    int len;
+
+    int ret = s->convert(ctx, in, root, NULL, NULL, NULL);
+    if (0 == ret)
+        goto done;
+
+    if (!json_object_object_get_ex(root, "objects", &objects))
+        goto done;
+
+    len = json_object_array_length(objects);
+    for (int i = 0; i < len; i++) {
+        CvPoint pt1, pt2, pt_txt;
+        json_object *object = json_object_array_get_idx(objects, i);
+        json_object *detection, *detect_label = NULL;
+        json_object *x, *y, *w, *h;
+        json_bool face_detection = FALSE;
+        CvRect location;
+        CvScalar color = s->color;
+        char text[512] = {0};
+
+        json_object_object_get_ex(object, "detection", &detection);
+        json_object_object_get_ex(detection, "label", &detect_label);
+
+        json_object_object_get_ex(object, "x", &x);
+        json_object_object_get_ex(object, "y", &y);
+        json_object_object_get_ex(object, "w", &w);
+        json_object_object_get_ex(object, "h", &h);
+
+        location.x = json_object_get_int(x);
+        location.y = json_object_get_int(y);
+        location.width = json_object_get_int(w);
+        location.height = json_object_get_int(h);
+
+        pt1.x = location.x;
+        pt1.y = location.y;
+        pt2.x = pt1.x + json_object_get_int(w);
+        pt2.y = pt1.y + json_object_get_int(h);
+
+        pt_txt.x = location.x;
+        pt_txt.y = location.y - 10;
+
+        if (detect_label) {
+            const char *string = json_object_get_string(detect_label);
+            if (!strcmp(string, "face"))
+                face_detection = TRUE;
+            strncat(text, string, (511 - strlen(text)));
+        }
+
+        json_object_object_foreach(object, key, val) {
+            json_object *label;
+
+            // skip keys already processed
+            if (!strcmp(key, "detection") || !strcmp(key, "x") || !strcmp(key, "y") || !strcmp(key, "w") ||
+                !strcmp(key, "h"))
+                continue;
+
+            if (json_object_object_get_ex(val, "label", &label)) {
+                const char *strings = json_object_get_string(label);
+                strncat(text, ",", (511 - strlen(text)));
+                strncat(text, strings, (511 - strlen(text)));
+                if (!strcmp(key, "gender"))
+                    color = !strcmp(strings, "Male") ? cvScalar(255, 0, 0, 255) : cvScalar(147, 20, 255, 255);
+            }
+        }
+
+        put_text(ctx, img, color, pt_txt, text);
+
+        if (face_detection) {
+            draw_photo_frame(img, color, pt1, pt2, json_object_get_int(w), json_object_get_int(h));
+        } else {
+            draw_rectangle(ctx, img, color, pt1, pt2);
+        }
+    }
+
+done:
+    json_object_put(root);
+}
+
+static void draw_results_from_sidedata(AVFilterContext *ctx, AVFrame *in, IplImage *img) {
+    CvPoint pt1, pt2;
+    BBoxesArray *boxes;
+    InferDetectionMeta *meta;
+    OCVOverlayContext *s = ctx->priv;
+    AVFrameSideData *sd = av_frame_get_side_data(in, AV_FRAME_DATA_INFERENCE_DETECTION);
+
+    if (!sd)
+        return;
+
+    // Draw bounding box according to side data
+    meta = (InferDetectionMeta *)sd->data;
+    boxes = meta->bboxes;
+    if (boxes && boxes->num > 0) {
+        for (int i = 0; i < boxes->num; i++) {
+            InferDetection *box = boxes->bbox[i];
+            pt1.x = box->roi_meta.x;
+            pt1.y = box->roi_meta.y;
+            pt2.x = box->roi_meta.x + box->roi_meta.w;
+            pt2.y = box->roi_meta.y + box->roi_meta.h;
+
+            if (box->label_buf && !strcmp(((LabelsArray *)box->label_buf->data)->label[box->label_id], "face"))
+                draw_photo_frame(img, s->color, pt1, pt2, box->roi_meta.w, box->roi_meta.h);
+            else
+                draw_rectangle(ctx, img, s->color, pt1, pt2);
+        }
+    }
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in) {
+    AVFilterContext *ctx = inlink->dst;
+    AVFilterLink *outlink = inlink->dst->outputs[0];
+    OCVOverlayContext *s = ctx->priv;
+    IplImage img;
+
+    if (in->nb_side_data == 0)
+        return ff_filter_frame(outlink, in);
+
+    fill_iplimage_from_frame(&img, in, inlink->format);
+
+    if (s->convert)
+        draw_results_from_metadata(ctx, in, &img);
+    else
+        draw_results_from_sidedata(ctx, in, &img);
+
+    fill_frame_from_iplimage(in, &img, inlink->format);
+
+    return ff_filter_frame(outlink, in);
+}
+
+#define OFFSET(x) offsetof(OCVOverlayContext, x)
+#define FLAGS AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM
+static const AVOption ocv_overlay_options[] = {{"content_mode",
+                                                "basic or detailed content display",
+                                                OFFSET(content_mode),
+                                                AV_OPT_TYPE_STRING,
+                                                {.str = "detailed"},
+                                                CHAR_MIN,
+                                                CHAR_MAX,
+                                                FLAGS},
+                                               {NULL}};
+
+AVFILTER_DEFINE_CLASS(ocv_overlay);
+
+static const AVFilterPad avfilter_vf_ocv_overlay_inputs[] = {{
+                                                                 .name = "default",
+                                                                 .type = AVMEDIA_TYPE_VIDEO,
+                                                                 .filter_frame = filter_frame,
+                                                             },
+                                                             {NULL}};
+
+static const AVFilterPad avfilter_vf_ocv_overlay_outputs[] = {{
+                                                                  .name = "default",
+                                                                  .type = AVMEDIA_TYPE_VIDEO,
+                                                              },
+                                                              {NULL}};
+
+AVFilter ff_vf_ocv_overlay = {
+    .name = "ocv_overlay",
+    .description = NULL_IF_CONFIG_SMALL("Draw rectangle and text using libopencv."),
+    .priv_size = sizeof(OCVOverlayContext),
+    .priv_class = &ocv_overlay_class,
+    .query_formats = query_formats,
+    .init = init,
+    .inputs = avfilter_vf_ocv_overlay_inputs,
+    .outputs = avfilter_vf_ocv_overlay_outputs,
+};
diff --git a/libavformat/Makefile b/libavformat/Makefile
old mode 100644
new mode 100755
index a434b00..3ec7fb5
--- a/libavformat/Makefile
+++ b/libavformat/Makefile
@@ -296,6 +296,7 @@ OBJS-$(CONFIG_MATROSKA_MUXER)            += matroskaenc.o matroska.o \
                                             flacenc_header.o avlanguage.o vorbiscomment.o wv.o \
                                             webmdashenc.o webm_chunk.o
 OBJS-$(CONFIG_MD5_MUXER)                 += hashenc.o
+OBJS-$(CONFIG_METAPUBLISH_MUXER)         += metapublishenc.o
 OBJS-$(CONFIG_MGSTS_DEMUXER)             += mgsts.o
 OBJS-$(CONFIG_MICRODVD_DEMUXER)          += microdvddec.o subtitles.o
 OBJS-$(CONFIG_MICRODVD_MUXER)            += microdvdenc.o
@@ -631,6 +632,7 @@ OBJS-$(CONFIG_LIBRTMPTE_PROTOCOL)        += librtmp.o
 OBJS-$(CONFIG_LIBSMBCLIENT_PROTOCOL)     += libsmbclient.o
 OBJS-$(CONFIG_LIBSRT_PROTOCOL)           += libsrt.o
 OBJS-$(CONFIG_LIBSSH_PROTOCOL)           += libssh.o
+OBJS-$(CONFIG_RDKAFKA_PROTOCOL)          += kafkaproto.o
 
 # libavdevice dependencies
 OBJS-$(CONFIG_IEC61883_INDEV)            += dv.o
diff --git a/libavformat/allformats.c b/libavformat/allformats.c
old mode 100644
new mode 100755
index cd00834..bc3eac7
--- a/libavformat/allformats.c
+++ b/libavformat/allformats.c
@@ -225,6 +225,7 @@ extern AVOutputFormat ff_md5_muxer;
 extern AVInputFormat  ff_matroska_demuxer;
 extern AVOutputFormat ff_matroska_muxer;
 extern AVOutputFormat ff_matroska_audio_muxer;
+extern AVOutputFormat ff_metapublish_muxer;
 extern AVInputFormat  ff_mgsts_demuxer;
 extern AVInputFormat  ff_microdvd_demuxer;
 extern AVOutputFormat ff_microdvd_muxer;
diff --git a/libavformat/kafkaproto.c b/libavformat/kafkaproto.c
new file mode 100644
index 0000000..e3d7007
--- /dev/null
+++ b/libavformat/kafkaproto.c
@@ -0,0 +1,186 @@
+/*
+ * Kafka network protocol
+ * Copyright (c) 2019 Shaofei Wang
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * Kafka protocol based producer
+ */
+
+#include "avformat.h"
+#include "url.h"
+
+#include "librdkafka/rdkafka.h"
+
+typedef struct KafkaContext {
+    rd_kafka_t *rk;         /* Producer instance handle */
+    rd_kafka_conf_t *conf;  /* Configuration object */
+    rd_kafka_topic_t *rkt;  /* Topic object */
+} KafkaContext;
+
+static void dr_msg_cb (rd_kafka_t *rk,
+                       const rd_kafka_message_t *rkmessage, void *opaque) {
+    if (rkmessage->err)
+        fprintf(stderr, "%% Message delivery failed: %s\n",
+                rd_kafka_err2str(rkmessage->err));
+    /* Avoid too much print mesg
+    else
+        fprintf(stderr, "%% Message delivered (%zd bytes, "
+                "partition %"PRId32")\n",
+                rkmessage->len, rkmessage->partition);
+                */
+}
+
+static int kafka_open(URLContext *s, const char *uri, int flags, AVDictionary **opts)
+{
+    KafkaContext *kc = s->priv_data;
+
+    char proto[8], hostname[256], path[1024], auth[100], brokers[256], errstr[512], *topic;
+    int port;
+
+    av_url_split(proto, sizeof(proto), auth, sizeof(auth),
+                 hostname, sizeof(hostname), &port,
+                 path, sizeof(path), s->filename);
+    topic = strrchr(s->filename, '/') + 1;
+
+    kc->conf = rd_kafka_conf_new();
+    port ? snprintf(brokers, 256, "%s:%d", hostname, port)
+        : snprintf(brokers, 256, "%s:9092", hostname);
+
+    /* Set bootstrap broker(s) as a comma-separated list of
+     * host or host:port (default port 9092).
+     * librdkafka will use the bootstrap brokers to acquire the full
+     * set of brokers from the cluster. */
+    if (rd_kafka_conf_set(kc->conf, "bootstrap.servers", brokers,
+                          errstr, sizeof(errstr)) != RD_KAFKA_CONF_OK) {
+        av_log(s, AV_LOG_ERROR, "%s\n", errstr);
+        return AVERROR_UNKNOWN;
+    }
+
+    rd_kafka_conf_set_dr_msg_cb(kc->conf, dr_msg_cb);
+
+    kc->rk = rd_kafka_new(RD_KAFKA_PRODUCER, kc->conf, errstr, sizeof(errstr));
+    if (!kc->rk) {
+        av_log(s, AV_LOG_ERROR,
+                "%% Failed to create new producer: %s\n", errstr);
+        return AVERROR_UNKNOWN;
+    }
+
+    kc->rkt = rd_kafka_topic_new(kc->rk, topic, NULL);
+    if (!kc->rkt) {
+        av_log(s, AV_LOG_ERROR,
+                "%% Failed to create topic object: %s\n",
+                rd_kafka_err2str(rd_kafka_last_error()));
+        rd_kafka_destroy(kc->rk);
+        return AVERROR_UNKNOWN;
+    }
+
+    return 0;
+}
+
+static int kafka_close(URLContext *h)
+{
+    KafkaContext *kc = h->priv_data;
+
+    rd_kafka_flush(kc->rk, 10*1000 /* wait for max 10 seconds */);
+
+    rd_kafka_topic_destroy(kc->rkt);
+
+    rd_kafka_destroy(kc->rk);
+
+    return 0;
+}
+
+static int kafka_write(URLContext *s, const uint8_t *buf, int size)
+{
+    KafkaContext *kc = s->priv_data;
+    rd_kafka_t *rk = kc->rk;
+    rd_kafka_topic_t *rkt = kc->rkt;
+
+    if (size == 0) {
+        /* Empty line: only serve delivery reports */
+        rd_kafka_poll(rk, 0/*non-blocking */);
+        return 0;
+    }
+
+retry:
+    if (rd_kafka_produce(
+        /* Topic object */
+        rkt,
+        /* Use builtin partitioner to select partition*/
+        RD_KAFKA_PARTITION_UA,
+        /* Make a copy of the payload. */
+        RD_KAFKA_MSG_F_COPY,
+        /* Message payload (value) and length */
+        buf, size,
+        /* Optional key and its length */
+        NULL, 0,
+        /* Message opaque, provided in
+         * delivery report callback as
+         * msg_opaque. */
+        NULL) == -1) {
+
+        /**
+         * Failed to *enqueue* message for producing.
+         */
+        av_log(s, AV_LOG_ERROR,
+                "%% Failed to produce to topic %s: %s\n",
+                rd_kafka_topic_name(rkt),
+                rd_kafka_err2str(rd_kafka_last_error()));
+
+        /* Poll to handle delivery reports */
+        if (rd_kafka_last_error() ==
+            RD_KAFKA_RESP_ERR__QUEUE_FULL) {
+                /* If the internal queue is full, wait for
+                 * messages to be delivered and then retry.
+                 * The internal queue represents both
+                 * messages to be sent and messages that have
+                 * been sent or failed, awaiting their
+                 * delivery report callback to be called.
+                 *
+                 * The internal queue is limited by the
+                 * configuration property
+                 * queue.buffering.max.messages */
+                rd_kafka_poll(rk, 1000/*block for max 1000ms*/);
+                goto retry;
+        }
+    } else {
+        rd_kafka_poll(rk, 0/*non-blocking*/);
+    }
+    return size;
+}
+
+#define KAFKA_PROTOCOL(flavor)                    \
+static const AVClass flavor##_class = {           \
+    .class_name = #flavor,                        \
+    .item_name  = av_default_item_name,           \
+    .version    = LIBAVUTIL_VERSION_INT,          \
+};                                                \
+                                                  \
+const URLProtocol ff_##flavor##_protocol = {      \
+    .name           = "kafka",                    \
+    .url_open       = kafka_open,                 \
+    .url_write      = kafka_write,                \
+    .url_close      = kafka_close,                \
+    .priv_data_size = sizeof(KafkaContext),     \
+    .flags          = URL_PROTOCOL_FLAG_NETWORK,  \
+};
+
+KAFKA_PROTOCOL(rdkafka)
diff --git a/libavformat/metapublishenc.c b/libavformat/metapublishenc.c
new file mode 100644
index 0000000..f66e209
--- /dev/null
+++ b/libavformat/metapublishenc.c
@@ -0,0 +1,167 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "avformat.h"
+#include "internal.h"
+#include "libavutil/dict.h"
+#include "libavutil/opt.h"
+
+#define BATCH "batch"
+#define STREAM "stream"
+
+typedef enum {
+    FFVA_METAPUBLISH_FILE,
+#if CONFIG_LIBRDKAFKA
+    FFVA_METAPUBLISH_KAFKA,
+#endif
+    FFVA_METAPUBLISH_NONE,
+} FFVAMetaPublishMethodType;
+
+// File specific constants
+typedef enum { FILE_PUBLISH_STREAM = 0, FILE_PUBLISH_BATCH = 1 } PublishOutputFormat;
+
+typedef struct MetaPublishContext {
+    AVClass *class;
+    FFVAMetaPublishMethodType method;
+
+    char *output_format;
+    PublishOutputFormat eOutFormat;
+} MetaPublishContext;
+
+static int metapublish_init(AVFormatContext *s) {
+    MetaPublishContext *mp = s->priv_data;
+
+    if (!mp->output_format) {
+        av_log(mp, AV_LOG_ERROR, "Output format for metapublish has not been set!\n");
+        return AVERROR(EINVAL);
+    }
+
+    av_log(mp, AV_LOG_INFO, "method:%d format:%s\n", mp->method, mp->output_format);
+
+    if (!strcmp(mp->output_format, STREAM))
+        mp->eOutFormat = FILE_PUBLISH_STREAM;
+    else
+        mp->eOutFormat = FILE_PUBLISH_BATCH;
+
+#if CONFIG_LIBRDKAFKA
+    if (mp->method == FFVA_METAPUBLISH_KAFKA && mp->eOutFormat != FILE_PUBLISH_STREAM) {
+        av_log(mp, AV_LOG_ERROR, "Kafka must set output format to 'stream'!\n");
+        return AVERROR(EINVAL);
+    }
+#endif
+
+    return 0;
+}
+
+static int metapublish_write_header(AVFormatContext *s) {
+    AVIOContext *pb = s->pb;
+    MetaPublishContext *mp = s->priv_data;
+
+    if (mp->eOutFormat == FILE_PUBLISH_BATCH)
+        avio_w8(pb, '[');
+
+    return 0;
+}
+
+static int metapublish_write_trailer(AVFormatContext *s) {
+    AVIOContext *pb = s->pb;
+    MetaPublishContext *mp = s->priv_data;
+
+    if (mp->eOutFormat == FILE_PUBLISH_BATCH)
+        avio_w8(pb, ']');
+
+    return 0;
+}
+
+static int metapublish_write_packet(AVFormatContext *s, AVPacket *pkt) {
+    AVFrame *frame = NULL;
+    AVIOContext *pb = s->pb;
+    MetaPublishContext *mp = s->priv_data;
+    AVBufferRef *meta_buff = NULL;
+    if (!pkt)
+        return 0;
+
+    frame = (AVFrame *)pkt->data;
+    if (!frame)
+        return 0;
+
+    if (!frame->opaque_ref) {
+        av_log(mp, AV_LOG_DEBUG, "No meta data in this frame!\n");
+        return 0;
+    }
+
+    meta_buff = av_buffer_ref(frame->opaque_ref);
+    if (!meta_buff)
+        return AVERROR(ENOMEM);
+
+    if (mp->method == FFVA_METAPUBLISH_FILE && mp->eOutFormat == FILE_PUBLISH_BATCH && pb->pos > 1) {
+        avio_w8(pb, ',');
+        avio_w8(pb, '\n');
+    }
+    avio_write(pb, meta_buff->data, meta_buff->size);
+    avio_flush(pb);
+    av_buffer_unref(&meta_buff);
+    return 0;
+}
+
+#define OFFSET(x) offsetof(MetaPublishContext, x)
+#define FLAGS AV_OPT_FLAG_ENCODING_PARAM
+static const AVOption options[] = {
+    {"method",
+     "Publishing method. Set to one of: 'file:0', 'kafka:1'",
+     OFFSET(method),
+     AV_OPT_TYPE_FLAGS,
+     {.i64 = FFVA_METAPUBLISH_FILE},
+     0,
+     FFVA_METAPUBLISH_NONE - 1,
+     FLAGS},
+    {"output_format",
+     "[method= file] Output format of published file. Set to one of: 'stream' "
+     "(raw inference per line) or 'batch' (each file holds array of JSON "
+     "inferences)",
+     OFFSET(output_format),
+     AV_OPT_TYPE_STRING,
+     {.str = BATCH},
+     0,
+     1,
+     FLAGS},
+    {NULL},
+};
+
+static const AVClass metapublish_muxer_class = {
+    .class_name = "metapublish_muxer",
+    .item_name = av_default_item_name,
+    .option = options,
+    .version = LIBAVUTIL_VERSION_INT,
+};
+
+AVOutputFormat ff_metapublish_muxer = {
+    .name = "metapublish",
+    .long_name = NULL_IF_CONFIG_SMALL("DNN inference metadata publisher"),
+    .extensions = "json",
+    .priv_data_size = sizeof(MetaPublishContext),
+    .video_codec = AV_CODEC_ID_WRAPPED_AVFRAME,
+    .init = metapublish_init,
+    .write_header = metapublish_write_header,
+    .write_packet = metapublish_write_packet,
+    .write_trailer = metapublish_write_trailer,
+    .flags = AVFMT_VARIABLE_FPS,
+    .priv_class = &metapublish_muxer_class,
+};
diff --git a/libavformat/protocols.c b/libavformat/protocols.c
old mode 100644
new mode 100755
index ad95659..d95bfdf
--- a/libavformat/protocols.c
+++ b/libavformat/protocols.c
@@ -68,6 +68,7 @@ extern const URLProtocol ff_librtmpte_protocol;
 extern const URLProtocol ff_libsrt_protocol;
 extern const URLProtocol ff_libssh_protocol;
 extern const URLProtocol ff_libsmbclient_protocol;
+extern const URLProtocol ff_rdkafka_protocol;
 
 #include "libavformat/protocol_list.c"
 
diff --git a/libavutil/buffer.c b/libavutil/buffer.c
old mode 100644
new mode 100755
index 8d1aa5f..fdb0b9d
--- a/libavutil/buffer.c
+++ b/libavutil/buffer.c
@@ -331,6 +331,17 @@ static AVBufferRef *pool_alloc_buffer(AVBufferPool *pool)
     return ret;
 }
 
+int av_buffer_pool_is_empty(AVBufferPool *pool)
+{
+    int empty = 0;
+
+    ff_mutex_lock(&pool->mutex);
+    empty = pool->pool ? 0 : 1;
+    ff_mutex_unlock(&pool->mutex);
+
+    return empty;
+}
+
 AVBufferRef *av_buffer_pool_get(AVBufferPool *pool)
 {
     AVBufferRef *ret;
diff --git a/libavutil/buffer.h b/libavutil/buffer.h
old mode 100644
new mode 100755
index 73b6bd0..9d74cd2
--- a/libavutil/buffer.h
+++ b/libavutil/buffer.h
@@ -285,6 +285,12 @@ void av_buffer_pool_uninit(AVBufferPool **pool);
 AVBufferRef *av_buffer_pool_get(AVBufferPool *pool);
 
 /**
+ * Check if the buffer pool is empty.
+ *
+ * @return 1: empty 0: not empty
+ */
+int av_buffer_pool_is_empty(AVBufferPool *pool);
+/**
  * @}
  */
 
diff --git a/libavutil/frame.c b/libavutil/frame.c
old mode 100644
new mode 100755
index dcf1fc3..90a586a
--- a/libavutil/frame.c
+++ b/libavutil/frame.c
@@ -383,12 +383,16 @@ FF_ENABLE_DEPRECATION_WARNINGS
 #endif
 
     for (i = 0; i < src->nb_side_data; i++) {
+        int keep_ref = 0;
         const AVFrameSideData *sd_src = src->side_data[i];
         AVFrameSideData *sd_dst;
         if (   sd_src->type == AV_FRAME_DATA_PANSCAN
             && (src->width != dst->width || src->height != dst->height))
             continue;
-        if (force_copy) {
+        if (sd_src->type == AV_FRAME_DATA_INFERENCE_CLASSIFICATION ||
+            sd_src->type == AV_FRAME_DATA_INFERENCE_DETECTION)
+            keep_ref = 1;
+        if (force_copy && !keep_ref) {
             sd_dst = av_frame_new_side_data(dst, sd_src->type,
                                             sd_src->size);
             if (!sd_dst) {
@@ -836,6 +840,8 @@ const char *av_frame_side_data_name(enum AVFrameSideDataType type)
     case AV_FRAME_DATA_S12M_TIMECODE:               return "SMPTE 12-1 timecode";
     case AV_FRAME_DATA_SPHERICAL:                   return "Spherical Mapping";
     case AV_FRAME_DATA_ICC_PROFILE:                 return "ICC profile";
+    case AV_FRAME_DATA_INFERENCE_CLASSIFICATION:    return "Inference classification metadata";
+    case AV_FRAME_DATA_INFERENCE_DETECTION:         return "Inference detection metadata";
 #if FF_API_FRAME_QP
     case AV_FRAME_DATA_QP_TABLE_PROPERTIES:         return "QP table properties";
     case AV_FRAME_DATA_QP_TABLE_DATA:               return "QP table data";
diff --git a/libavutil/frame.h b/libavutil/frame.h
old mode 100644
new mode 100755
index 5d3231e..d2f39ea
--- a/libavutil/frame.h
+++ b/libavutil/frame.h
@@ -142,6 +142,10 @@ enum AVFrameSideDataType {
      */
     AV_FRAME_DATA_ICC_PROFILE,
 
+    AV_FRAME_DATA_INFERENCE_CLASSIFICATION,
+
+    AV_FRAME_DATA_INFERENCE_DETECTION,
+
 #if FF_API_FRAME_QP
     /**
      * Implementation-specific description of the format of AV_FRAME_QP_TABLE_DATA.
diff --git a/libavutil/hwcontext_vaapi.c b/libavutil/hwcontext_vaapi.c
old mode 100644
new mode 100755
index cf11764..a1534bd
--- a/libavutil/hwcontext_vaapi.c
+++ b/libavutil/hwcontext_vaapi.c
@@ -120,6 +120,7 @@ static const VAAPIFormatDescriptor vaapi_format_map[] = {
     MAP(422V, YUV422,  YUV440P, 0),
     MAP(444P, YUV444,  YUV444P, 0),
     MAP(Y800, YUV400,  GRAY8,   0),
+    MAP(RGBP, RGBP,    RGBP,    0),
 #ifdef VA_FOURCC_P010
     MAP(P010, YUV420_10BPP, P010, 0),
 #endif
diff --git a/libavutil/log.c b/libavutil/log.c
old mode 100644
new mode 100755
index 93a156b..a6243290
--- a/libavutil/log.c
+++ b/libavutil/log.c
@@ -52,6 +52,9 @@ static AVMutex mutex = AV_MUTEX_INITIALIZER;
 #endif
 
 static int av_log_level = AV_LOG_INFO;
+static int av_profiling = 0;
+static int av_load_balance = 0;
+static int av_finit_firstly = 0;
 static int flags;
 
 #define NB_LEVELS 8
@@ -402,6 +405,36 @@ void av_log_set_callback(void (*callback)(void*, int, const char*, va_list))
     av_log_callback = callback;
 }
 
+int av_profiling_get(void)
+{
+    return av_profiling;
+}
+
+void av_profiling_set(int arg)
+{
+    av_profiling = arg;
+}
+
+void av_load_balance_set(int arg)
+{
+    av_load_balance = arg;
+}
+
+int av_load_balance_get(void)
+{
+    return av_load_balance;
+}
+
+void av_finit_firstly_set(int arg)
+{
+    av_finit_firstly = arg;
+}
+
+int av_finit_firstly_get(void)
+{
+    return av_finit_firstly;
+}
+
 static void missing_feature_sample(int sample, void *avc, const char *msg,
                                    va_list argument_list)
 {
diff --git a/libavutil/log.h b/libavutil/log.h
old mode 100644
new mode 100755
index d9554e6..bd893b7
--- a/libavutil/log.h
+++ b/libavutil/log.h
@@ -297,6 +297,15 @@ void av_log_set_callback(void (*callback)(void*, int, const char*, va_list));
 void av_log_default_callback(void *avcl, int level, const char *fmt,
                              va_list vl);
 
+int av_profiling_get(void);
+void av_profiling_set(int arg);
+
+int av_load_balance_get(void);
+void av_load_balance_set(int arg);
+
+int av_finit_firstly_get(void);
+void av_finit_firstly_set(int arg);
+
 /**
  * Return the context name
  *
diff --git a/libavutil/pixdesc.c b/libavutil/pixdesc.c
old mode 100644
new mode 100755
index b97b066..248bfdf
--- a/libavutil/pixdesc.c
+++ b/libavutil/pixdesc.c
@@ -229,6 +229,18 @@ static const AVPixFmtDescriptor av_pix_fmt_descriptors[AV_PIX_FMT_NB] = {
         },
         .flags = AV_PIX_FMT_FLAG_RGB,
     },
+    [AV_PIX_FMT_RGBP] = {
+        .name = "rgbp",
+        .nb_components = 3,
+        .log2_chroma_w = 0,
+        .log2_chroma_h = 0,
+        .comp = {
+            { 0, 1, 0, 0, 8, 0, 7, 1 },        /* R */
+            { 1, 1, 0, 0, 8, 0, 7, 1 },        /* G */
+            { 2, 1, 0, 0, 8, 0, 7, 1 },        /* B */
+        },
+        .flags = AV_PIX_FMT_FLAG_RGB | AV_PIX_FMT_FLAG_PLANAR,
+    },
     [AV_PIX_FMT_YUV422P] = {
         .name = "yuv422p",
         .nb_components = 3,
diff --git a/libavutil/pixfmt.h b/libavutil/pixfmt.h
old mode 100644
new mode 100755
index 8b54c94..e69cb62
--- a/libavutil/pixfmt.h
+++ b/libavutil/pixfmt.h
@@ -89,6 +89,7 @@ enum AVPixelFormat {
     AV_PIX_FMT_NV12,      ///< planar YUV 4:2:0, 12bpp, 1 plane for Y and 1 plane for the UV components, which are interleaved (first byte U and the following byte V)
     AV_PIX_FMT_NV21,      ///< as above, but U and V bytes are swapped
 
+    AV_PIX_FMT_RGBP,      ///< planar RGB 4:4:4 24bpp, 3 plane for R/G/B components
     AV_PIX_FMT_ARGB,      ///< packed ARGB 8:8:8:8, 32bpp, ARGBARGB...
     AV_PIX_FMT_RGBA,      ///< packed RGBA 8:8:8:8, 32bpp, RGBARGBA...
     AV_PIX_FMT_ABGR,      ///< packed ABGR 8:8:8:8, 32bpp, ABGRABGR...
-- 
2.7.4

